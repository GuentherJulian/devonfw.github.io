[{"id":"../website/pages/docs/Contributing-Code-of-Conduct.asciidoc.html","title":"60. Contributor Covenant Code of Conduct","body":"\n60. Contributor Covenant Code of Conduct\n60.1. Our Pledge\nIn the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.\n60.2. Our Standards\nExamples of behavior that contributes to creating a positive environment include:\nUsing welcoming and inclusive language\nBeing respectful of differing viewpoints and experiences\nGracefully accepting constructive criticism\nFocusing on what is best for the community\nShowing empathy towards other community members\nExamples of unacceptable behavior by participants include:\nThe use of sexualized language or imagery and unwelcome sexual attention or advances\nTrolling, insulting/derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others&apos; private information, such as a physical or electronic address, without explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n60.3. Our Responsibilities\nProject maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.\n60.4. Scope\nThis Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.\n60.5. Enforcement\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at [INSERT PRODUCT OWNER EMAIL ADDRESS]. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project&#x2019;s leadership.\n60.6. Attribution\nThis Code of Conduct is adapted from the Contributor Covenant, version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n&#x2190;&#xA0;Previous:&#xA0;Code Contributions&#xA0;| &#x2191;&#xA0;Up:&#xA0;Contributing Guide&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Development Guidelines&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/Contributing-Code.asciidoc.html","title":"59. Code Contributions","body":"\n59. Code Contributions\nMoved here. This document will be removed in the future.\n59.1. Notes on Code Contributions\ndevonfw platform is intended to be easy to contribute to. One service allowing such simplicity is GitHub was therefore selected as preferred collaboration platform.\nIn order to contribute code, git and GitHub specific pull-requests are being used.\nIt is mandatory to follow the code of conduct that must be present in the root of every OSS or private project as CODE_OF_CONDUCT.asciidoc or CODE_OF_CONDUCT.md.\n59.2. Introduction to Git and GitHub\nGit is a version control system used for a coordinated and versioned collaboration of computer files. It enables a project to be easily worked on by multiple developers and contributors.\nGitHub is an online repository used by deonvfw and OASP in order to host the corresponding files. Using the command line tool or any GUI Tool as &quot;GitHub Desktop&quot;, &quot;Fork&quot; or &quot;Atlassian SourceTree&quot; a user can easily manage project files. There are private and public repositories. Public ones can be accessed by everyone, private repositories require access permissions.\n59.2.1. Creating a new user account\nThe devonfw projects use GitHub as hosting service. Therefore you&#x2019;ll need an account to allow collaboration. Visit this page to create a new account. If available, use your CORP username as GitHub username and your CORP email address.\nA GitHub account is essential for contributing code and gaining permissions to access private repositories.\n59.2.2. Git Basics\nAn in-depth documentation on basic Git syntax and usage can be found on the official Git homepage. Another helpful and easy to follow instruction can be found here.\n59.3. Structure of our projects\nIn total, there are three GitHub organizations regarding devonfw:\ndevonfw-forge\nRepository organization used for work on incubators and other research projects.\ndevonfw\nThe official devonfw Platform project repository. Usually, two main branches exist inside each repository:\ndevelop\nThis branch contains software in the state of being in development.\nmaster\nThis branch contains software in release state.\n59.4. Contributing to our projects\nIn order to contribute to our projects, developers must follow the following development guidelines. Other sources about contributing to devonfw:\ndevonfw code of conduct\ndevonfw code contributions\ndevonfw development guidelines\nGit Fork guide\ndevonfw documentation\nEvery project must include the following files in order to establish the contributing rules and facilitate the process:\nCONTRIBUTING.asciidoc that establishes the specific guidelines of contributing in a project repository.\nCODE_OF_CONDUCT.asciidoc mandatory to contribute.\nISSUE_TEMPLATE.asciidoc that defines the appropriated way to submit an issue in a project repository.\nPULL_REQUEST_TEMPLATE.asciidoc that specifies the rules in order to submit a pull request in a project repository.\nImportant\nThis files must be included at the root folder or the the issues and pull request templates inside a .github folder.\nThis repository is a good resource to find the perfect templates for issues and pull requests that fit in your repository.\n59.4.1. Process of contributing code to the devonfw projects\nUse the issue tracker to check whether the issue you would like to be working on exists. Otherwise create a new issue.\nFigure 105. Using GitHub&#x2019;s issue tracker\nBefore making more complex changes you should probably notify the community. The worst case would be you investing time and effort into something that&#x2019;ll be later rejected. Oftentimes the devonfw Community on Yammer will have the right answer.\nAssign yourself to the issue you would like to work on. If a member was already assigned to your preferred issue, get in contact to contribute to the same issue.\nFork the desired repository to your corporate GitHub account. Afterwards you&#x2019;ll have your own copy of the repository you&#x2019;d like to work on.\nCreate a new branch for your feature/bugfix. Check out the develop branch for the upcoming release. The following changes will afterwards be merged when the new version is released.\nPlease read the Working with forked repositories document to learn all about this topic.\nCheck out the develop branch\ngit checkout develop-x.y.z\nCreate a new branch\ngit checkout -b myBranchName\nApply your modifications according to the coding conventions to the newly created branch\nVerify your changes to only include relevant and required changes.\nCommit your changes locally\nWhen commiting changes please follow this pattern for your commit message:\n#&lt;issueId&gt;: &lt;change description&gt;\nWhen working on multiple different repositories, the actual repository name of the change should also be declared in the commit message:\n&lt;project&gt;/&lt;repository&gt;#&lt;issueId&gt;: &lt;change description&gt;\nFor example:\ndevonfw/devon4j#1: added REST service for tablemanagement\nNote: Starting directly with a # symbol will comment out the line when using the editor to insert a commit message. Instead, you should use a prefix like a space or simply typing &quot;Issue&quot;. E.g.:\nIssue #4: Added some new feature, fixed some bug\nThe language to be used for commit messages is English.\nPush the changes to your Fork of the repository\nAfter completing the issue/bugfix/feature, use the pull request function in GitHub. This feature allows other members to look over your branch, automated CI systems may test your changes and finally apply the changes to the corresponding branch (if no conflicts occur).\nUse the tab &quot;Pull requests&quot; and the button labeled &quot;New pull request&quot;. Afterwards you can Choose different branches or forks above to discuss and review changes.\nNote\nMost of the devonfw projects contain a documents folder that includes the related documentation. It would be really interesting to add the documentation files of your new feature inside this pull request.\nBare in mind that, in case the changes have a big impact, the devonfw core team will request the necessary documentation in the pull request.\n59.5. Reviewing Pull Requests\nDetailed information about revieweing can be found on the official topic on GitHub Pull Requests.\nThere are two different methods to review Pull Requests:\nHuman based reviews\nOther project members are able to discuss the changes made in the pull request by having insight into changed files and file differences by commenting.\nFigure 106. People can add comments to pull requests and suggest further changes\nCI based reviews\nCI Systems like Jenkins or Travis.ci are able to listen for new pull requests on specified projects. As soon as the request was made, Travis for example checks out the to-be-merged branch and builds it. This enables an automated build which could even include testcases. Finally, the CI approves the pull requests if the build was built and tested successfully, otherwise it&#x2019;ll let the project members know that something went wrong.\nFigure 107. If Travis fails to build a project, it&#x2019;ll post the results directly to the pull request\nCombining these two possibilities should accelerate the reviewing process of pull requests.\nImportant\nThis document is the Official Covenant Code of Conduct that must be present in every OASP or devonfw project at the root folder as CODE_OF_CONDUCT.asciidoc or CODE_OF_CONDUCT.md. Please, include this contents in your repository and the Product Owner email address in the right place below.\n&#x2191;&#xA0;Up:&#xA0;Contributing Guide&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Contributor Covenant Code of Conduct&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/Contributing-Development-Guidelines.asciidoc.html","title":"61. Development Guidelines","body":"\n61. Development Guidelines\nAlways ask before creating a pull request. To avoid duplication efforts, its better to discuss it with us first or create an issue.\nAll code must be reviewed via a pull request. Before anything can be merged, it must be reviewed by other developer and ideally at least 2 others.\nUse git flow processes. Start a feature, release, or hotfix branch, and you should never commit and push directly to master.\nCode should adhere to lint and codestyle tests. While you can commit code that doesn&#x2019;t validate but still works, it is encouraged to validate your code. It saves other&#x2019;s headaches down the road.\nCode must pass existing tests when submitting a pull request. If your code breaks a test, it needs to be updated to pass the tests before merging.\nNew code should come with proper tests. Your code should come with proper test coverage, ideally 95%, minimum 80%, before it can be merged.\nBug fixes must come with a test. Any bug fixes should come with an appropriate test to verify the bug is fixed, and does not return.\nCode structure should be maintained. The structure of the repo and files has been carefully crafted, and any deviations from that should be only done when agreed upon by the entire community.\n&#x2190;&#xA0;Previous:&#xA0;Contributor Covenant Code of Conduct&#xA0;| &#x2191;&#xA0;Up:&#xA0;Contributing Guide&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Working with forked repositories&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/Contributing-Git-Fork-Guide.asciidoc.html","title":"62. Working with forked repositories","body":"\n62. Working with forked repositories\n62.1. Fork a repository\nA fork is a copy of a repository. Forking a repository allows you to freely experiment with changes without affecting the original project.\nMost commonly, forks are used to either propose changes to someone else&#x2019;s project or to use someone else&#x2019;s project as a starting point for your own idea.\n62.1.1. Propose changes to someone else&#x2019;s project\nA great example of using forks to propose changes is for bug fixes. Rather than logging an issue for a bug you&#x2019;ve found, you can:\nFork the repository.\nMake the fix.\nSubmit a pull request to the project owner.\nIf the project owner likes your work, they might pull your fix into the original repository!\n62.1.2. How to fork a repository\nGitHub, GitLab and Bitbucket have a very accesible option to fork any repository you can access to. For example, at GitHub you will only need to do the following:\nIn order to work locally you will need to pull your forked repository. Open the Terminal or Git Bash and run the following command:\n$ git clone https://github.com/YOUR-USERNAME/YOUR-REPOSITORY\n62.2. Configuring a remote for a fork\nYou must configure a remote that points to the upstream repository in Git to sync changes you make in a fork with the original repository. This also allows you to sync changes made in the original repository with the fork.\nOpen Terminal or Git Bash.\nList the current configured remote repository for your fork.\n$ git remote -v\norigin https://github.com/YOUR_USERNAME/YOUR_FORK.git (fetch)\norigin https://github.com/YOUR_USERNAME/YOUR_FORK.git (push)\nSpecify a new remote upstream repository that will be synced with the fork.\n$ git remote add upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git\nVerify the new upstream repository you&#x2019;ve specified for your fork.\n$ git remote -v\norigin https://github.com/YOUR_USERNAME/YOUR_FORK.git (fetch)\norigin https://github.com/YOUR_USERNAME/YOUR_FORK.git (push)\nupstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git (fetch)\nupstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git (push)\n62.3. Syncing a fork\nSync a fork of a repository to keep it up-to-date with the upstream repository.\nBefore you can sync your fork with an upstream repository, you must configure a remote that points to the upstream repository in Git.\nOpen Terminal or Git Bash.\nChange the current working directory to your local project.\nFetch the branches and their respective commits from the upstream repository. Commits to master will be stored in a local branch, upstream/master.\n$ git fetch upstream\nremote: Counting objects: 75, done.\nremote: Compressing objects: 100% (53/53), done.\nremote: Total 62 (delta 27), reused 44 (delta 9)\nUnpacking objects: 100% (62/62), done.\nFrom https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY\n* [new branch] master -&gt; upstream/master\nCheck out your fork&#x2019;s local master branch.\n$ git checkout master\nSwitched to branch &apos;master&apos;\nMerge the changes from upstream/master into your local master branch. This brings your fork&#x2019;s master branch into sync with the upstream repository, without losing your local changes.\n$ git merge upstream/master\nUpdating a422352..5fdff0f\nFast-forward\nREADME | 9 -------\nREADME.md | 7 ++++++\n2 files changed, 7 insertions(+), 9 deletions(-)\ndelete mode 100644 README\ncreate mode 100644 README.md\nIf your local branch didn&#x2019;t have any unique commits, Git will instead perform a &quot;fast-forward&quot;:\ngit merge upstream/master\nUpdating 34e91da..16c56ad\nFast-forward\nREADME.md | 5 +++--\n1 file changed, 3 insertions(+), 2 deletions(-)\nPush the changes to update your fork on GitHub, GitLab, Bitbucket, etc.\n&#x2190;&#xA0;Previous:&#xA0;Development Guidelines&#xA0;| &#x2191;&#xA0;Up:&#xA0;Contributing Guide&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Wiki Contributions&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/Contributing-Wiki.asciidoc.html","title":"63. Wiki Contributions","body":"\n63. Wiki Contributions\nContributing guidelines moved here. Feel free to merge over what remains valuable from here. This document will be delted in the future.\nOur wikis are written in the so called AsciiDoc format. Check the AsciiDoc cheatsheet and the AsciiDoc quick reference for more information. Knowing the following basic features should allow you to convert your Word documents into the Wiki friendly AsciiDoc format.\nIt is mandatory to follow the code of conduct that must be present in the root of every OSS or private project as CODE_OF_CONDUCT.asciidoc or CODE_OF_CONDUCT.md.\n63.1. Text styles\nItalic Text\n_Italic Text_\nBold Text\n*Bold Text*\nMono Spaced Text\n+Mono Spaced Text+\nText in Superscript\nText in ^Superscript^\nText in Subscript\nText in ~Subscript~\n63.2. Titles\nA title can be initiated like this:\n[[Contributing-Wiki.asciidoc]]\n= Level 1 header\n[[contributing-wiki.asciidoc_level-2-header]]\n== Level 2 header\n[[contributing-wiki.asciidoc_level-3-header]]\n== Level 3 header\n...\n63.3. Lists\nOrdered and unordered lists can be created like this:\nOrdered list:\n. Item 1\n. Item 2\n. Item 3\n. ...\nUnordered list:\n* Item 1\n* Item 2\n* Item 3\n* ...\n63.4. Tables\nThe following example shows how a table can be created. Note that the header flag is optional.\n[options=&quot;header&quot;]\n|===\n|Header 1|Header 2| Header 3\n| Item 1| Item 2| Item 3\n| ...| ...| ...\n|===\n63.5. Source Code\nIf you want to show off some code examples, you can use the code block:\n[source]\n----\nSome source code\n----\nYou can also specify which script language is used. This will allow GitHUb to use a matching color scheme. Therefore, just type in the type of code used:\n[source, bash]\nor\n[source, java]\n&#x2190;&#xA0;Previous:&#xA0;Working with forked repositories&#xA0;| &#x2191;&#xA0;Up:&#xA0;Contributing Guide&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;OSS Compliance&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/How-to-install.asciidoc.html","title":"49. How to install","body":"\n49. How to install\nThere is one important pre-requisite for Mr Checker installation - there has to be Java installed on the computer and an environmental variable has to be set in order to obtain optimal functioning of the framework.\nInstall Java 1.8 JDK 64bit\nDownload and install Java download link\nWindows Local Environment how to set:\nVariable name: JAVA_HOME | Variable value: c:\\Where_You&#x2019;ve_Installed_Java\nVariable name: PATH | Variable value: %JAVA_HOME%/bin;%JAVA_HOME%\\lib\nVerify in command line:\n&gt; java --version\nMr Checker installation can be done in three ways:\nEasy out of the box installation - Fast and easy solution - recommended for all users, who had previously not used any test automation environment. A drawback of these solutions is that it applies not for all Operation Systems\nOut of the box installation - with additional steps - when the first way is not working for you\nAdvanced installation - Manual step by step installation containing all framework ingredients\n49.1. Easy out of the box installation\nClick on the link Ready to use MrChecker_Test_Environment and download the package\nUnzip downloaded MrChecker Test Framework to the folder C:\\ on your PC - recommended tool: 7z\nAll necessary components, such as Eclipse, Java and Maven will be pre-installed for you. There is no need for additional installations.\nNote: Please double check the place in which you have unzipped MrChecker_Test_Framework\nGo to folder C:\\MrChecker_Test_Framework\\ , in which Mr.Checker has been unzipped\nDouble click on start-eclipse.bat\n. Update project structure (ALT + F5)\nIf the script is not working for you - try:\n49.2. Out of the box installation - with additional steps\nOpen Eclipse\nManually Delete folders that appear in Eclipse\nClick inside Eclipse with a right mouse click and open Import\nSelect Maven &#x2192; existing Maven project\nSelect Mr Checker &#x2192; workspace &#x2192; devon project and click OK\nAt this point, all test catalogues should be imported in Eclipse and ready to use.\n49.3. Advanced manual step by step installation\nInstall each component separately, or update the existing ones on your PC.\nMaven 3.5\nDownload Maven http://www-eu.apache.org/dist/maven/maven-3/3.5.0/binaries/apache-maven-3.5.0-bin.zip\nUnzip Maven in followin location C:\\maven\nSet Windows Local Environment\nVariable name: M2_HOME | Variable value: c:\\maven\nVariable name: PATH | Variable value: %M2_HOME%\\bin\nVerify in command line:\n&gt; mvn --version\nEclipse IDE\nDownload and unzip Eclipse\nDownload Mr Checker Test Framework source code\nImport projects in Eclipse\nImport:\nProjects from folders:\nOpen already created projects\nUpdate project structure - ALT + F5\n&#x2190;&#xA0;Previous:&#xA0;Home&#xA0;| &#x2191;&#xA0;Up:&#xA0;devonfw testing&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Mr Checker Test Framework modules&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/User-Stories.asciidoc.html","title":"53. User Stories","body":"\n53. User Stories\nThe list of user stories, exported from JIRA, can be downloaded from here.\n53.1. Epic: Invite friends\n53.1.1. US: create invite for friends\nEpic: Invite friends\nAs a guest I want to create an dinner event by entering date and time and adding potential guests by their emails so that each potential guests receives a email in order to confirm or decline my invite.\nAcceptance criteria\nonly date and time in future possible and both required\nonly valid email addresses: text@text.xx, one entered email-address is required\nif AGB are not checked, an error message is shown\nafter the invite is done\nI see the confirmation screen of my invite (see wireframe)\nI receive a confirmation email about my invite containing date, time and invited guests\nall guests receive a mail with my invite\n53.1.2. US: create reservation\nEpic: Invite friends\nAs a guest I want to create a reservation by entering date and time and number of adults and kids\nAcceptance criteria\nonly date and time in future possible and both required\nonly valid email addresses: text@text.xx, one entered email-address is required\nif AGB are not checked, an error message is shown\nafter the reservation is done\nI see a confirmation screen of my reservation with datetime, number of persons and kids\nI receive a confirmation email about my reservation\nWireframes\nsee realtimeboard\n53.1.3. US: handle invite\nAs an invited guest I would like to receive an email - after somebody as invited me - with the option to accept or decline the invite so that the system knows about my participation\nAC:\nthe mail contains the following information about the invite\nwho has invited\nwho else is invited\ndate and time of the invite\nbutton to accept or decline\nafter pressing the buttons the system will store the status (yes/no) of my invite\n53.1.4. US: revoke accepted invite\nAs an invited guest I would like to revoke my previous answer in order to inform the system and the inviter about my no showup\nAC:\nthe inviter and myself receives an email about my cancellation\nthe system sets my status of my invite to no\nin case I have placed an order, the order is also removed from the system.\nthe cancellation is only possible 10 minutes before the event takes place. The system shows a message that cancellation is not possible anymore.\n53.1.5. US: calculate best table\nAs a guest I would like the system to check (1 hour before my invite) all my invites and to reserve a table fitting the number of accepted users\nDetails\nPseudo-algorithm for reservation:\nFind table for given date and time where seats of guests &gt;= Count of invited guests plus one. In case no results, decline request and show error message to user. In case of any result, make a reservation for table&#x2026;&#x200B;.\nFor each decline of a guest remove guest and search with reduced number for new table. In case table is found, reserve it and remove reservation from previous table. In case not, do not change reservations.\n53.1.6. US: find table by reservation info\nAs a waiter I would like to search by reference number or email address for the reserved table in order to know the table for my visit. (when arriving at the restaurant)\nAC:\nAfter entering the email the systems shows the number of the table. In case no reservation found, a message is shown.\nEntered email address could be email of inviter or any invited guest.\n53.1.7. US: cancel invite\nEpic: Invite friends\nAs a guests who has sent an invite I want to be able to cancel my previous invite in order to inform the restaurant and my invited guests that I will not show up\nAC:\nthe option to cancel the invite is available in the confirmation-mail about my invite\nafter my cancellation all invited guests receives a mail about the cancelation\nI see a confirmation that my invite was cancelled successfully\nafter my cancelation my invite and reservation and all orders related to it are deleted from the system and no one can accept or decline any invite for it\nthe cancellation is only possible one hour before the invite takes place. After that I am not allowed to cancel it any more.\n53.2. Epic: Digital Menu\n53.2.1. US: filter menu\nAs a guest I want to filter the menu so that I only see the dishes I am interested in\nAC:\nthe guest can filter by\ntype: starter | main dish | dessert; XOR; if nothing is selected all are shown (default value)\nveggy (yes|no|does not matter (default))\nvegan (yes|no|does not matter (default))\nrice (yes|no|does not matter (default))\ncurry (yes|no|does not matter (default))\nnoodle (yes|no|does not matter (default))\nprice (range)\nratings (range)\nmy favorite (yes|no|does not matter (default))&#x2009;&#x2014;&#x2009;free text (search in title and description)\nthe guest can sort by price asc, rating asc\nafter setting the filter only dishes are shown which fulfills those criteria\nby pressing the button reset filter all filter are reset to the initial value\nby pressing the filter button the filter is applied [or is it triggered after each change?]\n53.2.2. US: Define order\nAs a guest I want to define my order by selecting dishes from the menu\nAC:\nThe guest can add each dish to the order\nIn case the guest adds the same dish multiple times, a counter in the order for this dish is increased for this dish\nThe guest can remove the dish from the order\nThe guest can add for each main dish the type of meat (pork, chicken, tofu)\nThe guest can add for each dish a free-text-comment\nAfter adding/removing any dish the price is calculated including VAT\n53.2.3. US: Order the order\nAs a guest I want to order my selected dishes (order)\nAC:\nI receive a mail containing my order with all dishes and the final price\nprecondition for ordering:\nEach order must be associated with a reservation / invite. Without any reference no order could be placed. The reference could be obtained from a previous reservation/invite (created during same session) or by the previous accepted invite (link in email) or by entering the reference id when asked by the system.\nIn case precondition is not fulfilled, the guest is asked\nwhether he/she would like to create a reservation/invite and is forwarded to US Invite Friends. Only after finalizing the reservation the order is accepted.\nor he/she would enter previous created reservation-id he/she knows in order to associate his/her order with this reservation\n53.2.4. US: Cancel order\nAs a guest I want to cancel my order.\nAC:\nin my received confirmation mail I have the option to cancel my order\nthe cancelation is only possible one hour before my reservation takes place\nmy order is deleted from the system\nRemark: Changing the order is not possible. For that the order must be canceled and created from scratch again\n53.2.5. US: Read twitter rating for dishes\nAs a guest I want to read for all dishes the rating done be twitter because I would like to know the opinion of others\nAC:\nFor each dish I see the latest 3 comments done by twitter for this vote (text, username, avatar)\nFor each dish I see the number of likes done by twitter\n53.3. Epic: User Profile\n53.3.1. US: User Profile\nAs a guest I want to have a user profile to associate it with my twitter account to be able to like/rate dishes\nAC:\nUsername of my profile is my email address\nMy profile is protected by password\nI can log in and log out to my profile\nI can reset my password by triggering the reset by mail\nI can associate my profile with my twitter account in order to rate dishes and store my favorites by liking posts associated to dishes\n53.4. Epic: Rate by twitter\n53.4.1. US: Receive mail to rate your dish\nAs a guest I want to receive a mail by the system in order to rate my dish\n53.4.2. US: Rate your dish\nAs a guest I want to add a comment or a like via my twitter account for a dish\nAC:\nBefore I write my rate I would like to be able to read all tweets of other users for this dish\nI would like to see the number of likes for a dish\n53.5. Epic: Waiter Cockpit\n53.5.1. US: See all orders/reservations\nAs a waiter I want to see all orders/reservation in order to know what is going on in my restaurant\nAC:\nall orders/reservations are shown in a list view (read-only). Those list can be filtered and sorted (similar to excel-data-filters)\norders/reservations are shown in separate lists.\nfor each order the dish, meat, comment, item, reservation-id, reservation datetime, creation-datetime is shown\nfor each reservation the inviters email, the guests-emails, the number of accepts and declines, calculated table number, the reservation-id, reservation date-time and creation-datetime are shown\nthe default filter for all lists is the todays date for reservation datetime. this filter can be deleted.\nonly reservations and orders with reservation date in the future shall be available in this view. All other orders and reservation shall not be deleted; for data analytics those orders and reservation shall still exist in the system.\nchecklist:\ntalk about:\nwho?\nwhat?\nwhy (purpose)\nwhy (objective)\nwhat happens outside the software\nwhat might go wrong\nany question or assumptions (write them down) , DoR should check that those sections are empty.\nis there any better solution?\nhow (technical perspective)\ndo a rough estimate\ncheck INVEST\n&#x2190;&#xA0;Previous:&#xA0;1.\tMy Thai Star &#x2013; Agile Framework&#xA0;| &#x2191;&#xA0;Up:&#xA0;MyThaiStar&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Technical design&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/agile.asciidoc.html","title":"52. 2.\tMy Thai Star &#x2013; Agile Diary","body":"\n52. 2.\tMy Thai Star &#x2013; Agile Diary\nIn parallel to the Diary Ideation we use this Agile Diary to document our Scrum events. The target of this diary is to describe the differences to the Scrum methodology as well as specific characteristics of the project. We also document the process on how we approach the Scrum methodology over the length of the project.\n52.1. 24.03.2017 Sprint 1 Planning\nWithin the Sprint 1 Planning we used planning poker.com for the estimation of the user stories. The estimation process usually is part of the backlog refinement meeting. Regarding the project circumstances we decided to estimate the user stories during the Sprint Planning. Starting the estimation process we noticed that we had to align our interpretation of the estimation effort as these story points are not equivalent to a certain time interval. The story points are relative values to compare the effort of the user stories. With this in mind we proceeded with the estimation of the user stories. We decided to start Sprint 1 with the following user stories and the total amount of 37 story points:\n&#x2022;\tICSDSHOW-2\tCreate invite for friends\t(8 Story Points)\n&#x2022;\tICSDSHOW-4\tCreate reservation\t\t(3)\n&#x2022;\tICSDSHOW-5\tHandle invite\t\t\t(3)\n&#x2022;\tICSDSHOW-6\tRevoke accepted invite \t(5)\n&#x2022;\tICSDSHOW-9\tCancel invite\t\t\t(3)\n&#x2022;\tICSDSHOW-11\tFilter menu\t\t\t(5)\n&#x2022;\tICSDSHOW-12\tDefine order\t\t\t(5)\n&#x2022;\tICSDSHOW-13\tOrder the order\t\t(5)\nAs the Sprint Planning is time boxed to one hour we managed to hold this meeting within this time window.\n52.2. 27.04.2017 Sprint 1 Review\nDuring the Sprint 1 Review we had a discussion about the data model proposal. For the discussion we extended this particular Review meeting to 90min. As this discussion took almost 2/3 of the Review meeting we only had a short time left for our review of Sprint 1. For the following scrum events we decided to focus on the primary target of these events and have discussions needed for alignments in separate meetings.\nRegarding the topic of splitting user stories we had the example of a certain user story which included a functionality of a twitter integration (ICSDSHOW-17 User Profile and Twitter integration). As the twitter functionality could not have been implemented at this early point of time we thought about cutting the user story into two user stories. We aligned on mocking the twitter functionality until the dependencies are developed in order to test the components. As this user story is estimated with 13 story points it is a good example for the question whether to cut a user story into multiple user stories or not.\nUnfortunately not all user stories of Sprint 1 could have been completed. Due this situation we discussed on whether pushing all unfinished user stories into the status done or moving them to Sprint 2. We aligned on transferring the unfinished user stories into the next Sprint. During the Sprint 1 the team underestimated that a lot of holidays crossed the Sprint 1 goals. As taking holidays and absences of team members into consideration is part of a Sprint Planning we have a learning effect on setting a Sprint Scope.\n52.3. 03.05.2017 Sprint 2 Planning\nAs we aligned during the Sprint 1 Review on transferring unfinished user stories into Sprint 2 the focus for Sprint 2 was on finishing these transferred user stories. During our discussion on how many user stories we could work on in Sprint 2 we needed to remind ourselves that the overall target is to develop an example application for the DevonFW. Considering this we aligned on a clear target for Sprint 2: To focus on finishing User Stories as we need to aim for a practicable and realizable solution. Everybody aligned on the aim of having a working application at the end of Sprint 2.\nFor the estimation process of user stories we make again usage of planningpoker.com as the team prefers this &#x201C;easy-to-use&#x201D; tool. During our second estimation process we had the situation in which the estimated story points differs strongly from one team member to another. In this case the team members shortly explains how the understood and interpreted the user story. It turned out that team members misinterpreted the user stories. With having this discussion all team members got the same understanding of the specific functionality and scope of a user story. After the alignment the team members adjusted their estimations.\nBeside this need for discussion the team estimated most of the user stories with very similar story points. This fact shows the increase within the effort estimation for each team member in comparison to Sprint 1 planning. Over the short time of two Sprint planning the team received a better understanding and feeling for the estimation with story points.\n52.4. 01.06.2017 Sprint 2 Review\nAs our Sprint 1 Review four weeks ago was not completely structured like a Sprint Review meeting we focused on the actual intention of a Sprint Review meeting during Sprint 2 Review. This means we demonstrated the completed and implemented functionalities with screen sharing and the product owner accepted the completed tasks.\nWithin the User Story ICSDSHOW-22 &#x201C;See all orders/reservations&#x201D; the functionality &#x201C;filtering the list by date&#x201D; could have not been implemented during Sprint 2. The team was unsure on how to proceed with this task. One team member added that especially in regards of having a coherent release, implementing less but working functionalities is much better than implementing more but not working functionalities. For this the team reminded itself focusing on completing functionalities and not working straight to a working application.\n&#x2190;&#xA0;Previous:&#xA0;1.\tMy Thai Star &#x2013; Agile Framework&#xA0;| &#x2191;&#xA0;Up:&#xA0;MyThaiStar&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;User Stories&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/devon4node-architecture.asciidoc.html","title":"28. devon4node Architechture","body":"\n28. devon4node Architechture\nAs we mention in the introduction, devon4node is based on NestJS. Nest (NestJS) is a framework for building efficient, scalable Node.js server-side applications.\n28.1. HTTP layer\nBy using NestJS, devon4node is a platform-agnostic framework. NestJS focuses only on the logical layer, and delegates the transport layer to another framework, such as ExpressJS. You can see it in the following diagram:\nAs you can see, NestJS do not listen directly for incoming request. It has an adapter to communicate with ExpressJS and ExpressJS is the responsible for that. ExpressJS is only one of the frameworks that NestJS can work with. We have also another adapter available out-of-the-box: the Fastify adapter. With that, you can replace ExpressJS for Fastify But you can still use all your NestJS components. You can also create your own adapter to make NestJS work with other HTTP framework.\nAt this point, you may think: why is NestJS (and devon4node) using ExpressJS by default instead of Fastify? Because, as you can see in the previous diagram, there is a component that is dependent on the HTTP framework: the middlewares. As ExpressJS is the most widely used framework, there are a lot of middlewares for it, so, in order to reuse them in our NestJS applications, NestJS use ExpressJS by default. Anyway, you may think which HTTP framework best fits your requirements.\n28.2. devon4node layers\nAs other devonfw technologies, devon4node separates the application into layers.\nThose layers are:\nController layer\nService layer\nData Access layer\n28.3. devon4node application structure\nAlthough there are many frameworks to create backend applications in NodeJS, none of them effectively solve the main problem of - Architecture. This is the main reason we have chosen NestJS for the devon4node applications. Besides, NestJS is highly inspired by Angular, therefore a developer who knows Angular can use his already acquired knowledge to write devon4node applications.\nNestJS adopts various Angular concepts, such as dependency injection, piping, interceptors and modularity, among others. By using modularity we can reuse some of our modules between applications. One example that devon4node provide is the mailer module.\n28.3.1. Modules\nCreate a application module is simple, you only need to create an empty class with the decorator Module:\n@Module({})\nexport class AppModule {}\nIn the module you can define:\nImports: the list of imported modules that export the providers which are required in this module\nControllers: the set of controllers defined in this module which have to be instantiated\nProviders: the providers that will be instantiated by the Nest injector and that may be shared at least across this module\nExports: the subset of providers that are provided by this module and should be available in other modules which import this module\nThe main difference between Angular and NestJS is NestJS modules encapsulates providers by default. This means that it&#x2019;s impossible to inject providers that are neither directly part of the current module nor exported from the imported modules. Thus, you may consider the exported providers from a module as the module&#x2019;s public interface, or API. Example of modules graph:\nIn devon4node we three different kind of modules:\nAppModule: this is the root module. Everything that our application need must be imported here.\nGlobal Modules: this is a special kind of modules. When you make a module global, it&#x2019;s accesible for every module in your application. Your can see it in the next diagram. It&#x2019;s the same as the previous one, but now the CoreModule is global:\nOne example of global module is the CoreModule. In the CoreModule you must import every module which have providers that needs to be accesible in all modules of you application\nFeature (or application) modules: modules which contains the logic of our application. We must import it in the AppModule.\nFor more information about modules, see NestJS documentation page\n28.3.2. Folder structure\ndevon4node defines a folder structure that every devon4node application must follow. The folder structure is:\n&#x251C;&#x2500;&#x2500;&#x2500;src\n&#x2502; &#x251C;&#x2500;&#x2500;&#x2500;app\n&#x2502; &#x2502; &#x251C;&#x2500;&#x2500;&#x2500;core\n&#x2502; &#x2502; &#x2502; &#x251C;&#x2500;&#x2500;&#x2500;auth\n&#x2502; &#x2502; &#x2502; &#x251C;&#x2500;&#x2500;&#x2500;configuration\n&#x2502; &#x2502; &#x2502; &#x251C;&#x2500;&#x2500;&#x2500;user\n&#x2502; &#x2502; &#x2502; &#x2514;&#x2500;&#x2500;&#x2500;core.module.ts\n&#x2502; &#x2502; &#x251C;&#x2500;&#x2500;&#x2500;shared\n&#x2502; &#x2502; &#x2514;&#x2500;&#x2500;&#x2500;feature\n&#x2502; &#x2502; &#x251C;&#x2500;&#x2500;&#x2500;sub-module\n&#x2502; &#x2502; &#x2502; &#x251C;&#x2500;&#x2500;&#x2500;controllers\n&#x2502; &#x2502; &#x2502; &#x251C;&#x2500;&#x2500;&#x2500;...\n&#x2502; &#x2502; &#x2502; &#x251C;&#x2500;&#x2500;&#x2500;services\n&#x2502; &#x2502; &#x2502; &#x2514;&#x2500;&#x2500;&#x2500;sub-module.module.ts\n&#x2502; &#x2502; &#x251C;&#x2500;&#x2500;&#x2500;controllers\n&#x2502; &#x2502; &#x251C;&#x2500;&#x2500;&#x2500;interceptors\n&#x2502; &#x2502; &#x251C;&#x2500;&#x2500;&#x2500;pipes\n&#x2502; &#x2502; &#x251C;&#x2500;&#x2500;&#x2500;guards\n&#x2502; &#x2502; &#x251C;&#x2500;&#x2500;&#x2500;filters\n&#x2502; &#x2502; &#x251C;&#x2500;&#x2500;&#x2500;middlewares\n&#x2502; &#x2502; &#x251C;&#x2500;&#x2500;&#x2500;model\n&#x2502; &#x2502; &#x2502; &#x251C;&#x2500;&#x2500;&#x2500;dto\n&#x2502; &#x2502; &#x2502; &#x2514;&#x2500;&#x2500;&#x2500;entities\n&#x2502; &#x2502; &#x251C;&#x2500;&#x2500;&#x2500;services\n&#x2502; &#x2502; &#x2514;&#x2500;&#x2500;&#x2500;feature.module.ts\n&#x2502; &#x251C;&#x2500;&#x2500;&#x2500;config\n&#x2502; &#x2514;&#x2500;&#x2500;&#x2500;migration\n&#x251C;&#x2500;&#x2500;&#x2500;test\n&#x2514;&#x2500;&#x2500;&#x2500;package.json\ndevon4node CLI ensures this folder structure so, please, do not create files by your own, use the devon4node CLI.\n28.3.3. NestJS components\nNestJS provides several components that you can use in your application:\nControllers\nProviders\nMiddleware\nGuards\nInterceptors\nPipes\nException filters\nIn the NestJS documentation you can find all information about each component. But, something that is missing in the documentation is the execution order. Every component can be defined in diferent levels: globally, in the controller or in the handler. As middleware is part of the HTTP server we can define it in a different way: globally or in the module.\nIt is not neccesary to have defined components in every level. For example, you can have defined a interceptor gobally but you do not have any other in the controller or handler level. If nothing is defined in some level, the request will continue to the next component.\nAs you can see in the previous image, the first component which receive the request is the global defined middleware. Then, it send the request to the module middleware. Each of them can return a response to the client, without passing the request to the next level.\nThen, the request continue to the guards: first the global guard, next to controller guard and finally to the handler guard. At this point, we can throw an exception in all components and the exception filter will catch it and send a proper error message to the client. We do not paint the filters in the graphic in order to simplify it.\nAfter the guards, is time to interceptors: global interceptors, crontroller interceptors and handler interceptors. And last, before arrive to the handler inside the controller, the request pass throught the pipes.\nWhen the handler has the response ready to send to the client, it does not go directly to the client. It come again to the interceptors, so we can also intercept the response. The order this time is the reverse: handler interceptors, controller interceptors and global interceptors. After that, we can finally send the reponse to the client.\nNow, with this in mind, you are able to create the components in a better way.\n&#x2191;&#xA0;Up:&#xA0;devon4node&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Layers&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/dsf-how-to-use.asciidoc.html","title":"33. How to use it","body":"\n33. How to use it\nThis is the documentation about shop floor and its different tools. Here you are going to learn how to create new projects, so that they can include continuous integration and continuous delivery processes, and be deployed automatically in different environments.\n33.1. Prerequisites - Provisioning environment\nTo start working you need to have some services running in your provisioning environment, such as Jenkins (automation server), GitLab (git repository), SonarQube (program analysis), Nexus (software repository) or similar.\nTo host those services we recommend to have a Production Line instance but you can use other platforms. Here is the list for the different options:\nProduction Line.\ndsf4docker.\n33.2. Step 1 - Configuration and services integration\nThe first step is configuring your services and integrate them with jenkins. Here you have an example about how to manually configure the next services:\nNexus.\nSonarQube.\n33.3. Step 2 - Create the project\n33.3.1. Create and integrate git repository\nThe second is create or git repository and integrate it with Jenkins.\nHere you can find a manual guide about how it:\nGitLab new project.\n33.3.2. Start new devonfw project\nIt is time to create your devonfw project:\nYou can find all that you need about how to create a new devonfw project\n33.3.3. cicd configuration\nNow you need to add cicd files in your project.\nManual configuration\nJenkinsfile\nHere you can find all that you need to know to do your Jenkinsfile.\nDockerfile\nHere you can find all that you need to know to do your Dockerfile.\nAutomatic configuration\ncicdgen\nIf you are using production line for provisioning you could use cicdgen to configure automatically almost everything explained in the manual configuration. To do it see the cicdgen documentation.\n33.4. Step 3 - Deployment\nThe third is configure our deployment environment. Here is the list for the different options:\ndsf4openshift.\n33.5. Step 4 - Monitoring\nHere you can find information about tools for monitoring:\nbuild monitor view for Jenkins. With this tool you will be able to see in real time what is the state of your Jenkins pipelines.\n&#x2190;&#xA0;Previous:&#xA0;What is devonfw shop floor?&#xA0;| &#x2191;&#xA0;Up:&#xA0;devonfw shop floor&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Provisioning environments&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/general-contributing.asciidoc.html","title":"XII. Contributing Guide","body":"\nXII. Contributing Guide\nCode Contributions\nContributor Covenant Code of Conduct\nDevelopment Guidelines\nWorking with forked repositories\nWiki Contributions\nOSS Compliance\n&#x2190;&#xA0;Previous:&#xA0;CI/CD&#xA0;| &#x2191;&#xA0;Up:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Code Contributions&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/general-contributing.asciidoc_oss-compliance.html","title":"64. OSS Compliance","body":"\n64. OSS Compliance\n64.1. Cookbook OSS Compliance\nThis chapter helps you to gain transparency on OSS usage and reach OSS compliance in your project.\n64.1.1. Preface\ndevonfw, as most Java software, makes strong use of Open Source Software (OSS). It is using about 150 OSS products on the server only and on the client even more. Using a platform like devonfw to develop your own custom solution requires handling contained OSS correctly, i.e acting OSS-compliant.\nPlease read the Open Source policy of your company first, e.g. the Capgemini OSS Policy which contains a short, comprehensive and well written explanation on relevant OSS-knowledge. Make sure you:\nunderstand the copyleft effect and its effect in commercial projects\nunderstand the 3 license categories: &quot;permissive&quot;, &quot;weak copyleft&quot; and &quot;strong copyleft&quot;\nknow prominent license types as e.g. &quot;Apache-2.0&quot; or GPL-3.0&quot; and what copyleft-category they are in\nare aware that some OSS offer dual/multi-licenses\nUnderstand that OSS libraries often come with sub-dependencies of other OSS carying licenses themselfes\nTo define sufficient OSS compliance measures, contact your IP officer or legal team as early as possible, especially if you develop software for clients.\n64.1.2. Obligations when using OSS\nIf you create a custom solution containing OSS, this in legal sense is a &quot;derived&quot; work. If you distribute your derived work to your business client or any other legal entity in binary packaged form, the license obligations of contained OSS get into effect. Ignoring these leads to a license infringement which can create high damage.\nTo carefully handle these obligations you must:\nmaintain an OSS inventory (to gain transparency on OSS usage and used licenses)\ncheck license conformity depending on usage/distribution in a commercial scenario\ncheck license compatibility between used OSS-licenses\nfulfill obligations defined by the OSS-licenses\nObligations need to be checked per license. Frequent obligations are:\ndeliver the license terms of all used versions of the OSS licenses\nnot to change any copyright statements or warranty exclusions contained in the used OSS components\ndeliver the source code of the OSS components (e.g. on a data carrier)\nwhen modifying OSS, track any source code modification (including date and name of the employee/company)\ndisplay OSS license notice in a user frontend (if any)\nother obligations depending on individual license\n64.1.3. Automate OSS handling\nCarefully judging the OSS usage in your project is a MANUAL activity! However, collecting OSS information and fulfilling license obligations should be automated as much as possible. A prominent professional tool to automate OSS compliance is the commercial software &quot;Blackduck&quot;. Unfortunately it is rather expensive - either purchased or used as Saas.\nThe most recommended lightweight tooling is a combination of Maven plugins. We will mainly use the Mojo Maven License Plugin.\n64.1.4. Configure the Mojo Maven License Plugin\nYou can use it from commandline but this will limit the ability to sustainably configure it (shown later).\nTherefore we add it permanently as a build-plugin to the project parent-pom like this (already contained in OASP-parent-pom):\n&lt;plugin&gt;\n&lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;\n&lt;artifactId&gt;license-maven-plugin&lt;/artifactId&gt;\n&lt;version&gt;1.14&lt;/version&gt;\n&lt;configuration&gt;\n&lt;outputDirectory&gt;${project.build.directory}/generated-resources&lt;/outputDirectory&gt;\n&lt;sortArtifactByName&gt;true&lt;/sortArtifactByName&gt;\n&lt;includeTransitiveDependencies&gt;true&lt;/includeTransitiveDependencies&gt;\n&lt;!-- the &quot;missing file&quot; declares licenses for dependencies that could not be detected automatically --&gt;\n&lt;useMissingFile&gt;true&lt;/useMissingFile&gt;\n&lt;!-- find the &quot;missing files&quot; in all child-projects at the following location --&gt;\n&lt;missingFile&gt;src/license/THIRD-PARTY.properties&lt;/missingFile&gt;\n&lt;!-- if the &quot;missing files&quot; are not yet existing in child-projects they will be created automatically --&gt;\n&lt;failOnMissing&gt;false&lt;/failOnMissing&gt;\n&lt;overrideFile&gt;src/license/override-THIRD-PARTY.properties&lt;/overrideFile&gt;\n&lt;!-- harmonize different ways of writing license names --&gt;\n&lt;licenseMerges&gt;\n&lt;licenseMerge&gt;Apache-2.0|Apache 2.0&lt;/licenseMerge&gt;\n&lt;licenseMerge&gt;Apache-2.0|Apache License, Version 2.0&lt;/licenseMerge&gt;\n&lt;licenseMerge&gt;Apache-2.0|Apache Software License, Version 2.0&lt;/licenseMerge&gt;\n&lt;licenseMerge&gt;Apache-2.0|The Apache Software License, Version 2.0&lt;/licenseMerge&gt;\n&lt;/licenseMerges&gt;\n&lt;encoding&gt;utf-8&lt;/encoding&gt;\n&lt;/configuration&gt;\n&lt;/plugin&gt;\nIn the config above there are several settings that help to permanently improve the result of an automated OSS scan. We explain these now.\nDeclare additional licenses\nSometimes the licenses of used OSS cannot be resolved automatically. That is not the mistake of the maven-license-tool, but the mistake of the OSS author who didn&#x2019;t make the respective license-information properly available.\nDeclare additional licenses in a &quot;missing file&quot; within each maven-subproject: /src/license/THIRD-PARTY.properties.\n# Generated by org.codehaus.mojo.license.AddThirdPartyMojo\n#-------------------------------------------------------------------------------\n# Already used licenses in project :\n# - ASF 2.0\n# - Apache 2\n...\n#-------------------------------------------------------------------------------\n# Please fill the missing licenses for dependencies :\n...\ndom4j--dom4j--1.6.1=BSD 3-Clause\njavax.servlet--jstl--1.2=CDDL\n...\nIn case the use of &quot;missing files&quot; is activated, but the THIRD-PARTY.properties-file is not yet existing, the first run of an &quot;aggregate-add-third-party&quot; goal (see below) will fail. Luckily the license-plugin just helped us and created the properties-files automatically (in each maven-subproject) and prefilled it with:\na list of all detected licenses within the maven project\nall OSS libraries where a license could not be detected automatically.\nYou now need to fill in missing license information and rerun the plugin.\nRedefine wrongly detected licenses\nIn case automatically detected licenses proof to be wrong by closer investigation, this wrong detection can be overwritten. Add a configuration to declare alternative licenses within each maven-subproject: /src/license/override-THIRD-PARTY.properties\ncom.sun.mail--javax.mail--1.5.6=Common Development and Distribution License 1.1\nThis can be also be useful for OSS that provides a multi-license to make a decision which license to actually choose .\nMerge licenses\nYou will see that many prominent licenses come in all sorts of notations, e.g. Apache-2.0 as: &quot;Apache 2&quot; or &quot;ASL-2.0&quot; or &quot;The Apache License, Version 2.0&quot;. The Mojo Maven License Plugin allows to harmonize different forms of a license-naming like this:\n&lt;!-- harmonize different ways of writing license names --&gt;\n&lt;licenseMerges&gt;\n&lt;licenseMerge&gt;Apache-2.0|Apache 2.0&lt;/licenseMerge&gt;\n&lt;licenseMerge&gt;Apache-2.0|Apache License, Version 2.0&lt;/licenseMerge&gt;\n&lt;licenseMerge&gt;Apache-2.0|Apache Software License, Version 2.0&lt;/licenseMerge&gt;\n&lt;licenseMerge&gt;Apache-2.0|The Apache Software License, Version 2.0&lt;/licenseMerge&gt;\n&lt;/licenseMerges&gt;\nLicense-names will be harmonized in the OSS report to one common term. We propose to harmonize to short-license-IDs defined by the SPDX standard.\n64.1.5. Retrieve licenses list\nFor a quick initial judgement of OSS license situation run the following maven command from commandline:\n$ mvn license:license-list\nYou receive the summary list of all used OSS licenses on the cmd-out.\n64.1.6. Create an OSS inventory\nTo create an OSS inventory means to report on the overall bill of material of used OSS and corresponding licenses.\nWithin the parent project, run the following maven goal from command line.\n$ mvn license:aggregate-download-licenses -Dlicense.excludedScopes=test,provided\nRunning the aggregate-download-licenses goal creates two results.\na license.xml that contains all used OSS depenencies (even sub-dependencies) with respective license information\nputs all used OSS-license-texts as html files into folder target/generated resources\nCarefully validate and judge the outcome of the license list. It is recommended to copy the license.xml to the project documentation and hand it over to your client. You may also import it into a spreadsheet to get a better overview.\n64.1.7. Create a THIRD PARTY file\nWithin Java software it is a common practice to add a &quot;THIRD-PARTY&quot; text file to the distribution. Contained is a summary-list of all used OSS and respective licenses. This can also be achieved with the Mojo Maven License Plugin.\nWithin the parent project, run the following maven goal from command line.\n$ mvn license:aggregate-add-third-party -Dlicense.excludedScopes=test,provided\nFind the THIRD-PARTY.txt in the folder: target\\generated-resources. The goal aggregate-add-third-party also profits from configuration as outlined above.\n64.1.8. Download and package OSS SourceCode\nSome OSS licenses require handing over the OSS source code which is packaged with your custom software to the client the solution is distributed to. It is a good practice to hand over the source code of all used OSS to your client. Collecting all source code can be accomplished by another Maven plugin: Apache Maven Dependency Plugin.\nIt downloads all OSS Source Jars into the folder: \\target\\sources across the parent and all child maven projects.\nYou configure the plugin like this:\n&lt;plugin&gt;\n&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n&lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;\n&lt;version&gt;3.0.2&lt;/version&gt;\n&lt;configuration&gt;\n&lt;classifier&gt;sources&lt;/classifier&gt;\n&lt;failOnMissingClassifierArtifact&gt;false&lt;/failOnMissingClassifierArtifact&gt;\n&lt;outputDirectory&gt;${project.build.directory}/sources&lt;/outputDirectory&gt;\n&lt;/configuration&gt;\n&lt;executions&gt;\n&lt;execution&gt;\n&lt;id&gt;src-dependencies&lt;/id&gt;\n&lt;phase&gt;package&lt;/phase&gt;\n&lt;goals&gt;\n&lt;!-- use unpack-dependencies instead if you want to explode the sources --&gt;\n&lt;goal&gt;copy-dependencies&lt;/goal&gt;\n&lt;/goals&gt;\n&lt;/execution&gt;\n&lt;/executions&gt;\n&lt;/plugin&gt;\nYou run the plugin from commandline like this:\n$ mvn dependency:copy-dependencies -Dclassifier=sources\nThe plugin provides another goal that also unzips the jars, which is not recommended, since contents get mixed up.\nDeliver the OSS source jars to your client with the release of your custom solution. This has been done physically - e.g. on DVD.\n64.1.9. Handle OSS within CI-process\nTo automate OSS handling in the regular build-process (which is not recommended to start with) you may declare the following executions and goals in your maven-configuration:\n&lt;plugin&gt;\n...\n&lt;executions&gt;\n&lt;execution&gt;\n&lt;id&gt;aggregate-add-third-party&lt;/id&gt;\n&lt;phase&gt;generate-resources&lt;/phase&gt;\n&lt;goals&gt;\n&lt;goal&gt;aggregate-add-third-party&lt;/goal&gt;\n&lt;/goals&gt;\n&lt;/execution&gt;\n&lt;execution&gt;\n&lt;id&gt;aggregate-download-licenses&lt;/id&gt;\n&lt;phase&gt;generate-resources&lt;/phase&gt;\n&lt;goals&gt;\n&lt;goal&gt;aggregate-download-licenses&lt;/goal&gt;\n&lt;/goals&gt;\n&lt;/execution&gt;\n&lt;/executions&gt;\n&lt;/plugin&gt;\nNote that the build may fail in case the OSS information was not complete. Check the build-output to understand and resolve the issue - like e.g. add missing license information in the &quot;missing file&quot;.\n&#x2190;&#xA0;Previous:&#xA0;Wiki Contributions&#xA0;| &#x2191;&#xA0;Up:&#xA0;Contributing Guide&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Release Notes&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/general-release-notes.asciidoc.html","title":"XIII. Release Notes","body":"\nXIII. Release Notes\ndevonfw Release notes 3.2 &#x201C;Homer&#x201D;\ndevonfw Release notes 3.1 &#x201C;Goku&#x201D;\ndevonfw Release notes 3.0 &#x201C;Fry&#x201D;\ndevonfw Release notes 2.4 &#x201C;EVE&#x201D;\ndevonfw Release notes 2.3 &quot;Dash&quot;\ndevonfw Release notes 2.2 &quot;Courage&quot;\nRelease notes devonfw 2.1.1 &quot;Balu&quot;\n&#x2190;&#xA0;Previous:&#xA0;OSS Compliance&#xA0;| &#x2191;&#xA0;Up:&#xA0;devonfw guide&#xA0;| Next:&#xA0;devonfw Release notes 3.2 &#x201C;Homer&#x201D;&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/getting-started-download-and-setup.asciidoc.html","title":"3. Download and Setup","body":"\n3. Download and Setup\nIn this section, you will learn how to setup the devonfw environment and start working on first project based on devonfw.\nThe devonfw environment contains all software and tools necessary to develop the applications with devonfw.\n3.1. Prerequisites\nIn order to setup the environment, following are the prerequisites:\nInternet connection (including details of your proxy configuration, if necessary)\n&gt; 1GB of free disk space to install customized environment\nThe devonfw IDE CLI installed\n3.2. Download\nImportant\nPlease refer to the devonfw IDE documentation for the current installation process.\nOlder devonfw distributions can be obtained from the FTP releases library and are packaged in a zip file that includes all the needed tools, software and configurations. Browse to the corresponding version folder in order to get the latest version.\n3.2.1. Setup the workspace for older versions\nUnzip the devonfw distribution into a directory of your choice. The path to the devonfw distribution directory should contain no spaces, to prevent problems with some of the tools.\nRun the batch file &quot;create-or-update-workspace.bat&quot;.\nThis will configure the included tools like Eclipse with the default settings of the devonfw distribution.\nThe result should be as seen below\nThe working devonfw environment is ready!!!\nNote : If you use a proxy to connect to the Internet, you have to manually configure it in Maven, Sencha Cmd and Eclipse. Next section explains about it.\n3.2.2. Setup the workspace for older versions (Linux)\nUnzip the devonfw distribution into a directory of your choice. The path to the devonfw distribution directory should contain no spaces, to prevent problems with some of the tools.\nRun the script: . env.sh\nRun the script: . create-or-update-workspace\nThese both . env.sh and . create-or-update-workspace will set all the softwares path that included with devon distribution like eclipse, maven , java etc. Also this will generate some file like eclipse_main used to invoke eclipse\nFor vscode setup we have to execute create-or-update-workspace-vs\nThere are a scripts initialize.sh and uninstallUI.sh.\ninitialize.sh : It installs angular,node, python ant subversion\nuninstallUI.sh : It is use to uninstalls the above softwares\n&#x2190;&#xA0;Previous:&#xA0;Why should I use devonfw?&#xA0;| &#x2191;&#xA0;Up:&#xA0;Getting Started&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;The Devon IDE&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/getting-started-the-devon-ide.asciidoc.html","title":"4. The Devon IDE","body":"\n4. The Devon IDE\n4.1. Introduction\nThe Devon IDE is the general name for two distinct versions of a customized Eclipse which comes in a Open Source variant, called devon-ide, and a more extended version included in the &quot;Devon Dist&quot; which is only available for Capgemini engagements.\n4.1.1. Features and advantages\ndevonfw comes with a fully featured IDE in order to simplify the installation, configuration and maintenance of this instrumental part of the development environment. As it is being included in the distribution, the IDE is ready to be used and some specific configuration of certain plugins only takes a few minutes.\nAs with the remainder of the distribution, the advantage of this approach is that you can have as many instances of the -ide &quot;installed&quot; on your machine for different projects with different tools, tool versions and configurations. No physical installation and no tweaking of your operating system required. &quot;Installations&quot; of the Devon distribution do not interfere with each other nor with other installed software.\n4.1.2. Multiple Workspaces\nThere is inbuilt support for working with different workspaces on different branches. Create and update new workspaces with a few clicks. You can see the workspace name in the title-bar of your IDE so you do not get confused and work on the right branch.\n4.2. Cobigen\nIn the Devon distribution we have a code generator to create CRUD code, called Cobigen. This is a generic incremental generator for end to end code generation tasks, mostly used in Java projects. Due to a template-based approach, CobiGen generates any set of text-based documents and document fragments.\nCobigen is distributed in the Devon distribution as an Eclipse plugin, and is available to all Devon developers for Capgemini engagements. Due to the importance of this component and the scope of its functionality, it is fully described here.\n4.3. IDE Plugins:\nSince an application&#x2019;s code can greatly vary, and every program can be written in lots of ways without being semantically different, IDE comes with pre-installed and pre-configured plugins that use some kind of a probabilistic approach, usually based on pattern matching, to determine which pieces of code should be reviewed. These hints are a real time-saver, helping you to review incoming changes and prevent bugs from propagating into the released artifacts. Apart from Cobigen mentioned in the previous paragraph, the IDE provides CheckStyle, SonarQube, FindBugs and SOAP-UI. Details of each can be found in subsequent sections.\n4.3.1. CheckStyle\nWhat is CheckStyle\nCheckStyle is a Open Source development tool to help you ensure that your Java code adheres to a set of coding standards. Checkstyle does this by inspecting your Java source code and pointing out items that deviate from a defined set of coding rules.\nWith the Checkstyle IDE Plugin, your code is constantly inspected for coding standard deviations. Within the Eclipse workbench, you are immediately notified with the problems via the Eclipse Problems View and source code annotations similar to compiler errors or warnings.\nThis ensures an extremely short feedback loop right at the developers fingertips.\nWhy use CheckStyle\nIf your development team consists of more than one person, then obviously a common ground for coding standards (formatting rules, line lengths etc.) must be agreed upon - even if it is just for practical reasons to avoid superficial, format related merge conflicts.\nCheckstyle Plugin helps you define and easily apply those common rules.\nThe plugin uses a project builder to check your project files with Checkstyle. Assuming the IDE Auto-Build feature is enabled, each modification of a project file will immediately get checked by Checkstyle on file save - giving you immediate feedback about the changes you made. To use a simple analogy, the Checkstyle Plug-in works very much like a compiler but instead of producing .class files, it produces warnings where the code violates Checkstyle rules. The discovered deviations are accessible in the Eclipse Problems View, as code editor annotations and via additional Checkstyle violations views.\nInstallation of CheckStyle\nAfter IDE installation, IDE provides default checkstyle configuration file which has certain check rules specified .\nThe set of rules used to check the code is highly configurable. A Checkstyle configuration specifies which check rules are validated against the code and with which severity violations will be reported. Once defined a Checkstyle configuration can be used across multiple projects. The IDE comes with several pre-defined Checkstyle configurations.\nYou can create custom configurations using the plugin&#x2019;s Checkstyle configuration editor or even use an existing Checkstyle configuration file from an external location.\nYou can see violations in your workspace as shown in below figure.\nUsage\nSo, once projects are created, follow steps mentioned below, to activate checkstyle:\nOpen the properties of the project you want to get checked.\nSelect the Checkstyle section within the properties dialog .\nActivate Checkstyle for your project by selecting the Checkstyle active for this project check box and press OK\nNow Checkstyle should begin checking your code. This may take a while depending on how many source files your project contains.\nThe Checkstyle Plug-in uses background jobs to do its work - so while Checkstyle audits your source files you should be able to continue your work.\nAfter Checkstyle has finished checking your code please look into your Eclipse Problems View.\nThere should be some warnings from Checkstyle. This warnings point to the code locations where your code violates the preconfigured Checks configuration.\nYou can navigate to the problems in your code by double-clicking the problem in you problems view.\nOn the left hand side of the editor an icon is shown for each line that contains a Checkstyle violation. Hovering with your mouse above this icon will show you the problem message.\nAlso note the editor annotations - they are there to make it even easier to see where the problems are.\n4.3.2. FindBugs\nWhat is FindBugs\nFindBugsis an open source project for a static analysis of the Java bytecode to identify potential software bugs. Findbugs provides early feedback about potential errors in the code.\nWhy use FindBugs\nIt scans your code for bugs, breaking down the list of bugs in your code into a ranked list on a 20-point scale. The lower the number, the more hardcore the bug.This helps the developer to access these problems early in the development phase.\nInstallation and Usage of FindBugs\nIDE comes preinstalled with FindBugs plugin.\nYou can configure that FindBugs should run automatically for a selected project. For this right-click on a project and select Properties from the popup menu. via the project properties. Select FindBugs &#x2192; Run automatically as shown below.\nTo run the error analysis of FindBugs on a project, right-click on it and select the Find Bugs&#x2026;&#x200B; &#x2192; Find Bugs menu entry.\nPlugin provides specialized views to see the reported error messages. Select Window &#x2192; Show View &#x2192; Other&#x2026;&#x200B; to access the views.\nThe FindBugs error messages are also displayed in the Problems view or as decorators in the Package Explorer view.\n4.3.3. SonarLint\nwhat is SonarLint\nSonarLint is an open platform to manage code quality.\nIt provides on-the-fly feedback to developers on new bugs and quality issues injected into their code..\nWhy use SonarLint\nIt covers seven aspects of code quality like junits, coding rules,comments,complexity,duplications, architecture and design and potential bugs.\nSonarLint has got a very efficient way of navigating, a balance between high-level view, dashboard and defect hunting tools. This enables to quickly uncover projects and / or components that are in analysis to establish action plans.\nInstallation and usage of SonarLint\nIDE comes preinstalled with SonarLint.\nTo configure it , please follow below steps:\nFirst of all, you need to start sonar service. For that , go to software folder which is extracted from Devon-dist zip, choose sonarqube&#x2192;bin&#x2192;&lt;choose appropriate folder according to your OS&gt;-&#x2192;and execute startSonar bat file.\nIf your project is not already under analysis, you&#x2019;ll need to declare it through the SonarQube web interface as described here.\nOnce your project exists in SonarQube, you&#x2019;re ready to get started with SonarQube in Eclipse.\nSonarLint in Eclipse is pre-configured to access a local SonarQube server listening on http://localhost:9000/.\nYou can edit this server, delete it or add new ones.By default, user and password is &quot;admin&quot;.If sonar service is started properly, test connection will give you successful result.\nFor getting a project analysed on sonar, refer this http://docs.sonarqube.org/display/SONAR/Analyzing+Source+Code [link].\nLinking a project to one analysed on sonar server.\nIn the SonarQube project text field, start typing the name of the project and select it in the list box:\nClick on Finish. Your project is now associated to one analyzed on your SonarQube server.\nChanging Binding\nAt any time, it is possible to change the project association.\nTo do so, right-click on the project in the Project Explorer, and then SonarQube &gt; Change Project Association.\nUnbinding a Project\nTo do so, right-click on the project in the Project Explorer, and then SonarQube &gt; Remove SonarQube Nature.\nAdvanced Configuration\nAdditional settings (such as markers for new issues) are available through Window &gt; Preferences &gt; SonarLint\nTo look for sonarqube analysed issue, go to Window&#x2192;Show View&#x2192; Others&#x2192;SonarLint&#x2192;SonarLint Issues.\nNow you can see issues in soanrqube issues tab as shown\nOr you can go to link http://localhost:9000 and login with admin as id and admin as password and goto Dashboard.you can see all the statistics of analysis of the configured projects on sonar server.\n&#x2190;&#xA0;Previous:&#xA0;Download and Setup&#xA0;| &#x2191;&#xA0;Up:&#xA0;Getting Started&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;devonfw ide&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/getting-started-what-is-devonfw.asciidoc.html","title":"1. devonfw Introduction","body":"\n1. devonfw Introduction\nWelcome to devonfw, the devonfw platform. This is a product of the CSD industrialization effort to bring a standardized platform for custom software development within Capgemini APPS2. This platform is aimed at engagements where clients do not force the use of a determined technology so we can offer a better alternative coming from our experience as a group.\ndevonfw is a development platform aiming for standardization of processes and productivity boost, that provides an architecture blueprint for Java/JavaScript applications, alongside a set of tools to provide a fully functional out-of-the-box development environment.\nPlease note that the devonfw name is a registered trademark of .\n1.1. Building Blocks of the Platform\ndevonfw uses a state-of-the-art open source core reference architecture for the server (today considered as commodity in the IT-industry) and on top of it an ever increasing number of high-value assets that are developed by Capgemini.\n1.2. devonfw Technology Stack\ndevonfw is fully Open Source and consists of the following technology stacks.\n1.2.1. Back-end solutions\ndevon4j: server implemented with Java. The devonfw platform provides an implementation for Java based on Spring and Spring Boot.\ndevon4net: server implementation based on .NET.\ndevon4node: server implementation based on NestJS.\n1.2.2. Front-end solutions\nFor client applications, devonfw includes two possible solutions based on TypeScript, JavaScript, C# and .NET:\ndevon4ng: Frontend implementation based on Angular and hybrid mobile implementation based on Ionic.\ndevon4X: Mobile implementation based on Xamarin.\nCheck out the links for more details.\n1.3. Custom Tools\n1.3.1. devonfw IDE\nThe devonfw-ide is the new and fantastic tool to automatically download, install, setup and update the IDE (integrated development environment) of your devonfw projects.\nIDEs\nIt supports the following IDEs:\nEclipse\nVisual Studio Code\nIntelliJ\nPlatforms\nIt supports the following platforms:\njava (see also devon4j)\nC# (see devon4net)\nnode.js and angular (see devon4ng)\nBuild-Systems\nIt supports the following build-systems:\nmvn (maven)\nnpm\ngradle\nHowever, also other IDEs, platforms, or tools can be easily integrated as commandlet.\n1.3.2. Cobigen\nCobigen is a code generator included in the context of devonfw that allows users to generate all the structure and code of the components, helping to save a lot of time wasted in repetitive tasks.\nFollowing the same philosophy CobiGen now bundles a new command line interface (CLI) that enables the generation of code using few commands. This feature allows us to decouple CobiGen from Eclipse and be able to use it alongside VS Code or IntelliJ.\n&#x2191;&#xA0;Up:&#xA0;Getting Started&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Why should I use devonfw?&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/getting-started-why-should-i-use-devonfw.asciidoc.html","title":"2. Why should I use devonfw?","body":"\n2. Why should I use devonfw?\nDevonnfw aims at providing a framework which is oriented at development of web applications based on the Java EE programming model using the Spring framework project as the default implementation.\n2.1. Objectives\n2.1.1. Standardization\nIt means that to stop reinventing the Wheel in thousands of projects, hundreds of centers, dozens of countries. This also includes rationalize, harmonize and standardize all development assets all over the group and industrialize the software development process\n2.1.2. Industrialization of Innovative technologies &amp; &#x201C;Digital&#x201D;\ndevonfw needs to standardize &amp; industrialize. But not just large volume, &#x201C;traditional&#x201D; custom software development. devonfw needs to offer a standardized platform which contains a range of state of the art methodologies and technology options. devonfw needs to support agile development by small teams utilizing the latest technologies for Mobile, IoT and the Cloud\n2.1.3. Deliver &amp; Improve Business Value\n2.1.4. Efficiency\nUp to 20% reduction in time to market with faster delivery due to automation and reuse.\nUp to 25% less implementation efforts due to code generation and reuse.\nFlat pyramid and rightshore, ready for juniors.\n2.1.5. Quality\nState of the Art architecture and design.\nLower cost on maintenance and warranty.\nTechnical debt reduction by reuse.\nRisk reduction due to assets continuous improvement.\nStandardized automated quality checks.\n2.1.6. Agility\nFocus on business functionality not on technical.\nShorter release cycles.\nDevOps by design - Infrastructure as Code.\nContinuous Delivery Pipeline.\nOn and Off-premise flexibility.\nPoCs and Prototypes in days not months.\n2.2. Features\n2.2.1. Everything in a single zip\nThe devonfw distributions is packaged in a zip file that includes all the Custom Tools, Software and configurations.\nHaving all the dependencies self-contained in the distribution&#x2019;s zip file, users don&#x2019;t need to install or configure anything. Just extracting the zip content is enough to have a fully functional devonfw.\n2.2.2. devonfw, the package\ndevonfw package provides:\nImplementation blueprints for a modern cloud-ready server and a choice on JS-Client technologies (either open source AngularJs or a very rich and impressive solution based on commercial Sencha UI).\nQuality documentation and step-by-step quick start guides.\nHighly integrated and packaged development environment based around Eclipse and Jenkins. You will be ready to start implementing your first customer-specific use case in 2h time.\nIterative eclipse-based code-generator that understands &quot;Java&quot; and works on higher architectural concepts than Java-classes.\nExample application as a reference implementation.\nSupport through large community + industrialization services (Standard Platform as a service) available in the iProd service catalog.\nTo read in details about devonfw features read here\n&#x2190;&#xA0;Previous:&#xA0;devonfw Introduction&#xA0;| &#x2191;&#xA0;Up:&#xA0;Getting Started&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Download and Setup&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/h52f0511b4b12e1c6117faf9c68cedda5.asciidoc.html","title":"48. Home","body":"\n48. Home\n48.1. Contact\nIn case of any questions, please send mail to DL PL E2E Test Framework &lt;e2etestframework.pl@capgemini.com&gt;\n48.2. What is E2E Mr Checker Test Framework\nMr Checker Test Framework in the end to end test automation framework written in Java.\n48.3. Where Mr Checker applies?\nThe main goal of MrChecker is to standardize the way we build BlackBox tests. It gives the possibility to have one common software standard in order to build: Component, Integration and System tests.\nA Test Engineer does not have access to the application source code in order to perform BlackBox tests, but he is able to attach his tests to any application interfaces, such as:\n- IP address\n- Domain Name\n- communication protocol\n- Command Line Interface.\nMr Checker&#x2019;s specification:\nResponsive Web Design application: Selenium Browser\nREST/SOAP: RestAssure\nService Virtualization: Wiremock\nDatabase: JDBC drivers for SQL\nSecurity: RestAssure + RestAssure Security lib\nStandalone java application: SWING\nNative mobile application for Android: Appium\n48.3.1. Test stages\nUnit test\nA module is the smallest compilable unit of source code. It is often too small to be tested by the functional tests (black-box tests). However, it is the ideal candidate for white-box testing.\nWhite - box tests have to be performed as the first static tests (e.g. Lint and inspections), followed by dynamic tests in order to check boundaries, branches and paths. Usually, that kind of testing would require the enablement of stubs and special test tools.&#xA0;\nComponent test\nThis is the black-box test of modules or groups of modules which represent certain functionalities. There are no rules about what could be called a component. Whatever a tester defines as a component, should make sense and be a testable unit. Components can be step by step integrated into the bigger components and tested as such.&#xA0;\nIntegration test\nFunctions are tested by feeding them input and examining the output, and internal program structure is rarely considered. The software is step by step completed and tested by the tests covering a collaboration of modules or classes. The integration depends on the kind of system.\nFor example, the steps could be as such: run the operating system first and gradually add one component after another, then check if the black-box tests still are running (the test cases will be extended together with every added component). The integration is done in the laboratory. It may be also completed by using simulators or emulators. Additionally, the input signals could be stimulated.&#xA0;\nSoftware / System test\nSystem testing is a type of testing conducted on a complete integrated system to evaluate the system&#x2019;s compliance with its specified requirements. This is a type of black-box testing of the complete software in the target system. The most important factor in the successful system testing is that the environmental conditions for the software have to be as realistic as possible (complete original hardware in the destination environment).\n48.4. Benefits for the project\nEvery customer may benefit from using Mr Checker Test Framework.\nThe main profits for the project are:\nResilient and robust building and validation process\nQuality gates shifted closer to the software development process\nTeam quality awareness increase - including Unit Tests, Static Analyze, Security Tests, Performance in the testing process\nTest execution environment transparent to any infrastructure\nTouch base with the Cloud solution\nFaster Quality and DevOps - driven delivery\nProven frameworks, technologies and processes.\n48.5. Road map plan\nAll slides: link\n48.6. Wiki Structure\nHow to install Mr Checker Test Framework_\nMr Checker Test Framework modules:\nCore test module\nSelenium test module\nWebAPI test module\nSecurity test module\nDataBase test module\nMobile test module\nStandalone test module\nDevOps module\n&#x2191;&#xA0;Up:&#xA0;devonfw testing&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;How to install&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/h593708e8db8bc7e0b53adee5b191ee9b.asciidoc.html","title":"40. CICDGEN","body":"\n40. CICDGEN\ncicdgen is a devonfw tool for generate all code/files related to CICD. It will include/modify into your project all files that the project needs run a Jenkins cicd pipeline, to create a docker image based on your project, etc. It&#x2019;s based on angular schematics, so you can add it as a dependency into your project and generate the code using ng generate. In addition, it has its own CLI for those projects that are not angular based.\n40.1. What is angular schematics?\nSchematics are generators that transform an existing filesystem. They can create files, refactor existing files, or move files around.\nWhat distinguishes Schematics from other generators, such as Yeoman or Yarn Create, is that schematics are purely descriptive; no changes are applied to the actual filesystem until everything is ready to be committed. There is no side effect, by design, in Schematics.\n40.2. cicdgen CLI\nFor know more about how to use the cicdgen CLI, you can check the CLI page\n40.3. cicdgen Schematics\nFor know more about how to use the cicdgen schematics, you can check the schematics page\n40.4. Usage example\nA specific page about how to use cicdgen is also available.\n&#x2191;&#xA0;Up:&#xA0;cicdgen&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;cicdgen CLI&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/hf27dbe18adc24040ccf8630dc598a83f.asciidoc.html","title":"32. What is devonfw shop floor?","body":"\n32. What is devonfw shop floor?\ndevonfw shop floor is a platform to industrialize continuous delivery and continuous integration processes.\ndevonfw shop floor is a set of documentation, tools and methodologies used to configure the provisioning, development and uat environments used in your projects. devonfw shop floor allows the administrators of those environments to apply CI/CD operations and enables automated application deployment.\ndevonfw shop floor is mainly oriented to configure the provisioning environment provided by Production Line and deploy applications on an OpenShift cluster. In the cases where Production Line or OpenShift cluster are not available, there will be alternatives to achieve similar goals.\nThe devonfw shop floor 4 OpenShift is a solution based on the experience of priming devonfw for OpenShift by RedHat.\nLet&#x2019;s start.\n&#x2191;&#xA0;Up:&#xA0;devonfw shop floor&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;How to use it&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-cicdgen.asciidoc.html","title":"VIII. cicdgen","body":"\nVIII. cicdgen\nCICDGEN\ncicdgen CLI\ncicdgen Schematics\n&#x2190;&#xA0;Previous:&#xA0;Annexes&#xA0;| &#x2191;&#xA0;Up:&#xA0;devonfw guide&#xA0;| Next:&#xA0;CICDGEN&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-cicdgen.asciidoc_cicdgen-cli.html","title":"41. cicdgen CLI","body":"\n41. cicdgen CLI\n41.1. CICDGEN CLI\ncicdgen is a command line interface that helps you with some CICD in a devonfw project. At this moment we can only generate files related to CICD in a project but we plan to add more functionlity in a future.\nInstallation\n$ npm i -g @devonfw/cicdgen\nUsage\nGlobal arguments\n--version\nPrints the cicdgen version number\n--help\nShows the usage of the command\nCommands\nGenerate.\nThis command wraps the usage of angular schematics CLI. With this we generate files in a easy way and also print a better help about usage.\nAvailable schematics that generate the code:\ndevon4j\ndevon4ng\ndevon4node\nExamples\nGenerate all CICD files related to a devon4j project\n$ cicdgen generate devon4j\nGenerate all CICD files related to a devon4ng project with docker deployment.\n$ cicdgen generate devon4ng --groupid com.devonfw --docker --plurl devon.s2-eu.capgemini.com\nGenerate all CICD files related to a devon4node project with OpenShift deployment.\n$ cicdgen generate devon4ng --groupid com.devonfw --openshift --plurl devon.s2-eu.capgemini.com --ocurl ocp.itaas.s2-eu.capgemini.com --ocn devonfw\n41.2. cicdgen usage example\nIn this example we are going to show how to use cicdgen step by step in a devon4ng project.\nInstall cicdgen\ncicdgen is already included in the devonfw distribution, but if you want to use it outside the devonfw console you can execute the following command:\n$ npm i -g cicdgen\nGenerate a new devon4ng project using devonfw ide.\nInside a devonfw ide distrubution execute the command (devon ng create &lt;app-name&gt;):\n$ devon ng create devon4ng\nExecute cicdgen generate command\nAs we want to send notifications to MS Teams, we need to create de connector first:\nGo to a channel in teams and click at the connectors button. Then click at the jenkins configure button.\nPut a name for the connector\nCopy the name and the Webhook URL, we will use it later.\nWith the values that we get in the previous steps, we will execute the cicdgen command inside the project folder. If you have any doubt you can use the help.\n$ cicdgen generate devon4ng --groupid com.devonfw --docker --plurl devon.s2-eu.capgemini.com --teams --teamsname devon4ng --teamsurl https://outlook.office.com/webhook/...\nCreate a git repository and upload the code\n$ git remote add origin https://devon.s2-eu.capgemini.com/gitlab/darrodri/devon4ng.git\n$ git push -u origin master\nAs you can see, no git init or git commit is required, cicdgen do it for you.\nCreate a multibranch-pipeline in Jenkins\nWhen you push the save button, it will download the repository and execute the pipeline defined in the Jenkinsfile. If you get any problem, check the environment variables defined in the Jenkinsfile. Here we show all variables related with Jenkins:\nchrome\nsonarTool\nsonarEnv\nrepositoryId\nglobalSettingsId\nmavenInstallation\ndockerTool\nAdd a webhook in GitLab\nIn order to run the pipeline every time that you push code to GitLab, you need to configure a webhook in your repository.\nNow your project is ready to work following a CICD strategy.\nThe last thing to take into account is the branch naming. We prepare the pipeline in order to work following the git-flow strategy. So all stages of the pipeline will be executed for the branchs: develop, release/*, master. For the branchs: feature/*, hotfix/*, bugfix/* only the steps related to unit testing will be executed.\n&#x2190;&#xA0;Previous:&#xA0;CICDGEN&#xA0;| &#x2191;&#xA0;Up:&#xA0;cicdgen&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;cicdgen Schematics&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-cicdgen.asciidoc_cicdgen-schematics.html","title":"42. cicdgen Schematics","body":"\n42. cicdgen Schematics\n42.1. CICDGEN SCHEMATICS\nWe use angular schematics to create and update an existing devonfw project in order to adapt it to a CICD environment. All schematics are prepared to work with Production Line, a Capgemini CICD platform, but it can also work in other environment which have the following tools:\nJenkins\nNexus 3\nSonarQube\nThe list of available schematics are:\ndevon4j\ndevon4ng\ndevon4node\nHow to run the schematics\nYou can run the schematics using the schematic CLI provided by the angular team, but the easiest way to run it is using the cicdgen CLI which is a wrapper for the angular CLI in order to use it in a easy way.\nTo generate files you only need to run the command\n$ cicdgen generate &lt;schematic-name&gt; [arguments]\n&lt;schematic-name&gt; is the name of the schematic that you want to execute.\nYou can find all information about arguments in the schematic section.\n42.2. Devon4j schematic\nWith the cicdgen generate devon4j command you can generate some files required for CICD. In this section we will explain the arguments of this command and also the files that will be generated.\nDevon4j schematic arguments\nWhen you execute the cicdgen generate devon4j command you can also add some arguments in order to modify the behaviour of the command. Those arguments are:\n--docker\nThe type of this paramter if boolean. If it is present, docker related files and pipeline stage will be also generated. For more details see docker section of Jenkinsfile and files generated for docker\n--plurl\nUrl of Production Line. It is required when --docker is true, and it will be used to know where the docker image will be uploaded.\n--openshift\nThe type of this paramter if boolean. If it is present, OpenShift related files and pipeline stage will be also generated. For more details see OpenShift section of Jenkinsfile and files generated for docker (same as --docker)\n--ocurl\nOpenShift cluster url where the application will be builded and deployed.\n--ocn\nOpenshift cluster namespace\n--teams\nWith this argument we can add the teams notification option in the Jenkinsfile.\n--teamsname\nThe name of the Microsft Teams webhook. It is defined at Microsoft Teams connectors.\n--teamsurl\nThe url of the Microsft Teams webhook. It is returned by Microsoft Teams when you create a connector.\nDevon4ng generated files\nWhen you execute the generate devon4ng command, some files will be added/updated in your project.\nFiles\n.gitignore\nDefines all files that git will ignore. e.g: compiled files, IDE configurations.\npom.xml\nThe pom.xml is modified in order to add the distributionManagement.\nJenkinsfile\nThe Jenkinsfile is the file which define the Jenkins pipeline of our project. With this we can execute the test, build the application and deploy it automatically following a CICD methodology. This file is prepared to work with the Production Line default values, but it is also fully configurable to your needs.\nPrerequisites\nA Production Line instance. It can works also if you have a Jenkins, SonarQube and Nexus3, but in this case maybe you need to configure them properly.\nJava installed in Jenkins as a global tool.\nGoogle Chrome installed in Jenkins as a global custom tool.\nSonarQube installed in Jenkins as a global tool.\nMaven3 installed in Jenkins as a global tool.\nA maven global settings properly configured in Jenkins.\nIf you will use docker to deploy:\nDocker installed in Jenkins as a global custom tool.\nThe Nexus3 with a docker repository.\nA machine with docker installed where the build and deploy will happen.\nA docker network called application.\nIf you will use OpenShift to deploy:\nAn OpenShift instance\nThe OpenShift projects created\nThe Jenkins syntax\nIn this section we will explain a little bit the syntax of the Jenkins, so if you need to change something you will be able to do it properly.\nagent: Here you can specify the Jenkins agente where the pipeline will be executed. The default value is any.\noptions: Here you can set global options to the pipeline. By default, we add a build discarded to delete old artifacts/buils of the pipeline and also we disable the concurrent builds.\nIf the teams option is passed to cicdgen, we add a new option in order to send notifications to Microsoft Teams with the status of the pipeline executions.\nenvironment: Here all environment variables are defined. All values defined here matches with the Production Line defaults. If you Jenkins has other values, you need to update it manually.\nstages: Here are defined all stages that our pipeline will execute. Those stages are:\nSetup pipeline: We set some variables depending on the git branch which you are executing. Also, we set properly the version number in all pom files. It means that if your branch is develop, your version should end with the word -SNAPSHOT, in order case, if -SNAPSHOT is present it will be removed.\nFresh Dependency Installation: install all packages need to build/run your java project.\nUnit Tests: execute the mvn test command.\nSonarQube code analysis: send the project to SonarQube in order to get the static code analysis of your project.\nDeliver application into Nexus: build the project and send all bundle files to Nexsu3.\nIf --docker is present:\nCreate the Docker image: build a new docker image that contains the new version of the project.\nDeploy the new image: deploy a new version of the application using the image created in the previous stage. The previous version is removed.\nIf --openshift is present:\nCreate the Docker image: build a new docker image that contains the new version of the project using a OpenShift build config.\nDeploy the new image: deploy a new version of the application in OpenShift.\nCheck pod status: checks that the application deployed in the previous stage is running properly. If the application does not run the pipeline will fail.\npost: actions that will be executed after the stages. We use it to clean up all files.\nDevon4j Docker generated files\nWhen you generate the files for a devon4ng you can also pass the option --docker. It will generate also some extra files related to docker.\nNote\nIf you pass the --docker option the option --plurl is also required. It will be used to upload the images to the Nexus3 inside Production Line. Example: if your PL url is test.s2-eu.capgemini.com you should execute the command in this way: cicdgen generate devon4ng --groupid com.devonfw --docker --plurl test.s2-eu.capgemini.com, and it will use docker-registry-test.s2-eu.capgemini.com as docker registry.\nFiles\nDockerfile\nThis file contains the instructions to build a docker image for you project. This Dockerfile is for local development purposes, you can use it in your machine executing:\n$ cd &lt;path-to-your-project&gt;\n$ docker build -t &lt;project-name&gt;/&lt;tag&gt; .\nThis build is using a multi-stage build. First, it use a maven image in order to compile the source code, then it will use a java image to run the application. With the multi-stage build we keep the final image as clean as possible.\nDockerfile.ci\nThis file contains the instructions to create a docker image for you project. The main difference with the Dockerfile is that this file will be only used in the Jenkins pipeline. Instead of compiling again the code, it takes the compiled war from Jenkins to the image.\n42.3. Devon4ng schematic\nWith the cicdgen generate devon4ng command you can generate some files required for CICD. In this section we will explain the arguments of this command and also the files that will be generated.\nDevon4ng schematic arguments\nWhen you execute the cicdgen generate devon4ng command you can also add some arguments in order to modify the behaviour of the command. Those arguments are:\n--docker\nThe type of this paramter if boolean. If it is present, docker related files and pipeline stage will be also generated. For more details see docker section of Jenkinsfile and files generated for docker\n--plurl\nUrl of Production Line. It is required when --docker is true, and it will be used to know where the docker image will be uploaded.\n--openshift\nThe type of this paramter if boolean. If it is present, OpenShift related files and pipeline stage will be also generated. For more details see OpenShift section of Jenkinsfile and files generated for OpenShift (same as --docker)\n--ocurl\nOpenShift cluster url where the application will be builded and deployed.\n--ocn\nOpenshift cluster namespace\n--groupid\nThe project groupId. This argument is required. It will be used for store the project in a maven repository at Nexus 3. Why maven? Because is the kind of repository where we can upload/download a zip file easily. Npm repository needs a package.json file but, as we compile the angular application to static javascript and html files, the package.json is no needed anymore.\n--teams\nWith this argument we can add the teams notification option in the Jenkinsfile.\n--teamsname\nThe name of the Microsft Teams webhook. It is defined at Microsoft Teams connectors.\n--teamsurl\nThe url of the Microsft Teams webhook. It is returned by Microsoft Teams when you create a connector.\nDevon4ng generated files\nWhen you execute the generate devon4ng command, some files will be added/updated in your project.\nFiles\nangular.json\nThe angular.json is modified in order to change the compiled files destination folder. Now, when you make a build of your project, the compiled files will be generated into dist folder instead of dist/&lt;project-name&gt; folder.\npackage.json\nThe package.json is modified in order to add a script for test the application using Chrome Headless instead of a regular chrome. This script is called test:ci.\nkarma.conf.js\nThe karma.conf.js is also modified in order to add the Chrome Headless as a browser to execute test. The coverage output folder is change to ./coverage instead of ./coverage/&lt;project-name&gt;\nJenkinsfile\nThe Jenkinsfile is the file which define the Jenkins pipeline of our project. With this we can execute the test, build the application and deploy it automatically following a CICD methodology. This file is prepared to work with the Production Line default values, but it is also fully configurable to your needs.\nPrerequisites\nA Production Line instance. It can works also if you have a Jenkins, SonarQube and Nexus3, but in this case maybe you need to configure them properly.\nNodeJS installed in Jenkins as a global tool.\nGoogle Chrome installed in Jenkins as a global custom tool.\nSonarQube installed in Jenkins as a global tool.\nMaven3 installed in Jenkins as a global tool.\nA maven global settings properly configured in Jenkins.\nIf you will use docker :\nDocker installed in Jenkins as a global custom tool.\nThe Nexus3 with a docker repository.\nA machine with docker installed where the build and deploy will happen.\nA docker network called application.\nIf you will use OpenShift :\nAn OpenShift instance\nThe OpenShift projects created\nThe Jenkins syntax\nIn this section we will explain a little bit the syntax of the Jenkins, so if you need to change something you will be able to do it properly.\nagent: Here you can specify the Jenkins agente where the pipeline will be executed. The default value is any.\noptions: Here you can set global options for the pipeline. By default, we add a build discarded to delete old artifacts/buils of the pipeline and also we disable the concurrent builds.\nIf the teams option is passed to cicdgen, we add a new option in order to send notifications to Microsoft Teams with the status of the pipeline executions.\ntools: Here we define the global tools configurations. By default a version of nodejs is added here.\nenvironment: Here all environment variables are defined. All values defined here matches with the Production Line defaults. If you Jenkins has other values, you need to update it manually.\nstages: Here are defined all stages that our pipeline will execute. Those stages are:\nLoading Custom Tools: in this stage some custom tools are loaded. Also we set some variables depending on the git branch which you are executing.\nFresh Dependency Installation: install all packages need to build/run your angular project.\nCode Linting: execute the linter analysis.\nExecute Angular tests: execute the angular test in a Chrome Headless.\nSonarQube code analysis: send the project to SonarQube in order to get the static code analysis of your project.\nBuild Application: compile the application to be ready to deploy in a web server.\nDeliver application into Nexus: store all compiled files in Nexus3 as a zip file.\nIf --docker is present:\nCreate the Docker image: build a new docker image that contains the new version of the project.\nDeploy the new image: deploy a new version of the application using the image created in the previous stage. The previous version is removed.\nIf --openshift is present:\nCreate the Docker image: build a new docker image that contains the new version of the project using a OpenShift build config.\nDeploy the new image: deploy a new version of the application in OpenShift.\nCheck pod status: checks that the application deployed in the previous stage is running properly. If the application does not run the pipeline will fail.\npost: actions that will be executed after the stages. We use it to clean up all files.\nDevon4ng Docker generated files\nWhen you generate the files for a devon4ng you can also pass the option --docker. It will generate also some extra files related to docker.\nNote\nIf you pass the --docker option the option --plurl is also required. It will be used to upload the images to the Nexus3 inside Production Line. Example: if your PL url is test.s2-eu.capgemini.com you should execute the command in this way: cicdgen generate devon4ng --groupid com.devonfw --docker --plurl test.s2-eu.capgemini.com, and it will use docker-registry-test.s2-eu.capgemini.com as docker registry.\nFiles\n.dockerignore\nIn this files are defined the folders that will not be copied to the docker image. Fore more information read the official documentation.\nDockerfile\nThis file contains the instructions to build a docker image for you project. This Dockerfile is for local development purposes, you can use it in your machine executing:\n$ cd &lt;path-to-your-project&gt;\n$ docker build -t &lt;project-name&gt;/&lt;tag&gt; .\nThis build is using a multi-stage build. First, it use a node image in order to compile the source code, then it will use a nginx image as a web server for our devon4ng application. With the multi-stage build we avoid everything related to node.js in our final image, where we only have a nginx with our application compiled.\nDockerfile.ci\nThis file contains the instructions to create a docker image for you project. The main difference with the Dockerfile is that this file will be only used in the Jenkins pipeline. Instead of compiling again the code, it takes all compiled files and the nginx.conf from Jenkins to the image.\nnginx.conf\nConfiguration file for our nginx server. It defines the root folder of our application where docker copy the files to. Also it defines a fallback route to the index as described in the angular deployment guide in oder to enable the angular routes.\n42.4. Devon4node schematic\nWith the cicdgen generate devon4node command you can generate some files required for CICD. In this section we will explain the arguments of this command and also the files that will be generated.\nDevon4node schematic arguments\nWhen you execute the cicdgen generate devon4node command you can also add some arguments in order to modify the behaviour of the command. Those arguments are:\n--docker\nThe type of this paramter if boolean. If it is present, docker related files and pipeline stage will be also generated. For more details see docker section of Jenkinsfile and files generated for docker\n--plurl\nUrl of Production Line. It is required when --docker is true, and it will be used to know where the docker image will be uploaded.\n--openshift\nThe type of this paramter if boolean. If it is present, OpenShift related files and pipeline stage will be also generated. For more details see OpenShift section of Jenkinsfile and files generated for OpenShift (same as --docker)\n--ocurl\nOpenShift cluster url where the application will be builded and deployed.\n--ocn\nOpenshift cluster namespace\n--groupid\nThe project groupId. This argument is required. It will be used for store the project in a maven repository at Nexus 3. Why maven? Because is the kind of repository where we can upload/download a zip file easily. Npm repository needs a package.json file but, as we compile the angular application to static javascript and html files, the package.json is no needed anymore.\n--teams\nWith this argument we can add the teams notification option in the Jenkinsfile.\n--teamsname\nThe name of the Microsft Teams webhook. It is defined at Microsoft Teams connectors.\n--teamsurl\nThe url of the Microsft Teams webhook. It is returned by Microsoft Teams when you create a connector.\nDevon4node generated files\nWhen you execute the generate devon4node command, some files will be added/updated in your project.\nFiles\npackage.json\nThe package.json is modified in order to add a script to run the application in a docker container. It is necessary because we change a little bit the folder structure when we put all files in a docker image, so the script start:prod does not work.\n.gitignore\nDefines all files that git will ignore. e.g: compiled files, IDE configurations.\nJenkinsfile\nThe Jenkinsfile is the file which define the Jenkins pipeline of our project. With this we can execute the test, build the application and deploy it automatically following a CICD methodology. This file is prepared to work with the Production Line default values, but it is also fully configurable to your needs.\nPrerequisites\nA Production Line instance. It can works also if you have a Jenkins, SonarQube and Nexus3, but in this case maybe you need to configure them properly.\nNodeJS installed in Jenkins as a global tool.\nGoogle Chrome installed in Jenkins as a global custom tool.\nSonarQube installed in Jenkins as a global tool.\nMaven3 installed in Jenkins as a global tool.\nA maven global settings properly configured in Jenkins.\nIf you will use docker :\nDocker installed in Jenkins as a global custom tool.\nThe Nexus3 with a docker repository.\nA machine with docker installed where the build and deploy will happen.\nA docker network called application.\nIf you will use OpenShift :\nAn OpenShift instance\nThe OpenShift projects created\nThe Jenkins syntax\nIn this section we will explain a little bit the syntax of the Jenkins, so if you need to change something you will be able to do it properly.\nagent: Here you can specify the Jenkins agente where the pipeline will be executed. The default value is any.\noptions: Here you can set global options for the pipeline. By default, we add a build discarded to delete old artifacts/buils of the pipeline and also we disable the concurrent builds.\nIf the teams option is passed to cicdgen, we add a new option in order to send notifications to Microsoft Teams with the status of the pipeline executions.\ntools: Here we define the global tools configurations. By default a version of nodejs is added here.\nenvironment: Here all environment variables are defined. All values defined here matches with the Production Line defaults. If you Jenkins has other values, you need to update it manually.\nstages: Here are defined all stages that our pipeline will execute. Those stages are:\nLoading Custom Tools: in this stage some custom tools are loaded. Also we set some variables depending on the git branch which you are executing.\nFresh Dependency Installation: install all packages need to build/run your node project.\nCode Linting: execute the linter analysis.\nExecute tests: execute the tests.\nSonarQube code analysis: send the project to SonarQube in order to get the static code analysis of your project.\nBuild Application: compile the application to be ready to deploy in a web server.\nDeliver application into Nexus: store all compiled files in Nexus3 as a zip file.\nIf --docker is present:\nCreate the Docker image: build a new docker image that contains the new version of the project.\nDeploy the new image: deploy a new version of the application using the image created in the previous stage. The previous version is removed.\nIf --openshift is present:\nCreate the Docker image: build a new docker image that contains the new version of the project using a OpenShift build config.\nDeploy the new image: deploy a new version of the application in OpenShift.\nCheck pod status: checks that the application deployed in the previous stage is running properly. If the application does not run the pipeline will fail.\npost: actions that will be executed after the stages. We use it to clean up all files.\nDevon4node Docker generated files\nWhen you generate the files for a devon4node you can also pass the option --docker. It will generate also some extra files related to docker.\nNote\nIf you pass the --docker option the option --plurl is also required. It will be used to upload the images to the Nexus3 inside Production Line. Example: if your PL url is test.s2-eu.capgemini.com you should execute the command in this way: cicdgen generate devon4node --groupid com.devonfw --docker --plurl test.s2-eu.capgemini.com, and it will use docker-registry-test.s2-eu.capgemini.com as docker registry.\nFiles\n.dockerignore\nIn this files are defined the folders that will not be copied to the docker image. Fore more information read the official documentation.\nDockerfile\nThis file contains the instructions to build a docker image for you project. This Dockerfile is for local development purposes, you can use it in your machine executing:\n$ cd &lt;path-to-your-project&gt;\n$ docker build -t &lt;project-name&gt;/&lt;tag&gt; .\nThis build is installs all dependencies in ordre to build the project and then remove all devDependencies in order to keep only the production dependencies.\nDockerfile.ci\nThis file contains the instructions to create a docker image for you project. The main difference with the Dockerfile is that this file will be only used in the Jenkins pipeline. Instead of compiling again the code, it takes all compiled files from Jenkins to the image.\n&#x2190;&#xA0;Previous:&#xA0;cicdgen CLI&#xA0;| &#x2191;&#xA0;Up:&#xA0;cicdgen&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;CobiGen&#x2009;&#x2014;&#x2009;Code-based incremental Generator&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-cobigen.asciidoc.html","title":"IX. CobiGen&#x2009;&#x2014;&#x2009;Code-based incremental Generator","body":"\nIX. CobiGen&#x2009;&#x2014;&#x2009;Code-based incremental Generator\nDocument Description\nCobiGen\nMaven Build Integration\nEclipse Integration\nTemplate Development\n&#x2190;&#xA0;Previous:&#xA0;cicdgen Schematics&#xA0;| &#x2191;&#xA0;Up:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Document Description&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-cobigen.asciidoc_cobigen.html","title":"44. CobiGen","body":"\n44. CobiGen\n44.1. Configuration\nCobiGen will be configured using a configuration folder containing a context configuration, multiple template folders with a templates configuration per template folder, and a number of templates in each template folder. Find some examples here. Thus, a simple folder structure might look like this:\nCobiGen_Templates\n|- templateFolder1\n|- templates.xml\n|- templateFolder2\n|- templates.xml\n|- context.xml\n44.1.1. Context Configuration\nThe context configuration (context.xml) always has the following root structure:\nListing 91. Context Configuration\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n&lt;contextConfiguration xmlns=&quot;http://capgemini.com&quot;\nxmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;\nversion=&quot;1.0&quot;&gt;\n&lt;triggers&gt;\n...\n&lt;/triggers&gt;\n&lt;/contextConfiguration&gt;\nThe context configuration has a version attribute, which should match the XSD version the context configuration is an instance of. It should not state the version of the currently released version of CobiGen. This attribute should be maintained by the context configuration developers. If configured correctly, it will provide a better feedback for the user and thus higher user experience. Currently there is only the version v1.0. For further version there will be a changelog later on.\nTrigger Node\nAs children of the &lt;triggers&gt; node you can define different triggers. By defining a &lt;trigger&gt; you declare a mapping between special inputs and a templateFolder, which contains all templates, which are worth to be generated with the given input.\nListing 92. trigger configuration\n&lt;trigger id=&quot;...&quot; type=&quot;...&quot; templateFolder=&quot;...&quot; inputCharset=&quot;UTF-8&quot; &gt;\n...\n&lt;/trigger&gt;\nThe attribute id should be unique within an context configuration. It is necessary for efficient internal processing.\nThe attribute type declares a specific trigger interpreter, which might be provided by additional plug-ins. A trigger interpreter has to provide an input reader, which reads specific inputs and creates a template object model out of it to be processed by the FreeMarker template engine later on. Have a look at the plug-in&#x2019;s documentation of your interest and see, which trigger types and thus inputs are currently supported.\nThe attribute templateFolder declares the relative path to the template folder, which will be used if the trigger gets activated.\nThe attribute inputCharset (optional) determines the charset to be used for reading any input file.\nMatcher Node\nA trigger will be activated if its matchers hold the following formula:\n!(NOT || &#x2026;&#x200B; || NOT) &amp;&amp; AND &amp;&amp; &#x2026;&#x200B; &amp;&amp; AND &amp;&amp; (OR || &#x2026;&#x200B; || OR)\nWhereas NOT/AND/OR describes the accumulationType of a matcher (see below) and e.g. NOT means &apos;a matcher with accumulationType NOT matches a given input&apos;. Thus additionally to an input reader, a trigger interpreter has to define at least one set of matchers, which are satisfiable, to be fully functional. A &lt;matcher&gt; node declares a specific characteristics a valid input should have.\nListing 93. Matcher Configuration\n&lt;matcher type=&quot;...&quot; value=&quot;...&quot; accumulationType=&quot;...&quot;&gt;\n...\n&lt;/matcher&gt;\nThe attribute type declares a specific type of matcher, which has to be provided by the surrounding trigger interpreter. Have a look at the plug-in&#x2019;s documentation, which also provides the used trigger type for more information about valid matcher and their functionalities.\nThe attribute value might contain any information necessary for processing the matcher&#x2019;s functionality. Have a look at the relevant plug-in&#x2019;s documentation for more detail.\nThe attribute accumulationType (optional) specifies how the matcher will influence the trigger activation. Valid values are:\nOR (default): if any matcher of accumulation type OR matches, the trigger will be activated as long as there are no further matchers with different accumulation types\nAND: if any matcher with AND accumulation type does not match, the trigger will not be activated\nNOT: if any matcher with NOT accumulation type matches, the trigger will not be activated\nVariableAssignment Node\nFinally, a &lt;matcher&gt; node can have multiple &lt;variableAssignment&gt; nodes as children. Variable assignments allow to parametrize the generation by additional values, which will be added to the object model for template processing. The variables declared using variable assignments, will be made accessible in the templates.xml as well in the object model for template processing via the namespace variables.*.\nListing 94. Complete Configuration Pattern\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n&lt;contextConfiguration xmlns=&quot;http://capgemini.com&quot;\nxmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;\nversion=&quot;1.0&quot;&gt;\n&lt;triggers&gt;\n&lt;trigger id=&quot;...&quot; type=&quot;...&quot; templateFolder=&quot;...&quot;&gt;\n&lt;matcher type=&quot;...&quot; value=&quot;...&quot;&gt;\n&lt;variableAssignment type=&quot;...&quot; key=&quot;...&quot; value=&quot;...&quot; /&gt;\n&lt;/matcher&gt;\n&lt;/trigger&gt;\n&lt;/triggers&gt;\n&lt;/contextConfiguration&gt;\nThe attribute type declares the type of variable assignment to be processed by the trigger interpreter providing plug-in. This attribute enables variable assignments with different dynamic value resolutions.\nThe attribute key declares the namespace under which the resolved value will be accessible later on.\nThe attribute value might declare a constant value to be assigned or any hint for value resolution done by the trigger interpreter providing plug-in. For instance, if type is regex, then on value you will assign the matched group number by the regex (1, 2, 3&#x2026;&#x200B;)\nContainerMatcher Node\nThe &lt;containerMatcher&gt; node is an additional matcher for matching containers of multiple input objects.\nSuch a container might be a package, which encloses multiple types or---more generic---a model, which encloses multiple elements. A container matcher can be declared side by side with other matchers:\nListing 95. ContainerMatcher Declaration\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n&lt;contextConfiguration xmlns=&quot;http://capgemini.com&quot;\nxmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;\nversion=&quot;1.0&quot;&gt;\n&lt;triggers&gt;\n&lt;trigger id=&quot;...&quot; type=&quot;...&quot; templateFolder=&quot;...&quot; &gt;\n&lt;containerMatcher type=&quot;...&quot; value=&quot;...&quot; retrieveObjectsRecursively=&quot;...&quot; /&gt;\n&lt;matcher type=&quot;...&quot; value=&quot;...&quot;&gt;\n&lt;variableAssignment type=&quot;...&quot; variable=&quot;...&quot; value=&quot;...&quot; /&gt;\n&lt;/matcher&gt;\n&lt;/trigger&gt;\n&lt;/triggers&gt;\n&lt;/contextConfiguration&gt;\nThe attribute type declares a specific type of matcher, which has to be provided by the surrounding trigger interpreter. Have a look at the plug-in&#x2019;s documentation, which also provides the used trigger type for more information about valid matcher and their functionalities.\nThe attribute value might contain any information necessary for processing the matcher&#x2019;s functionality. Have a look at the relevant plug-in&#x2019;s documentation for more detail.\nThe attribute retrieveObjectsRecursively (optional boolean) states, whether the children of the input should be retrieved recursively to find matching inputs for generation.\nThe semantics of a container matchers are the following:\nA &lt;containerMatcher&gt; does not declare any &lt;variableAssignment&gt; nodes\nA &lt;containerMatcher&gt; matches an input if and only if one of its enclosed elements satisfies a set of &lt;matcher&gt; nodes of the same &lt;trigger&gt;\nInputs, which match a &lt;containerMatcher&gt; will cause a generation for each enclosed element\n44.1.2. Templates Configuration\nThe template configuration (templates.xml) specifies, which templates exist and under which circumstances it will be generated. There are two possible configuration styles:\nConfigure the template meta-data for each template file by template nodes\n(since cobigen-core-v1.2.0): Configure templateScan nodes to automatically retrieve a default configuration for all files within a configured folder and possibly modify the automatically configured templates using templateExtension nodes\nTo get an intuition of the idea, the following will initially describe the first (more extensive) configuration style. Such an configuration root structure looks as follows:\nListing 96. Extensive Templates Configuration\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n&lt;templatesConfiguration xmlns=&quot;http://capgemini.com&quot;\nxmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;\nversion=&quot;1.0&quot; templateEngine=&quot;FreeMarker&quot;&gt;\n&lt;templates&gt;\n...\n&lt;/templates&gt;\n&lt;increments&gt;\n...\n&lt;/increments&gt;\n&lt;/templatesConfiguration&gt;\nThe root node &lt;templatesConfiguration&gt; specifies two attributes. The attribute version provides further usability support and will be handled analogous to the version attribute of the context configuration. The optional attribute templateEngine specifies the template engine to be used for processing the templates (since cobigen-core-4.0.0). By default it is set to FreeMarker.\nThe node &lt;templatesConfiguration&gt; allows two different grouping nodes as children. First, there is the &lt;templates&gt; node, which groups all declarations of templates. Second, there is the &lt;increments&gt; node, which groups all declarations about increments.\nTemplate Node\nThe &lt;templates&gt; node groups multiple &lt;template&gt; declarations, which enables further generation. Each template file should be registered at least once as a template to be considered.\nListing 97. Example Template Configuration\n&lt;templates&gt;\n&lt;template name=&quot;...&quot; destinationPath=&quot;...&quot; templateFile=&quot;...&quot; mergeStrategy=&quot;...&quot; targetCharset=&quot;...&quot; /&gt;\n...\n&lt;/templates&gt;\nA template declaration consist of multiple information:\nThe attribute name specifies an unique ID within the templates configuration, which will later be reused in the increment definitions.\nThe attribute destinationPath specifies the destination path the template will be generated to. It is possible to use all variables defined by variable assignments within the path declaration using the FreeMarker syntax ${variables.*}. While resolving the variable expressions, each dot within the value will be automatically replaced by a slash. This behavior is accounted for by the transformations of Java packages to paths as CobiGen has first been developed in the context of the Java world. Furthermore, the destination path variable resolution provides the following additional built-in operators analogue to the FreeMarker syntax:\n?cap_first analogue to FreeMarker\n?uncap_first analogue to FreeMarker\n?lower_case analogue to FreeMarker\n?upper_case analogue to FreeMarker\n?replace(regex, replacement) - Replaces all occurrences of the regular expression regex in the variable&#x2019;s value with the given replacement string. (since cobigen-core v1.1.0)\n?removeSuffix(suffix) - Removes the given suffix in the variable&#x2019;s value iff the variable&#x2019;s value ends with the given suffix. Otherwise nothing will happen. (since cobigen-core v1.1.0)\n?removePrefix(prefix) - Analogue to ?removeSuffix but removes the prefix of the variable&#x2019;s value. (since cobigen-core v1.1.0)\nThe attribute templateFile describes the relative path dependent on the template folder specified in the trigger to the template file to be generated.\nThe attribute mergeStrategy (optional) can be optionally specified and declares the type of merge mechanism to be used, when the destinationPath points to an already existing file. CobiGen by itself just comes with a mergeStrategy override, which enforces file regeneration in total. Additional available merge strategies have to be obtained from the different plug-in&#x2019;s documentations (see here for java, XML, properties, and text). Default: not set (means not mergeable)\nThe attribute targetCharset (optional) can be optionally specified and declares the encoding with which the contents will be written into the destination file. This also includes reading an existing file at the destination path for merging its contents with the newly generated ones. Default: UTF-8\n(Since version 4.1.0) It is possible to reference external template (templates defined on another trigger), thanks to using &lt;incrementRef &#x2026;&#x200B;&gt; that are explained here.\nTemplateScan Node\n(since cobigen-core-v1.2.0)\nThe second configuration style for template meta-data is driven by initially scanning all available templates and automatically configure them with a default set of meta-data. A scanning configuration might look like this:\nListing 98. Example of Template-scan configuration\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n&lt;templatesConfiguration xmlns=&quot;http://capgemini.com&quot;\nxmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;\nversion=&quot;1.2&quot;&gt;\n&lt;templateScans&gt;\n&lt;templateScan templatePath=&quot;templates&quot; templateNamePrefix=&quot;prefix_&quot; destinationPath=&quot;src/main/java&quot;/&gt;\n&lt;/templateScans&gt;\n&lt;/templatesConfiguration&gt;\nYou can specify multiple &lt;templateScan &#x2026;&#x200B;&gt; nodes for different templatePaths and different templateNamePrefixes.\nThe name can be specified to later on reference the templates found by a template-scan within an increment. (since cobigen-core-v2.1.)\nThe templatePath specifies the relative path from the templates.xml to the root folder from which the template scan should be performed.\nThe templateNamePrefix (optional) defines a common id prefix, which will be added to all found and automatically configured templates.\nThe destinationPath defines the root folder all found templates should be generated to, whereas the root folder will be a prefix for all found and automatically configured templates.\nA templateScan will result in the following default configuration of templates. For each file found, new template will be created virtually with the following default values:\nid: file name without .ftl extension prefixed by templateNamePrefix from template-scan\ndestinationPath: relative file path of the file found with the prefix defined by destinationPath from template-scan. Furthermore,\nit is possible to use the syntax for accessing and modifying variables as described for the attribute destinationPath of the template node, besides the only difference, that due to file system restrictions you have to replace all ?-signs (for built-ins) with #-signs.\nthe files to be scanned, should provide their final file extension by the following file naming convention: &lt;filename&gt;.&lt;extension&gt;.ftl Thus the file extension .ftl will be removed after generation.\ntemplateFile: relative path to the file found\nmergeStrategy: (optional) not set means not mergeable\ntargetCharset: (optional) defaults to UTF-8\n(Since version 4.1.0) It is possible to reference external templateScan (templateScans defined on another trigger), thanks to using &lt;incrementRef &#x2026;&#x200B;&gt; that are explained here.\nTemplateExtension Node\n(since cobigen-core-v1.2.0)\nAdditionally to the templateScan declaration it is easily possible to rewrite specific attributes for any scanned and automatically configured template.\nListing 99. Example Configuration of a TemplateExtension\n&lt;templates&gt;\n&lt;templateExtension ref=&quot;prefix_FooClass.java&quot; mergeStrategy=&quot;javamerge&quot; /&gt;\n&lt;/templates&gt;\n&lt;templateScans&gt;\n&lt;templateScan templatePath=&quot;foo&quot; templateNamePrefix=&quot;prefix_&quot; destinationPath=&quot;src/main/java/foo&quot;/&gt;\n&lt;/templateScans&gt;\nLets assume, that the above example declares a template-scan for the folder foo, which contains a file FooClass.java.ftl in any folder depth. Thus the template scan will automatically create a virtual template declaration with id=prefix_FooClass.java and further default configuration.\nUsing the templateExtension declaration above will reference the scanned template by the attribute ref and overrides the mergeStrategy of the automatically configured template by the value javamerge. Thus we are able to minimize the needed templates configuration.\n(Since version 4.1.0) It is possible to reference external templateExtension (templateExtensions defined on another trigger), thanks to using &lt;incrementRef &#x2026;&#x200B;&gt; that are explained here.\nIncrement Node\nThe &lt;increments&gt; node groups multiple &lt;increment&gt; nodes, which can be seen as a collection of templates to be generated. An increment will be defined by a unique id and a human readable description.\n&lt;increments&gt;\n&lt;increment id=&quot;...&quot; description=&quot;...&quot;&gt;\n&lt;incrementRef ref=&quot;...&quot; /&gt;\n&lt;templateRef ref=&quot;...&quot; /&gt;\n&lt;templateScanRef ref=&quot;...&quot; /&gt;\n&lt;/increment&gt;\n&lt;/increments&gt;\nAn increment might contain multiple increments and/or templates, which will be referenced using &lt;incrementRef &#x2026;&#x200B;&gt;, &lt;templateRef &#x2026;&#x200B;&gt;, resp. &lt;templateScanRef &#x2026;&#x200B;&gt; nodes. These nodes only declare the attribute ref, which will reference an increment, a template, or a template-scan by its id or name.\n(Since version 4.1.0) An special case of &lt;incrementRef &#x2026;&#x200B;&gt; is the external incrementsRef. By default, &lt;incrementRef &#x2026;&#x200B;&gt; are used to reference increments defined in the same templates.xml file. So for example, we could have:\n&lt;increments&gt;\n&lt;increment id=&quot;incA&quot; description=&quot;...&quot;&gt;\n&lt;incrementRef ref=&quot;incB&quot; /&gt;\n&lt;/increment&gt;\n&lt;increment id=&quot;incB&quot; description=&quot;...&quot;&gt;\n&lt;templateRef .... /&gt;\n&lt;templateScan .... /&gt;\n&lt;/increment&gt;\n&lt;/increments&gt;\nHowever, if we want to reference an increment that it is not defined inside our templates.xml (an increment defined for another trigger), then we can use external incrementRef as shown below:\n&lt;increment name=&quot;...&quot; description=&quot;...&quot;&gt;\n&lt;incrementRef ref=&quot;trigger_id::increment_id&quot;/&gt;\n&lt;/increment&gt;\nThe ref string is split using as delimiter ::. The first part of the string, is the trigger_id to reference. That trigger contains an increment_id. Currently, this functionality only works when both templates use the same kind of input file.\n44.1.3. Java Template Logic\nsince cobigen-core-3.0.0 which is included in the Eclipse and Maven Plugin since version 2.0.0\nIn addition, it is possible to implement more complex template logic by custom Java code. To enable this feature, you can simply import the the CobiGen_Templates by clicking on Adapt Templates, turn it into a simple maven project (if it is not already) and implement any Java logic in the common maven layout (e.g. in the source folder src/main/java). Each Java class will be instantiated by CobiGen for each generation process. Thus, you can even store any state within a Java class instance during generation. However, there is currently no guarantee according to the template processing order.\nAs a consequence, you have to implement your Java classes with a public default (non-parameter) constructor to be used by any template. Methods of the implemented Java classes can be called within templates by the simple standard FreeMarker expression for calling Bean methods: SimpleType.methodName(param1). Until now, CobiGen will shadow multiple types with the same simple name non-deterministically. So please prevent yourself from that situation.\nFinally, if you would like to do some reflection within your Java code accessing any type of the template project or any type referenced by the input, you should load classes by making use of the classloader of the util classes. CobiGen will take care of the correct classloader building including the classpath of the input source as well as of the classpath of the template project. If you use any other classloader or build it by your own, there will be no guarantee, that generation succeeds.\n44.1.4. Template Properties\nsince cobigen-core-4.0.0\nUsing a configuration with template scan, you can make use of properties in templates specified in property files named cobigen.properties next to the templates. The property files are specified as Java property files. Property files can be nested in subfolders. Properties will be resolved including property shading. Properties defined nearest to the template to be generated will take precedence.\nIn addition, a cobigen.properties file can be specified in the target folder root (in eclipse plugin, this is equal to the source project root). These properties take precedence over template properties specified in the template folder.\nNote\nIt is not allowed to override context variables in cobigen.properties specifications as we have not found any interesting use case. This is most probably an error of the template designer, CobiGen will raise an error in this case.\nMulti module support or template target path redirects\nsince cobigen-core-4.0.0\nOne special property you can specify in the template properties is the property relocate. It will cause the current folder and its subfolders to be relocated at destination path resolution time. Take the following example:\nfolder\n- sub1\nTemplate.java.ftl\ncobigen.properties\nLet the cobigen.properties file contain the line relocate=../sub2/${cwd}. Given that, the relative destination path of Template.java.ftl will be resolved to folder/sub2/Template.java. Compare template scan configuration for more information about basic path resolution. The relocate property specifies a relative path from the location of the cobigen.properties. The ${cwd} placeholder will contain the remaining relative path from the cobigen.properties location to the template file. In this basic example it just contains Template.java.ftl, but it may even be any relative path including subfolders of sub1 and its templates.\nGiven the relocate feature, you can even step out of the root path, which in general is the project/maven module the input is located in. This enables template designers to even address, e.g., maven modules located next to the module the input is coming from.\n44.1.5. Basic Template Model\nIn addition to what is served by the different model builders of the different plug-ins, CobiGen provides a minimal model based on context variables as well as CobiGen properties. The following model is independent of the input format and will be served as a template model all the time:\nvariables\nall triggered context variables mapped to its assigned/mapped value\nall template properties\nall simple names of Java template logic implementation classes\nall full qualified names of Java template logic implementation classes\nfurther input related model, e.g. model from Java inputs\n44.1.6. Plugin Mechanism\nSince cobigen-core 4.1.0, we changed the plug-in discovery mechanism. So far it was necessary to register new plugins programmatically, which introduces the need to let every tool integration, i.e. for eclipse or maven, be dependent on every plug-in, which should be released. This made release cycles take long time as all plug-ins have to be integrated into a final release of maven or eclipse integration.\nNow, plug-ins are automatically discovered by the Java Service Loader mechanism from the classpath. This also effects the setup of eclipse and maven integrations to allow modular releases of CobiGen in future. We are now able to provide faster rollouts of bug-fixes in any of the plug-ins as they can be released completely independently.\n44.2. Plug-ins\n44.2.1. Java Plug-in\nThe CobiGen Java Plug-in comes with a new input reader for java artifacts, new java related trigger and matchers, as well as a merging mechanism for Java sources.\nTrigger extension\nThe Java Plug-in provides a new trigger for Java related inputs. It accepts different representations as inputs (see Java input reader) and provides additional matching and variable assignment mechanisms. The configuration in the context.xml for this trigger looks like this:\ntype &apos;java&apos;\nListing 100. Example of a java trigger definition\n&lt;trigger id=&quot;...&quot; type=&quot;java&quot; templateFolder=&quot;...&quot;&gt;\n...\n&lt;/trigger&gt;\nThis trigger type enables Java elements as inputs.\nMatcher types\nWith the trigger you might define matchers, which restrict the input upon specific aspects:\ntype &apos;fqn&apos; &#x2192; full qualified name matching\nListing 101. Example of a java trigger definition with a full qualified name matcher\n&lt;trigger id=&quot;...&quot; type=&quot;java&quot; templateFolder=&quot;...&quot;&gt;\n&lt;matcher type=&quot;fqn&quot; value=&quot;(.+)\\.persistence\\.([^\\.]+)\\.entity\\.([^\\.]+)&quot;&gt;\n...\n&lt;/matcher&gt;\n&lt;/trigger&gt;\nThis trigger will be enabled if the full qualified name (fqn) of the declaring input class matches the given regular expression (value).\ntype &apos;package&apos; &#x2192; package name of the input\nListing 102. Example of a java trigger definition with a package name matcher\n&lt;trigger id=&quot;...&quot; type=&quot;java&quot; templateFolder=&quot;...&quot;&gt;\n&lt;matcher type=&quot;package&quot; value=&quot;(.+)\\.persistence\\.([^\\.]+)\\.entity&quot;&gt;\n...\n&lt;/matcher&gt;\n&lt;/trigger&gt;\nThis trigger will be enabled if the package name (package) of the declaring input class matches the given regular expression (value).\ntype &apos;expression&apos;\nListing 103. Example of a java trigger definition with a package name matcher\n&lt;trigger id=&quot;...&quot; type=&quot;java&quot; templateFolder=&quot;...&quot;&gt;\n&lt;matcher type=&quot;expression&quot; value=&quot;instanceof java.lang.String&quot;&gt;\n...\n&lt;/matcher&gt;\n&lt;/trigger&gt;\nThis trigger will be enabled if the expression evaluates to true. Valid expressions are\ninstanceof fqn: checks an &apos;is a&apos; relation of the input type\nisAbstract: checks, whether the input type is declared abstract\nContainerMatcher types\nAdditionally, the java plugin provides the ability to match packages (containers) as follows:\ntype &apos;package&apos;\nListing 104. Example of a java trigger definition with a container matcher for packages\n&lt;trigger id=&quot;...&quot; type=&quot;java&quot; templateFolder=&quot;...&quot;&gt;\n&lt;containerMatcher type=&quot;package&quot; value=&quot;com\\.example\\.app\\.component1\\.persistence.entity&quot; /&gt;\n&lt;/trigger&gt;\nThe container matcher matches packages provided by the type com.capgemini.cobigen.javaplugin.inputreader.to.PackageFolder with a regular expression stated in the value attribute. (See containerMatcher semantics to get more information about containerMatchers itself.)\nVariableAssignment types\nFurthermore, it provides the ability to extract information from each input for further processing in the templates. The values assigned by variable assignments will be made available in template and the destinationPath of context.xml through the namespace variables.&lt;key&gt;. The Java Plug-in currently provides two different mechanisms:\ntype &apos;regex&apos; &#x2192; regular expression group\n&lt;trigger id=&quot;...&quot; type=&quot;java&quot; templateFolder=&quot;...&quot;&gt;\n&lt;matcher type=&quot;fqn&quot; value=&quot;(.+)\\.persistence\\.([^\\.]+)\\.entity\\.([^\\.]+)&quot;&gt;\n&lt;variableAssignment type=&quot;regex&quot; key=&quot;rootPackage&quot; value=&quot;1&quot; /&gt;\n&lt;variableAssignment type=&quot;regex&quot; key=&quot;component&quot; value=&quot;2&quot; /&gt;\n&lt;variableAssignment type=&quot;regex&quot; key=&quot;pojoName&quot; value=&quot;3&quot; /&gt;\n&lt;/matcher&gt;\n&lt;/trigger&gt;\nThis variable assignment assigns the value of the given regular expression group number to the given key.\ntype &apos;constant&apos; &#x2192; constant parameter\n&lt;trigger id=&quot;...&quot; type=&quot;java&quot; templateFolder=&quot;...&quot;&gt;\n&lt;matcher type=&quot;fqn&quot; value=&quot;(.+)\\.persistence\\.([^\\.]+)\\.entity\\.([^\\.]+)&quot;&gt;\n&lt;variableAssignment type=&quot;constant&quot; key=&quot;domain&quot; value=&quot;restaurant&quot; /&gt;\n&lt;/matcher&gt;\n&lt;/trigger&gt;\nThis variable assignment assigns the value to the key as a constant.\nJava input reader\nThe Cobigen Java Plug-in implements an input reader for parsed java sources as well as for java Class&lt;?&gt; objects (loaded by reflection). So API user can pass Class&lt;?&gt; objects as well as JavaClass objects for generation. The latter depends on QDox, which will be used for parsing and merging java sources. For getting the right parsed java inputs you can easily use the JavaParserUtil, which provides static functionality to parse java files and get the appropriate JavaClass object.\nFurthermore, due to restrictions on both inputs according to model building (see below), it is also possible to provide an array of length two as an input, which contains the Class&lt;?&gt; as well as the JavaClass object of the same class.\nTemplate object model\nNo matter whether you use reflection objects or parsed java classes as input, you will get the following object model for template creation:\nclassObject (&apos;Class&apos; :: Class object of the Java input)\npojo\nname (&apos;String&apos; :: Simple name of the input class)\npackage (&apos;String&apos; :: Package name of the input class)\ncanonicalName (&apos;String&apos; :: Full qualified name of the input class)\nannotations (&apos;Map&lt;String, Object&gt;&apos; :: Annotations, which will be represented by a mapping of the full qualified type of an annotation to its value. To gain template compatibility, the key will be stored with &apos;_&apos; instead of &apos;.&apos; in the full qualified annotation type. Furthermore, the annotation might be recursively defined and thus be accessed using the same type of mapping. Example ${pojo.annotations.javax_persistence_Id})\njavaDoc (&apos;Map&lt;String, Object&gt;&apos;) :: A generic way of addressing all available javaDoc doclets and comments. The only fixed variable is comment (see below). All other provided variables depend on the doclets found while parsing. The value of a doclet can be accessed by the doclets name (e.g. ${&#x2026;&#x200B;javaDoc.author}). In case of doclet tags that can be declared multiple times (currently @param and @throws), you will get a map, which you access in a specific way (see below).\ncomment (&apos;String&apos; :: javaDoc comment, which does not include any doclets)\nparams (&apos;Map&lt;String,String&gt; :: javaDoc parameter info. If the comment follows proper conventions, the key will be the name of the parameter and the value being its description. You can also access the parameters by their number, as in arg0, arg1 etc, following the order of declaration in the signature, not in order of javadoc)\nthrows (&apos;Map&lt;String,String&gt; :: javaDoc exception info. If the comment follows proper conventions, the key will be the name of the thrown exception and the value being its description)\nextendedType (&apos;Map&lt;String, Object&gt;&apos; :: The supertype, represented by a set of mappings (since cobigen-javaplugin v1.1.0)\nname (&apos;String&apos; :: Simple name of the supertype)\ncanonicalName (&apos;String&apos; :: Full qualified name of the supertype)\npackage (&apos;String&apos; :: Package name of the supertype)\nimplementedTypes (&apos;List&lt;Map&lt;String, Object&gt;&gt;&apos; :: A list of all implementedTypes (interfaces) represented by a set of mappings (since cobigen-javaplugin v1.1.0)\ninterface (&apos;Map&lt;String, Object&gt;&apos; :: List element)\nname (&apos;String&apos; :: Simple name of the interface)\ncanonicalName (&apos;String&apos; :: Full qualified name of the interface)\npackage (&apos;String&apos; :: Package name of the interface)\nfields (&apos;List&lt;Map&lt;String, Object&gt;&gt;&apos; :: List of fields of the input class) (renamed since cobigen-javaplugin v1.2.0; previously attributes)\nfield (&apos;Map&lt;String, Object&gt;&apos; :: List element)\nname (&apos;String&apos; :: Name of the Java field)\ntype (&apos;String&apos; :: Type of the Java field)\ncanonicalType (&apos;String&apos; :: Full qualified type declaration of the Java field&#x2019;s type)\n&apos;isId&apos; (&apos;Deprecated&apos; :: &apos;boolean&apos; :: true if the Java field or its setter or its getter is annotated with the javax.persistence.Id annotation, false otherwise. Equivalent to ${pojo.attributes[i].annotations.javax_persistence_Id?has_content})\njavaDoc (see pojo.javaDoc)\nannotations (see pojo.annotations with the remark, that for fields all annotations of its setter and getter will also be collected)\nmethodAccessibleFields (&apos;List&lt;Map&lt;String, Object&gt;&gt;&apos; :: List of fields of the input class or its inherited classes, which are accessible using setter and getter methods)\nsame as for field (but without javaDoc!)\nmethods (&apos;List&lt;Map&lt;String, Object&gt;&gt;&apos; :: The list of all methods, whereas one method will be represented by a set of property mappings)\nmethod (&apos;Map&lt;String, Object&gt;&apos; :: List element)\nname (&apos;String&apos; :: Name of the method)\njavaDoc (see pojo.javaDoc)\nannotations (see pojo.annotations)\nFurthermore, when providing a Class&lt;?&gt; object as input, the Java Plug-in will provide additional functionalities as template methods (deprecated):\nisAbstract(String fqn) (Checks whether the type with the given full qualified name is an abstract class. Returns a boolean value.) (since cobigen-javaplugin v1.1.1) (deprecated)\nisSubtypeOf(String subType, String superType) (Checks whether the subType declared by its full qualified name is a sub type of the superType declared by its full qualified name. Equals the Java expression subType instanceof superType and so also returns a boolean value.) (since cobigen-javaplugin v1.1.1) (deprecated)\nModel Restrictions\nAs stated before both inputs (Class&lt;?&gt; objects and JavaClass objects ) have their restrictions according to model building. In the following these restrictions are listed for both models, the ParsedJava Model which results from an JavaClass input and the ReflectedJava Model, which results from a Class&lt;?&gt;` input.\nIt is important to understand, that these restrictions are only present if you work with either Parsed Model OR the Reflected Model. If you use the Maven Build Plug-in or Eclipse Plug-in these two models are merged together so that they can mutually compensate their weaknesses.\nParsed Model\nannotations of the input&#x2019;s supertype are not accessible due to restrictions in the QDox library. So pojo.methodAccessibleFields[i].annotations will always be empty for super type fields.\nannotations&apos; parameter values are available as Strings only (e.g. the Boolean value true is transformed into &quot;true&quot;). This also holds for the Reflected Model.\nfields of &quot;supersupertypes&quot; of the input JavaClass are not available at all. So pojo.methodAccessibleFields will only contain the input type&#x2019;s and the direct superclass&#x2019;s fields.\n[resolved, since cobigen-javaplugin 1.3.1] field types of supertypes are always canonical. So pojo.methodAccessibleFields[i].type will always provide the same value as pojo.methodAccessibleFields[i].canonicalType (e.g. java.lang.String instead of the expected String) for super type fields.\nReflected Model\nannotations&apos; parameter values are available as Strings only (e.g. the Boolean value true is transformed into &quot;true&quot;). This also holds for the Parsed Model.\nannotations are only available if the respective annotation has @Retention(value=RUNTIME), otherwise the annotations are to be discarded by the compiler or by the VM at run time. For more information see RetentionPolicy.\ninformation about generic types is lost. E.g. a field&#x2019;s/ methodAccessibleField&#x2019;s type for List&lt;String&gt; can only be provided as List&lt;?&gt;.\nMerger extensions\nThe Java Plug-in provides two additional merging strategies for Java sources, which can be configured in the templates.xml:\nMerge strategy javamerge (merges two Java resources and keeps the existing Java elements on conflicts)\nMerge strategy javamerge_override (merges two Java resources and overrides the existing Java elements on conflicts)\nIn general merging of two Java sources will be processed as follows:\nPrecondition of processing a merge of generated contents and existing ones is a common Java root class resp. surrounding class. If this is the case this class and all further inner classes will be merged recursively. Therefore, the following Java elements will be merged and conflicts will be resolved according to the configured merge strategy:\nextends and implements relations of a class: Conflicts can only occur for the extends relation.\nAnnotations of a class: Conflicted if an annotation declaration already exists.\nFields of a class: Conflicted if there is already a field with the same name in the existing sources. (Will be replaced / ignored in total, also including annotations)\nMethods of a class: Conflicted if there is already a method with the same signature in the existing sources. (Will be replaced / ignored in total, also including annotations)\n44.2.2. Property Plug-in\nThe CobiGen Property Plug-in currently only provides different merge mechanisms for documents written in Java property syntax.\nMerger extensions\nThere are two merge strategies for Java properties, which can be configured in the templates.xml:\nMerge strategy propertymerge (merges two properties documents and keeps the existing properties on conflicts)\nMerge strategy propertymerge_override (merges two properties documents and overrides the existing properties on conflicts)\nBoth documents (base and patch) will be parsed using the Java 7 API and will be compared according their keys. Conflicts will occur if a key in the patch already exists in the base document.\n44.2.3. XML Plug-in\nThe CobiGen XML Plug-in comes with an input reader for xml artifacts, xml related trigger and matchers and provides different merge mechanisms for XML result documents.\nTrigger extension\n(since cobigen-xmlplugin v2.0.0)\nThe XML Plug-in provides a trigger for xml related inputs. It accepts xml documents as input (see XML input reader) and provides additional matching and variable assignment mechanisms. The configuration in the context.xml for this trigger looks like this:\ntype &apos;xml&apos;\nListing 105. Example of a xml trigger definition.\n&lt;trigger id=&quot;...&quot; type=&quot;xml&quot; templateFolder=&quot;...&quot;&gt;\n...\n&lt;/trigger&gt;\nThis trigger type enables xml documents as inputs.\ntype &apos;xpath&apos;\nListing 106. Example of a xpath trigger definition.\n&lt;trigger id=&quot;...&quot; type=&quot;xpath&quot; templateFolder=&quot;...&quot;&gt;\n...\n&lt;/trigger&gt;\nThis trigger type enables xml documents as container inputs, which consists of several subdocuments.\nContainerMatcher type\nA ContainerMatcher check if the input is a valid container.\nxpath: type: &apos;xpath&apos;\nListing 107. Example of a xml trigger definition with a nodename matcher.\n&lt;trigger id=&quot;...&quot; type=&quot;xml&quot; templateFolder=&quot;...&quot;&gt;\n&lt;containerMatcher type=&quot;xpath&quot; value=&quot;./uml:Model//packagedElement[@xmi:type=&apos;uml:Class&apos;]&quot;&gt;\n...\n&lt;/matcher&gt;\n&lt;/trigger&gt;\nBefore applying any Matcher, this containerMatcher checks if the XML file contains a node &quot;uml:Model&quot; with a childnode &quot;packagedElement&quot; which contains an attribute &quot;xmi:type&quot; with the value &quot;uml:Class&quot;.\nMatcher types\nWith the trigger you might define matchers, which restrict the input upon specific aspects:\nxml: type &apos;nodename&apos; &#x2192; document&#x2019;s root name matching\nListing 108. Example of a xml trigger definition with a nodename matcher\n&lt;trigger id=&quot;...&quot; type=&quot;xml&quot; templateFolder=&quot;...&quot;&gt;\n&lt;matcher type=&quot;nodename&quot; value=&quot;\\D\\w*&quot;&gt;\n...\n&lt;/matcher&gt;\n&lt;/trigger&gt;\nThis trigger will be enabled if the root name of the declaring input document matches the given regular expression (value).\nxpath: type: &apos;xpath&apos; &#x2192; matching a node with a xpath value\nListing 109. Example of a xpath trigger definition with a xpath matcher.\n&lt;trigger id=&quot;...&quot; type=&quot;xml&quot; templateFolder=&quot;...&quot;&gt;\n&lt;matcher type=&quot;xpath&quot; value=&quot;/packagedElement[@xmi:type=&apos;uml:Class&apos;]&quot;&gt;\n...\n&lt;/matcher&gt;\n&lt;/trigger&gt;\nThis trigger will be enabled if the XML file contains a node &quot;/packagedElement&quot; where the &quot;xmi:type&quot; property equals &quot;uml:Class&quot;.\nVariableAssignment types\nFurthermore, it provides the ability to extract information from each input for further processing in the templates. The values assigned by variable assignments will be made available in template and the destinationPath of context.xml through the namespace variables.&lt;key&gt;. The XML Plug-in currently provides only one mechanism:\ntype &apos;constant&apos; &#x2192; constant parameter\n&lt;trigger id=&quot;...&quot; type=&quot;xml&quot; templateFolder=&quot;...&quot;&gt;\n&lt;matcher type=&quot;nodename&quot; value=&quot;\\D\\w*&quot;&gt;\n&lt;variableAssignment type=&quot;constant&quot; key=&quot;domain&quot; value=&quot;restaurant&quot; /&gt;\n&lt;/matcher&gt;\n&lt;/trigger&gt;\nThis variable assignment assigns the value to the key as a constant.\nXML input reader\nThe Cobigen XML Plug-in implements an input reader for parsed xml documents. So API user can pass org.w3c.dom.Document objects for generation. For getting the right parsed xml inputs you can easily use the xmlplugin.util.XmlUtil, which provides static functionality to parse xml files or input streams and get the appropriate Document object.\nTemplate object\nDue to the heterogeneous structure an xml document can have, the xml input reader does not always create exactly the same model structure (in contrast to the java input reader). For example the model&#x2019;s depth differs strongly, according to it&#x2019;s input document. To allow navigational access to the nodes, the model also depends on the document&#x2019;s element&#x2019;s node names. All child elements with unique names, are directly accessible via their names. In addition it is possible to iterate over all child elements with held of the child list Children. So it is also possible to access child elements with non unique names.\nThe XML input reader will create the following object model for template creation (EXAMPLEROOT, EXAMPLENODE1, EXAMPLENODE2, EXAMPLEATTR1,&#x2026;&#x200B; are just used here as examples. Of course they will be replaced later by the actual node or attribute names):\n~EXAMPLEROOT~ (&apos;Map&lt;String, Object&gt;&apos; :: common element structure)\n_nodeName_ (&apos;String&apos; :: Simple name of the root node)\n_text_ (&apos;String&apos; :: Concatenated text content (PCDATA) of the root node)\nTextNodes (&apos;List&lt;String&gt;&apos; :: List of all the root&#x2019;s text node contents)\n_at_~EXAMPLEATTR1~ (&apos;String&apos; :: String representation of the attribute&#x2019;s value)\n_at_~EXAMPLEATTR2~ (&apos;String&apos; :: String representation of the attribute&#x2019;s value)\n_at_&#x2026;&#x200B;\nAttributes (&apos;List&lt;Map&lt;String, Object&gt;&gt;&apos; :: List of the root&#x2019;s attributes\nat (&apos;Map&lt;String, Object&gt;&apos; :: List element)\n_attName_ (&apos;String&apos; :: Name of the attribute)\n_attValue_ (&apos;String&apos; :: String representation of the attribute&#x2019;s value)\nChildren (&apos;List&lt;Map&lt;String, Object&gt;&gt;&apos; :: List of the root&#x2019;s child elements\nchild (&apos;Map&lt;String, Object&gt;&apos; :: List element)\n&#x2026;&#x200B;common element sub structure&#x2026;&#x200B;\n~EXAMPLENODE1~ (&apos;Map&lt;String, Object&gt;&apos; :: One of the root&#x2019;s child nodes)\n&#x2026;&#x200B;common element structure&#x2026;&#x200B;\n~EXAMPLENODE2~ (&apos;Map&lt;String, Object&gt;&apos; :: One of the root&#x2019;s child nodes)\n&#x2026;&#x200B;common element sub structure&#x2026;&#x200B;\n~EXAMPLENODE21~ (&apos;Map&lt;String, Object&gt;&apos; :: One of the nodes&apos; child nodes)\n&#x2026;&#x200B;common element structure&#x2026;&#x200B;\n~EXAMPLENODE&#x2026;&#x200B;~\n~EXAMPLENODE&#x2026;&#x200B;~\nIn contrast to the java input reader, this xml input reader does currently not provide any additional template methods.\nMerger extensions\nThe XML plugin uses the LeXeMe merger library to produce semantically correct merge products. The following four merge strategies are implemented and can be configured in the templates.xml:\nxmlmerge: In case of a conflict the base value is preferred\nxmlmerge_override: In case of a conflict the patch value is preferred\nxmlmerge_attachTexts: In case of a conflict the base value is preferred. Attributes and text nodes will be merged where possible\nxmlmerge_override_attachTexts: In case of a conflict the patch value is preferred. Attributes and text nodes will be merged where possible\nCurrently only the document types included in LeXeMe are supported.\nOn how the merger works consult the LeXeMe Wiki.\n44.2.4. Text Merger Plug-in\nThe Text Merger Plug-in enables merging result free text documents to existing free text documents. Therefore, the algorithms are also very rudimentary.\nMerger extensions\nThere are currently three main merge strategies that apply for the whole document:\nmerge strategy textmerge_append (appends the text directly to the end of the existing document)\n_Remark_: If no anchors are defined, this will simply append the patch.\nmerge strategy textmerge_appendWithNewLine (appends the text after adding a new line break to the existing document)\n_Remark_: empty patches will not result in appending a new line any more since v1.0.1\nRemark: Only suitable if no anchors are defined, otherwise it will simply act as textmerge_append\nmerge strategy textmerge_override (replaces the contents of the existing file with the patch)\n_Remark_: If anchors are defined, override is set as the default mergestrategy for every text block if not redefined in an anchor specification.\nAnchor functionality\nIf a template contains text that fits the definition of anchor:${documentpart}:${mergestrategy}:anchorend or more specifically the regular expression (.*)anchor:([:]+):(newline_)?([:]+)(_newline)?:anchorend\\\\s*(\\\\r\\\\n|\\\\r|\\\\n), some additional functionality becomes available about specific parts of the incoming text and the way it will be merged with the existing text. These anchors always change things about the text to come up until the next anchor, text before it is ignored.\nIf no anchors are defined, the complete patch will be appended depending on your choice for the template in the file templates.xml.\nAnchor Definition\nAnchors should always be defined as a comment of the language the template results in, as you do not want them to appear in your readable version, but cannot define them as freemarker comments in the template, or the merger will not know about them.\nAnchors will also be read when they are not comments due to the merger being able to merge multiple types of text-based languages, thus making it practically impossible to filter for the correct comment declaration. That is why anchors have to always be followed by line breaks. That way there is a universal way to filter anchors that should have anchor functionality and ones that should appear in the text.\nRemark: If the resulting language has closing tags for comments, they have to appear in the next line.\nRemark: If you do not put the anchor into a new line, all the text that appears before it will be added to the anchor.\nDocumentparts\nIn general, ${documentpart} is an id to mark a part of the document, that way the merger knows what parts of the text to merge with which parts of the patch (e.g. if the existing text contains anchor:table:${}:anchorend that part will be merged with the part tagged anchor:table:${}:anchorend of the patch).\nIf the same documentpart is defined multiple times, it can lead to errors, so instead of defining table multiple times, use table1, table2, table3 etc.\nIf a ${documentpart} is defined in the document but not in the patch and they are in the same position, it is processed in the following way: If only the documentparts header, test and footer are defined in the document in that order, and the patch contains header, order and footer, the resulting order will be header, test, order then footer.\nThe following documentparts have default functionality:\nanchor:header:${mergestrategy}:anchorend marks the beginning of a header, that will be added once when the document is created, but not again.\nRemark: This is only done once, if you have header in another anchor, it will be ignored\nanchor:footer:${mergestrategy}:anchorend marks the beginning of a footer, that will be added once when the document is created, but not again. Once this is invoked, all following text will be included in the footer, including other anchors.\nMergestrategies\nMergestrategies are only relevant in the patch, as the merger is only interested in how text in the patch should be managed, not how it was managed in the past.\nanchor:${documentpart}::anchorend will use the merge strategy from templates.xml, see Merger-Extensions.\nanchor:${}:${mergestrategy}_newline:anchorend or anchor:${}:newline_${mergestrategy}:anchorend states that a new line should be appended before or after this anchors text, depending on where the newline is (before or after the mergestrategy). anchor:${documentpart}:newline:anchorend puts a new line after the anchors text.\nRemark: Only works with appending strategies, not merging/replacing ones. These strategies currently include: appendbefore, append/appendafter\nanchor:${documentpart}:override:anchorend means that the new text of this documentpart will replace the existing one completely\nanchor:${documentpart}:appendbefore:anchorend or anchor:${documentpart}:appendafter:anchorend/anchor:${documentpart}:append:anchorend specifies whether the text of the patch should come before the existing text or after.\nUsage Examples\nGeneral\nBelow you can see how a file with anchors might look like (using Asciidoc comment tags), with examples of what you might want to use the different functions for.\n// anchor:header:append:anchorend\nTable of contents\nIntroduction/Header\n// anchor:part1:appendafter:anchorend\nLists\nTable entries\n// anchor:part2:nomerge:anchorend\nDocument Separators\nAsciidoc table definitions\n// anchor:part3:override:anchorend\nAnything that you only want once but changes from time to time\n// anchor:footer:append:anchorend\nCopyright Info\nImprint\nMerging\nIn this section you will see a comparison on what files look like before and after merging\noverride\nListing 110. Before\n// anchor:part:override:anchorend\nLorem Ipsum\nListing 111. Patch\n// anchor:part:override:anchorend\nDolor Sit\nListing 112. After\n// anchor:part:override:anchorend\nDolor Sit\nAppending\nListing 113. Before\n// anchor:part:append:anchorend\nLorem Ipsum\n// anchor:part2:appendafter:anchorend\nLorem Ipsum\n// anchor:part3:appendbefore:anchorend\nLorem Ipsum\nListing 114. Patch\n// anchor:part:append:anchorend\nDolor Sit\n// anchor:part2:appendafter:anchorend\nDolor Sit\n// anchor:part3:appendbefore:anchorend\nDolor Sit\nListing 115. After\n// anchor:part:append:anchorend\nLorem Ipsum\nDolor Sit\n// anchor:part2:appendafter:anchorend\nLorem Ipsum\nDolor Sit\n// anchor:part3:appendbefore:anchorend\nDolor Sit\nLorem Ipsum\nNewline\nListing 116. Before\n// anchor:part:newline_append:anchorend\nLorem Ipsum\n// anchor:part:append_newline:anchorend\nLorem Ipsum\n(end of file)\nListing 117. Patch\n// anchor:part:newline_append:anchorend\nDolor Sit\n// anchor:part:append_newline:anchorend\nDolor Sit\n(end of file)\nListing 118. After\n// anchor:part:newline_append:anchorend\nLorem Ipsum\nDolor Sit\n// anchor:part:append_newline:anchorend\nLorem Ipsum\nDolor Sit\n(end of file)\nError List\nIf there are anchors in the text, but either base or patch do not start with one, the merging process wil be aborted, as text might go missing this way.\nUsing _newline or newline_ with mergestrategies that don&#x2019;t support it , like override, will abort the merging process. See Merge Strategies &#x2192;2 for details.\nUsing undefined mergestrategies will abort the merging process.\nWrong anchor definitions, for example anchor:${}:anchorend will abort the merging process, see Anchor Definition for details.\n44.2.5. JSON Plug-in\nAt the moment the plug-in can be used for merge generic JSOn files depending on the merge strategy defined at the templates.\nMerger extensions\nThere are currently these merge strategies:\nGeneric JSON Merge\nmerge strategy jsonmerge(add the new code respecting the existent is case of conflict)\nmerge strategy jsonmerge_override (add the new code overwriting the existent in case of conflict)\nJsonArray&#x2019;s will be ignored / replaced in total\nJsonObjects in conflict will be processed recursively ignoring adding non existent elements.\nMerge Process\nGeneric JSON Merging\nThe merge process will be:\nAdd non existent JSON Objects from patch file to base file.\nFor existent object in both files, will add non existent keys from patch to base object. This process will be done recursively for all existent objects.\nFor Json Arrays existent in both files, the arrays will be just concatenated.\n44.2.6. TypeScript Plug-in\nThe TypeScript Plug-in enables merging result TS files to existing ones. This plug-in is used at the moment for generate an Angular2 client with all CRUD functionalities enabled. The plug-in also generates de i18n functionality just appending at the end of the word the ES or EN suffixes, to put into the developer knowledge that this words must been translated to the correspondent language. Currently, the generation of Angular2 client requires an ETO java object as input so, there is no need to implement an input reader for ts artifacts for the moment.\nTrigger Extensions\nAs for the Angular2 generation the input is a java object, the trigger expressions (including matchers and variable assignments) are implemented as Java.\nMerger extensions\nThis plugin uses the OASP TypeScript Merger to merge files. There are currently two merge strategies:\nmerge strategy tsmerge (add the new code respecting the existing is case of conflict)\nmerge strategy tsmerge_override (add the new code overwriting the existent in case of conflict)\nThe merge algorithm mainly handles the following AST nodes:\nImportDeclaration\nWill add non existent imports whatever the merge strategy is.\nFor different imports from same module, the import clauses will be merged.\nimport { a } from &apos;b&apos;;\nimport { c } from &apos;b&apos;;\n//Result\nimport { a, c } from &apos;b&apos;;\nClassDeclaration\nAdds non existent base properties from patch based on the name property.\nAdds non existent base methods from patch based on the name signature.\nAdds non existent annotations to class, properties and methods.\nPropertyDeclaration\nAdds non existent decorators.\nMerge existent decorators.\nWith override strategy, the value of the property will be replaced by the patch value.\nMethodDeclaration\nWith override strategy, the body will be replaced.\nThe parameters will be merged.\nParameterDeclaration\nReplace type and modifiers with override merge strategy, adding non existent from patch into base.\nConstructorDeclaration\nMerged in the same way as Method is.\nFunctionDeclaration\nMerged in the same way as Method is.\nInput reader\nThe TypeScript input reader is based on the one that the TypeScript merger uses. The current extensions are additional module fields giving from which library any entity originates.\nmodule: null specifies a standard entity or type as string or number.\nObject model\nTo get a first impression of the created object after parsing, let us start with analyzing a small example, namely the parsing of a simple type-orm model written in TypeScript.\nimport {Entity, PrimaryGeneratedColumn, Column} from &quot;typeorm&quot;;\n@Entity()\nexport class User {\n@PrimaryGeneratedColumn()\nid: number;\n@Column()\nfirstName: string;\n@Column()\nlastName: string;\n@Column()\nage: number;\n}\nThe returned object has the following structure\n{\n&quot;importDeclarations&quot;: [\n{\n&quot;module&quot;: &quot;typeorm&quot;,\n&quot;named&quot;: [\n&quot;Entity&quot;,\n&quot;PrimaryGeneratedColumn&quot;,\n&quot;Column&quot;\n],\n&quot;spaceBinding&quot;: true\n}\n],\n&quot;classes&quot;: [\n{\n&quot;identifier&quot;: &quot;User&quot;,\n&quot;modifiers&quot;: [\n&quot;export&quot;\n],\n&quot;decorators&quot;: [\n{\n&quot;identifier&quot;: {\n&quot;name&quot;: &quot;Entity&quot;,\n&quot;module&quot;: &quot;typeorm&quot;\n},\n&quot;isCallExpression&quot;: true\n}\n],\n&quot;properties&quot;: [\n{\n&quot;identifier&quot;: &quot;id&quot;,\n&quot;type&quot;: {\n&quot;name&quot;: &quot;number&quot;,\n&quot;module&quot;: null\n},\n&quot;decorators&quot;: [\n{\n&quot;identifier&quot;: {\n&quot;name&quot;: &quot;PrimaryGeneratedColumn&quot;,\n&quot;module&quot;: &quot;typeorm&quot;\n},\n&quot;isCallExpression&quot;: true\n}\n]\n},\n{\n&quot;identifier&quot;: &quot;firstName&quot;,\n&quot;type&quot;: {\n&quot;name&quot;: &quot;string&quot;,\n&quot;module&quot;: null\n},\n&quot;decorators&quot;: [\n{\n&quot;identifier&quot;: {\n&quot;name&quot;: &quot;Column&quot;,\n&quot;module&quot;: &quot;typeorm&quot;\n},\n&quot;isCallExpression&quot;: true\n}\n]\n},\n{\n&quot;identifier&quot;: &quot;lastName&quot;,\n&quot;type&quot;: {\n&quot;name&quot;: &quot;string&quot;,\n&quot;module&quot;: null\n},\n&quot;decorators&quot;: [\n{\n&quot;identifier&quot;: {\n&quot;name&quot;: &quot;Column&quot;,\n&quot;module&quot;: &quot;typeorm&quot;\n},\n&quot;isCallExpression&quot;: true\n}\n]\n},\n{\n&quot;identifier&quot;: &quot;age&quot;,\n&quot;type&quot;: {\n&quot;name&quot;: &quot;number&quot;,\n&quot;module&quot;: null\n},\n&quot;decorators&quot;: [\n{\n&quot;identifier&quot;: {\n&quot;name&quot;: &quot;Column&quot;,\n&quot;module&quot;: &quot;typeorm&quot;\n},\n&quot;isCallExpression&quot;: true\n}\n]\n}\n]\n}\n]\n}\nIf we only consider the first level of the JSON response, we spot two lists of imports and classes, providing information about the only import statement and the only User class, respectively. Moving one level deeper we observe that:\nEvery import statement is translated to an import declaration entry in the declarations list, containing the module name, as well as a list of entities imported from the given module.\nEvery class entry provides besides the class identifier, its decoration(s), modifier(s), as well as a list of properties that the original class contains.\nNote that, for each given type, the module from which it is imported is also given as in\n&quot;identifier&quot;: {\n&quot;name&quot;: &quot;Column&quot;,\n&quot;module&quot;: &quot;typeorm&quot;\n}\nReturning to the general case, independently from the given TypeScript file, an object having the following Structure will be created.\nimportDeclarations: A list of import statement as described above\nexportDeclarations: A list of export declarations\nclasses: A list of classes extracted from the given file, where each entry is full of class specific fields, describing its properties and decorator for example.\ninterfaces: A list of interfaces.\nvariables: A list of variables.\nfunctions: A list of functions.\nenums: A list of enumerations.\n44.2.7. HTML Plug-in\nThe HTML Plug-in enables merging result HTML files to existing ones. This plug-in is used at the moment for generate an Angular2 client. Currently, the generation of Angular2 client requires an ETO java object as input so, there is no need to implement an input reader for ts artifacts for the moment.\nTrigger Extensions\nAs for the Angular2 generation the input is a java object, the trigger expressions (including matchers and variable assignments) are implemented as Java.\nMerger extensions\nThere are currently two merge strategies:\nmerge strategy html-ng* (add the new code respecting the existing is case of conflict)\nmerge strategy html-ng*_override (add the new code overwriting the existent in case of conflict)\nThe merging of two Angular2 files will be processed as follows:\nThe merge algorithm handles the following AST nodes:\nmd-nav-list\na\nform\nmd-input-container\ninput\nname (for name attribute)\nngIf\nWarning\nBe aware, that the HTML merger is not generic and only handles the described tags needed for merging code of a basic Angular client implementation. For future versions, it is planned to implement a more generic solution.\n&#x2190;&#xA0;Previous:&#xA0;Document Description&#xA0;| &#x2191;&#xA0;Up:&#xA0;CobiGen&#x2009;&#x2014;&#x2009;Code-based incremental Generator&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Maven Build Integration&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-cobigen.asciidoc_document-description.html","title":"43. Document Description","body":"\n43. Document Description\nThis document contains the documentation of the CobiGen core module as well as all CobiGen plug-ins and the CobiGen eclipse integration.\nCurrent versions:\nCobiGen - Eclipse Plug-in v4.4.4\nCobiGen - Maven Build Plug-in v4.1.0\nCobiGen v5.4.0\nCobiGen - Java Plug-in v2.1.0\nCobiGen - XML Plug-in v4.1.0\nCobiGen - TypeScript Plug-in v2.3.3\nCobiGen - Property Plug-in v2.0.0\nCobiGen - Text Merger v2.0.0\nCobiGen - JSON Plug-in v2.0.0\nCobiGen - HTML Plug-in v2.0.1\nCobiGen - Open API Plug-in v2.3.0\nCobiGen - FreeMaker Template Engine v2.0.0\nCobiGen - Velocity Template Engine v2.0.0\nAuthors:\nMalte Brunnlieb\nJaime Diaz Gonzalez\nSteffen Holzer\nRuben Diaz Martinez\nJoerg Hohwiller\nFabian Kreis\nLukas Goerlach\nKrati Shah\nChristian Richter\nErik Gr&#xFC;ner\nMike Schumacher\nMarco Rose\nMohamed Ghanmi\n43.1. Guide to the Reader\nDependent on the intention you are reading this document, you might be most interested in the following chapters:\nIf this is your first contact with CobiGen, you will be interested in the general purpose of CobiGen, in the licensing of CobiGen, as well as in the Shared Service provided for CobiGen. Additionally, there are some general use cases, which are currently implemented and maintained to be used out of the box.\nAs a user of the CobiGen Eclipse integration, you should focus on the Installation and Usage chapters to get a good introduction how to use CobiGen in eclipse.\nAs a user of the Maven integration, you should focus on the Maven configuration chapter, which guides you through the integration if CobiGen into your build configuration.\nIf you like to adapt the configuration of CobiGen, you have to step deeper into the configuration guide as well as into the plug-in configuration extensions for the Java Plug-in, XML-Plugin, Java Property Plug-in, as well as for the Text-Merger Plug-in.\nFinally, if want to develop your own templates, you will be thankful for helpful links in addition to the plug-ins documentation as referenced in the previous point.\n43.2. CobiGen - Code-based incremental Generator\n43.2.1. Overview\nCobiGen is a generic incremental generator for end to end code generation tasks, mostly used in Java projects.\nDue to a template-based approach, CobiGen generates any set of text-based documents and document fragments.\nInput (currently):\nJava classes\nXML-based files\nOpenAPI documents\nPossibly more inputs like WSDL, which is currently not implemented.\nOutput:\nany text-based document or document fragments specified by templates\n43.2.2. Architecture\nCobiGen is build as an extensible framework for incremental code generation. It provides extension points for new input readers which allow reading new input types and converting them to an internally processed model. The model is used to process templates of different kinds to generate patches. The template processing will be done by different template engines. There is an extension point for template engines to support multiple ones as well. Finally, the patch will be structurally merged into potentially already existing code. To allow structural merge on different programming languages, the extension point for structural mergers has been introduced. Here you will see an overview of the currently available extension points and plug-ins:\n43.2.3. Features and Characteristics\nGenerate fresh files across all the layers of a application - ready to run.\nAdd on to existing files merging code into it. E.g. generate new methods into existing java classes or adding nodes to an XML file. Merging of contents into existing files will be done using structural merge mechanisms.\nStructural merge mechanisms are currently implemented for Java, XML, Java Property Syntax, JSON, Basic HTML, Text Append, TypeScript.\nConflicts can be resolved individually but automatically by former configuration for each template.\nCobiGen provides an Eclipse integration as well as a Maven Integration.\nCobiGen comes with an extensive documentation for users and developers.\nTemplates can be fully tailored to project needs - this is considered as a simple task.\n43.2.4. Selection of current and past CobiGen applications\nGeneral applications:\nGeneration of a Java CRUD application based on devonfw architecture including all software-layers on the server plus code for js-clients (Angular). You can find details here.\nGeneration of a Java CRUD application according to the Register Factory architecture. Persistence entities are the input for generation.\nGeneration of builder classes for generating test data for JUnit-Tests. Input are the persistence entities.\nGeneration of a EXT JS 6 client with full CRUD operations connected a devon4j server.\nGeneration of a Angular 6 client with full CRUD operations connected a devon4j server.\nProject-specific applications in the past:\nGeneration of an additional Java type hierarchy on top of existing Java classes in combination with additional methods to be integrated in the modified classes. Hibernate entities were considered as input as well as output of the generation. The rational in this case, was to generate an additional business object hierarchy on top of an existing data model for efficient business processing.\nGeneration of hash- and equals-methods as well as copy constructors depending on the field types of the input Java class. Furthermore, CobiGen is able to re-generate these methods/constructors triggered by the user, i.e, when fields have been changed.\nExtraction of JavaDoc of test classes and their methods for generating a csv test documentation. This test documentation has been further processed manually in Excel to provide a good overview about the currently available tests in the software system, which enables further human analysis.\n43.3. General use cases\nIn addition to the selection of CobiGen applications introduced before, this chapter provides a more detailed overview about the currently implemented and maintained general use cases. These can be used by any project following a supported reference architecture as e.g. the devonfw or Register Factory.\n43.3.1. devon4j\nWith our templates for devon4j, you can generate a whole CRUD application from a single Entity class. You save the effort for creating, DAOs, Transfer Objects, simple CRUD use cases with REST services and even the client application can be generated.\nCRUD server application for devon4j\nFor the server, the required files for all architectural layers (Data access, logic, and service layer) can be created based on your Entity class. After the generation, you have CRUD functionality for the entity from bottom to top which can be accessed via a RESTful web service. Details are provided in the Devon wiki.\nCRUD client application for devon4ng\nBased on the REST services on the server, you can also generate an Angular client based on devon4ng. With the help of Node.js, you have a working client application for displaying your entities within minutes!\nTestdata Builder for devon4j\nGenerating a builder pattern for POJOs to easily create test data in your tests. CobiGen is not only able to generate a plain builder pattern but rather builder, which follow a specific concept to minimize test data generation efforts in your unit tests. The following Person class as an example:\nListing 88. Person class\npublic class Person {\nprivate String firstname;\nprivate String lastname;\nprivate int birthyear;\n@NotNull\nprivate Address address;\n@NotNull\npublic String getFirstname() {\nreturn this.firstname;\n}\n// additional default setter and getter\n}\nIt is a simple POJO with a validation annotation, to indicate, that firstname should never be null. Creating this object in a test would imply to call every setter, which is kind of nasty. Therefore, the Builder Pattern has been introduced for quite a long time in software engineering, allowing to easily create POJOs with a fluent API. See below.\nListing 89. Builder pattern example\nPerson person = new PersonBuilder()\n.firstname(&quot;Heinz&quot;)\n.lastname(&quot;Erhardt&quot;)\n.birthyear(1909)\n.address(\nnew AddressBuilder().postcode(&quot;22222&quot;)\n.city(&quot;Hamburg&quot;).street(&quot;Luebecker Str. 123&quot;)\n.createNew())\n.addChild(\nnew PersonBuilder()[...].createNew()).createNew();\nThe Builder API generated by CobiGen allows you to set any setter accessible field of a POJO in a fluent way. But in addition lets assume a test, which should check the birth year as precondition for any business operation. So specifying all other fields of Person, especially firstname as it is mandatory to enter business code, would not make sense. The test behavior should just depend on the specification of the birth year and on no other data. So we would like to just provide this data to the test.\nThe Builder classes generated by CobiGen try to tackle this inconvenience by providing the ability to declare default values for any mandatory field due to validation or database constraints.\nListing 90. Builder Outline\npublic class PersonBuilder {\nprivate void fillMandatoryFields() {\nfirstname(&quot;lasdjfa&#xF6;skdlfja&quot;);\naddress(new AddressBuilder().createNew());\n};\nprivate void fillMandatoryFields_custom() {...};\npublic PersonBuilder firstname(String value);\npublic PersonBuilder lastname(String value);\n...\npublic Person createNew();\npublic Person persist(EntityManager em);\npublic List&lt;Person&gt; persistAndDuplicate(EntityManager em, int count);\n}\nLooking at the plotted builder API generated by CobiGen, you will find two private methods. The method fillMandatoryFields will be generated by CobiGen and regenerated every time CobiGen generation will be triggered for the Person class. This method will set every automatically detected field with not null constraints to a default value. However, by implementing fillMandatoryFields_custom on your own, you can reset these values or even specify more default values for any other field of the object. Thus, running new PersonBuilder().birthyear(1909).createNew(); will create a valid object of Person, which is already pre-filled such that it does not influence the test execution besides the fact that it circumvents database and validation issues.\nThis even holds for complex data structures as indicated by address(new AddressBuilder().createNew());. Due to the use of the AddressBuilder for setting the default value for the field address, also the default values for Address will be set automatically.\nFinally, the builder API provides different methods to create new objects.\ncreateNew() just creates a new object from the builder specification and returns it.\npersist(EntityManager) will create a new object from the builder specification and persists it to the database.\npersistAndDuplicate(EntityManager, int) will create the given amount of objects form the builder specification and persists all of these. After the initial generation of each builder, you might want to adapt the method body as you will most probably not be able to persist more than one object with the same field assignments to the database due to unique constraints. Thus, please see the generated comment in the method to adapt unqiue fields accordingly before persisting to the database.\nCustom Builder for Business Needs\nCobiGen just generates basic builder for any POJO. However, for project needs you probably would like to have even more complex builders, which enable the easy generation of more complex test data which are encoded in a large object hierarchy. Therefore, the generated builders can just be seen as a tool to achieve this. You can define your own business driven builders in the same way as the generated builders, but explicitly focusing on your business needs. Just take this example as a demonstration of that idea:\nUniversity uni = new ComplexUniversityBuilder()\n.withStudents(200)\n.withProfessors(4)\n.withExternalStudent()\n.createNew();\nE.g. the method withExternalStudent() might create a person, which is a student and is flagged to be an external student. Basing this implementation on the generated builders will even assure that you would benefit from any default values you have set before. In addition, you can even imagine any more complex builder methods setting values driven your reusable testing needs based on the specific business knowledge.\n43.3.2. Register Factory\nCRUD server application\nGenerates a CRUD application with persistence entities as inputs. This includes DAOs, TOs, use cases, as well as a CRUD JSF user interface if needed.\nTestdata Builder\nAnalogous to Testdata Builder for devon4J\nTest documentation\nGenerate test documentation from test classes. The input are the doclet tags of several test classes, which e.g. can specify a description, a cross-reference, or a test target description. The result currently is a csv file, which lists all tests with the corresponding meta-information. Afterwards, this file might be styled and passed to the customer if needed and it will be up-to-date every time!\n&#x2191;&#xA0;Up:&#xA0;CobiGen&#x2009;&#x2014;&#x2009;Code-based incremental Generator&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;CobiGen&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-cobigen.asciidoc_eclipse-integration.html","title":"46. Eclipse Integration","body":"\n46. Eclipse Integration\n46.1. Installation\nRemark: CobiGen is preinstalled in the devonfw/devon-ide.\n46.1.1. Preconditions\nEclipse 4.x\nJava 7 Runtime (for starting eclipse with CobiGen). This is independent from the target version of your developed code.\n46.1.2. Installation steps\nOpen the eclipse installation dialog\nmenu bar &#x2192; Help &#x2192; Install new Software&#x2026;&#x200B;\nOpen CobiGen&#x2019;s update site\nInsert the update site of your interest into the filed Work with and press Add &#x2026;&#x200B;\nStable releases: http://de-mucevolve02/files/cobigen/updatesite/stable/\nBeta/RC releases: http://de-mucevolve02/files/cobigen/updatesite/experimental/\nFollow the installation wizard\nSelect CobiGen Eclipse Plug-in &#x2192; Next &#x2192; Next &#x2192; accept the license &#x2192; Finish &#x2192; OK &#x2192; Yes\nOnce installed, a new menu entry named &quot;CobiGen&quot; will show up in the Package Explorer&#x2019;s context menu. In the sub menu there will the Generate&#x2026;&#x200B; command, which may ask you to update the templates, and then you can start the generation wizard of CobiGen. You can adapt the templates by clicking on Adapt Templates which will give you the possibility to import the CobiGen_Templates automatically so that you can modified them.\nCheckout (clone) your project&#x2019;s templates folder or use the current templates released with CobiGen (https://github.com/devonfw/tools-cobigen/tree/master/cobigen-templates) and then choose Import -&gt; General -&gt; Existing Projects into Workspace to import the templates into your workspace.\nNow you can start generating. To get an introduction of CobiGen try the devon4j templates and work on the devon4j sample application. There you might want to start with Entity objects as a selection to run CobiGen with, which will give you a good overview of what CobiGen can be used for right out of the box in devon4j based development. If you need some more introduction in how to come up with your templates and increments, please be referred to the documentation of the context configuration and the templates configuration\nDependent on your context configuration menu entry Generate&#x2026;&#x200B; may be greyed out or not. See for more information about valid selections for generation.\n46.1.3. Updating\nIn general updating CobiGen for eclipse is done via the update mechanism of eclipse directly, as shown on image below:\nUpgrading eclipse CobiGen plug-in to v3.0.0 needs some more attention of the user due to a changed plug-in architecture of CobiGen&#x2019;s core module and the eclipse integration. Eventually, we were able to provide any plug-in of CobiGen separately as its own eclipse bundle (fragment), which is automatically discovered by the main CobiGen Eclipse plug-in after installation.\n46.2. Usage\nCobiGen has two different generation modes depending on the input selected for generation. The first one is the simple mode, which will be started if the input contains only one input artifact, e.g. for Java an input artifact currently is a Java file. The second one is the batch mode, which will be started if the input contains multiple input artifacts, e.g. for Java this means a list of files. In general this means also that the batch mode might be started when selecting complex models as inputs, which contain multiple input artifacts. The latter scenario has only been covered in the research group,yet.\n46.2.1. Simple Mode\nSelecting the menu entry Generate&#x2026;&#x200B; the generation wizard will be opened:\nThe left side of the wizard shows all available increments, which can be selected to be generated. Increments are a container like concept encompassing multiple files to be generated, which should result in a semantically closed generation output.\nOn the right side of the wizard all files are shown, which might be effected by the generation - dependent on the increment selection of files on the left side. The type of modification of each file will be encoded into following color scheme if the files are selected for generation:\ngreen: files, which are currently non-existent in the file system. These files will be created during generation\nyellow: files, which are currently existent in the file system and which are configured to be merged with generated contents.\nred: files, which are currently existent in the file system. These files will be overwritten if manually selected.\nno color: files, which are currently existent in the file system. Additionally files, which were unselected and thus will be ignored during generation.\nSelecting an increment on the left side will initialize the selection of all shown files to be generated on the right side, whereas green and yellow categorized files will be selected initially. A manual modification of the pre-selection can be performed by switching to the customization tree using the Customize button on the right lower corner.\nOptional: If you want to customize the generation object model of a Java input class, you might continue with the Next &gt; button instead of finishing the generation wizard. The next generation wizard page is currently available for Java file inputs and lists all non-static fields of the input. Unselecting entries will lead to an adapted object model for generation, such that unselected fields will be removed in the object model for generation. By default all fields will be included in the object model.\nUsing the Finish button, the generation will be performed. Finally, CobiGen runs the eclipse internal organize imports and format source code for all generated sources and modified sources. Thus it is possible, that---especially organize imports opens a dialog if some types could not be determined automatically. This dialog can be easily closed by pressing on Continue. If the generation is finished, the Success! dialog will pop up.\n46.2.2. Batch mode\nIf there are multiple input elements selected, e.g., Java files, CobiGen will be started in batch mode. For the generation wizard dialog this means, that the generation preview will be constrained to the first selected input element. It does not preview the generation for each element of the selection or of a complex input. The selection of the files to be generated will be generated for each input element analogously afterwards.\nThus the color encoding differs also a little bit:\nyellow: files, which are configured to be merged.\nred: files, which are not configured with any merge strategy and thus will be created if the file does not exist or overwritten if the file already exists\nno color: files, which will be ignored during generation\nInitially all possible files to be generated will be selected.\n46.2.3. Health Check\nTo check whether CobiGen runs appropriately for the selected element(s) the user can perform a Health Check by activating the respective menu entry as shown below.\nThe simple Health Check includes 3 checks. As long as any of these steps fails, the Generate menu entry is grayed out.\nThe first step is to check whether the generation configuration is available at all. If this check fails you will see the following message:\nThis indicates, that there is no Project named CobiGen_Templates available in the current workspace. To run CobiGen appropriately, it is necessary to have a configuration project named CobiGen_Templates imported into your workspace. For more information see chapter Eclipse Installation.\nThe second step is to check whether the template project includes a valid context.xml. If this check fails, you will see the following message:\nThis means that either your context.xml\ndoes not exist (or has another name)\nor it is not valid one in any released version of CobiGen\nor there is simply no automatic routine of upgrading your context configuration to a valid state.\nIf all this is not the case, such as, there is a context.xml, which can be successfully read by CobiGen, you might get the following information:\nThis means that your context.xml is available with the correct name but it is outdated (belongs to an older CobiGen version). In this case just click on Upgrade Context Configuration to get the latest version.\nRemark: This will create a backup of your current context configuration and converts your old configuration to the new format. The upgrade will remove all comments from the file, which could be retrieved later on again from the backup.\nIf the creation of the backup fails, you will be asked to continue or to abort.\nThe third step checks whether there are templates for the selected element(s). If this check fails, you will see the following message:\nThis indicates, that there no trigger has been activated, which matches the current selection. The reason might be that your selection is faulty or that you imported the wrong template project (e.g. you are working on a devon4j project, but imported the Templates for the Register Factory). If you are a template developer, have a look at the trigger configuration and at the corresponding available plug-in implementations of triggers, like e.g., Java Plug-in or XML Plug-in.\nIf all the checks are passed you see the following message:\nIn this case everything is OK and the Generate button is not grayed out anymore so that you are able to trigger it and see the simple-mode.\nIn addition to the basic check of the context configuration, you also have the opportunity to perform an Advanced Health Check, which will check all available templates configurations (templates.xml) of path-depth=1 from the configuration project root according to their compatibility.\nAnalogous to the upgrade of the context configuration, the Advanced Health Check will also provide upgrade functionality for templates configurations if available.\n46.2.4. Update Templates\nUpdate Template: Select Entity file and right click then select cobigen Update Templates after that click on download then download successfully message will be come .\n46.2.5. Adapt Templates\nAdapt Template: Select any file and right click, then select cobigen &#x2192; Adapt Templates .If cobigen templates jar is not available then it downloads them automatically. If Cobigen templates is already present then it will override existing template in workspace and click on OK then imported template successfully message will be come.\nFinally, please change the Java version of the project to 1.8 so that you don&#x2019;t have any compilation errors.\n46.3. Logging\nIf you have any problem with the CobiGen eclipse plug-in, you might want to enable logging to provide more information for further problem analysis. This can be done easily by adding the logback.xml to the root of the CobiGen_templates configuration folder. The file should contain at least the following contents, whereas you should specify an absolute path to the target log file (at the TODO). If you are using the (cobigen-templates project, you might have the contents already specified but partially commented.\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n&lt;!-- This file is for logback classic. The file contains the configuration for sl4j logging --&gt;\n&lt;configuration&gt;\n&lt;appender name=&quot;FILE&quot; class=&quot;ch.qos.logback.core.FileAppender&quot;&gt;\n&lt;file&gt;&lt;!-- TODO choose your log file location --&gt;&lt;/file&gt;\n&lt;encoder class=&quot;ch.qos.logback.classic.encoder.PatternLayoutEncoder&quot;&gt;\n&lt;Pattern&gt;%n%date %d{HH:mm:ss.SSS} [%thread] %-5level %logger - %msg%n\n&lt;/Pattern&gt;\n&lt;/encoder&gt;\n&lt;/appender&gt;\n&lt;root level=&quot;DEBUG&quot;&gt;\n&lt;appender-ref ref=&quot;FILE&quot; /&gt;\n&lt;/root&gt;\n&lt;/configuration&gt;\n&#x2190;&#xA0;Previous:&#xA0;Maven Build Integration&#xA0;| &#x2191;&#xA0;Up:&#xA0;CobiGen&#x2009;&#x2014;&#x2009;Code-based incremental Generator&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Template Development&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-cobigen.asciidoc_maven-build-integration.html","title":"45. Maven Build Integration","body":"\n45. Maven Build Integration\n45.1. Maven Build Integration\nFor maven integration of CobiGen you can include the following build plugin into your build:\nListing 119. Build integration of CobiGen\n&lt;build&gt;\n&lt;plugins&gt;\n&lt;plugin&gt;\n&lt;groupId&gt;com.devonfw.cobigen&lt;/groupId&gt;\n&lt;artifactId&gt;maven-plugin&lt;/artifactId&gt;\n&lt;version&gt;VERSION-YOU-LIKE&lt;/version&gt;\n&lt;executions&gt;\n&lt;execution&gt;\n&lt;id&gt;cobigen-generate&lt;/id&gt;\n&lt;phase&gt;site&lt;/phase&gt;\n&lt;goals&gt;\n&lt;goal&gt;generate&lt;/goal&gt;\n&lt;/goals&gt;\n&lt;/execution&gt;\n&lt;/executions&gt;\n&lt;/plugin&gt;\n&lt;/plugins&gt;\n&lt;/build&gt;\nAvailable goals\ngenerate: Generates contents configured by the standard non-compiled configuration folder. Thus generation can be controlled/configured due to an location URI of the configuration and template or increment ids to be generated for a set of inputs.\nAvailable phases are all phases, which already provide compiled sources such that CobiGen can perform reflection on it. Thus possible phases are for example package, site.\n45.1.1. Provide Template Set\nFor generation using the CobiGen maven plug-in, the CobiGen configuration can be provided in two different styles:\nBy a configurationFolder, which should be available on the file system whenever you are running the generation. The value of configurationFolder should correspond to the maven file path syntax.\nListing 120. Provide CobiGen configuration by configuration folder (file)\n&lt;build&gt;\n&lt;plugins&gt;\n&lt;plugin&gt;\n...\n&lt;configuration&gt;\n&lt;configurationFolder&gt;cobigen-templates&lt;/configurationFolder&gt;\n&lt;/configuration&gt;\n...\n&lt;/plugin&gt;\n&lt;/plugins&gt;\n&lt;/build&gt;\nBy maven dependency, whereas the maven dependency should stick on the same conventions as the configuration folder. This explicitly means that it should contain non-compiled resources as well as the context.xml on top-level.\nListing 121. Provide CobiGen configuration by maven dependency (jar)\n&lt;build&gt;\n&lt;plugins&gt;\n&lt;plugin&gt;\n...\n&lt;dependencies&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.devonfw.cobigen&lt;/groupId&gt;\n&lt;artifactId&gt;templates-XYZ&lt;/artifactId&gt;\n&lt;version&gt;VERSION-YOU-LIKE&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;/dependencies&gt;\n...\n&lt;/plugin&gt;\n&lt;/plugins&gt;\n&lt;/build&gt;\nWe currently provide a generic deployed version of the templates on the devonfw-nexus for Register Factory (&lt;artifactId&gt;cobigen-templates-rf&lt;/artifactId&gt;) and for the devonfw itself (&lt;artifactId&gt;cobigen-templates-devonfw&lt;/artifactId&gt;).\n45.1.2. Build Configuration\nUsing the following configuration you will be able to customize your generation as follows:\n&lt;destinationRoot&gt; specifies the root directory the relative destinationPath of CobiGen templates configuration should depend on. Default ${basedir}\n&lt;inputPackage&gt; declares a package name to be used as input for batch generation. This refers directly to the CobiGen Java Plug-in container matchers of type package configuration.\n&lt;inputFile&gt; declares a file to be used as input. The CobiGen maven plug-in will try to parse this file to get an appropriate input to be interpreted by any CobiGen plug-in.\n&lt;increment&gt; specifies an increment ID to be generated. You can specify one single increment with content ALL to generate all increments matching the input(s).\n&lt;template&gt; specifies a template ID to be generated. You can specify one single template with content ALL to generate all templates matching the input(s).\n&lt;forceOverride&gt; specifies an overriding behavior, which enables non-mergeable resources to be completely rewritten by generated contents. For mergeable resources this flag indicates, that conflicting fragments during merge will be replaced by generated content. Default: false\n&lt;failOnNothingGenerated&gt; specifies whether the build should fail if the execution does not generate anything.\nListing 122. Example for a simple build configuration\n&lt;build&gt;\n&lt;plugins&gt;\n&lt;plugin&gt;\n...\n&lt;configuration&gt;\n&lt;destinationRoot&gt;${basedir}&lt;/destinationRoot&gt;\n&lt;inputPackages&gt;\n&lt;inputPackage&gt;package.to.be.used.as.input&lt;/inputPackage&gt;\n&lt;/inputPackages&gt;\n&lt;inputFiles&gt;\n&lt;inputFile&gt;path/to/file/to/be/used/as/input&lt;/inputFile&gt;\n&lt;/inputFiles&gt;\n&lt;increments&gt;\n&lt;increment&gt;IncrementID&lt;/increment&gt;\n&lt;/increments&gt;\n&lt;templates&gt;\n&lt;template&gt;TemplateID&lt;/template&gt;\n&lt;/templates&gt;\n&lt;forceOverride&gt;false&lt;/forceOverride&gt;\n&lt;/configuration&gt;\n...\n&lt;/plugin&gt;\n&lt;/plugins&gt;\n&lt;/build&gt;\n45.1.3. Plugin Injection Since v3\nSince version 3.0.0, the plug-in mechanism has changed to support modular releases of the CobiGen plug-ins. Therefore, you need to add all plug-ins to be used for generation. Take the following example to get the idea:\nListing 123. Example of a full configuration including plugins\n&lt;build&gt;\n&lt;plugins&gt;\n&lt;plugin&gt;\n&lt;groupId&gt;com.devonfw.cobigen&lt;/groupId&gt;\n&lt;artifactId&gt;maven-plugin&lt;/artifactId&gt;\n&lt;version&gt;VERSION-YOU-LIKE&lt;/version&gt;\n&lt;executions&gt;\n...\n&lt;/executions&gt;\n&lt;configuration&gt;\n...\n&lt;/configuration&gt;\n&lt;dependencies&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.devonfw.cobigen&lt;groupId&gt;\n&lt;artifactId&gt;templates-devon4j&lt;/artifactId&gt;\n&lt;version&gt;2.0.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.devonfw.cobigen&lt;/groupId&gt;\n&lt;artifactId&gt;tempeng-freemarker&lt;/artifactId&gt;\n&lt;version&gt;1.0.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.devonfw.cobigen&lt;/groupId&gt;\n&lt;artifactId&gt;javaplugin&lt;/artifactId&gt;\n&lt;version&gt;1.6.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;/dependencies&gt;\n&lt;/plugin&gt;\n&lt;/plugins&gt;\n&lt;/build&gt;\n45.1.4. A full example\nA complete maven configuration example\n&lt;build&gt;\n&lt;plugins&gt;\n&lt;plugin&gt;\n&lt;groupId&gt;com.devonfw.cobigen&lt;/groupId&gt;\n&lt;artifactId&gt;maven-plugin&lt;/artifactId&gt;\n&lt;version&gt;3.0.0&lt;/version&gt;\n&lt;executions&gt;\n&lt;execution&gt;\n&lt;id&gt;generate&lt;/id&gt;\n&lt;phase&gt;package&lt;/phase&gt;\n&lt;goals&gt;\n&lt;goal&gt;generate&lt;/goal&gt;\n&lt;/goals&gt;\n&lt;/execution&gt;\n&lt;/executions&gt;\n&lt;configuration&gt;\n&lt;inputFiles&gt;\n&lt;inputFile&gt;src/main/java/io/github/devonfw/cobigen/generator/dataaccess/api/InputEntity.java&lt;/inputFile&gt;\n&lt;/inputFiles&gt;\n&lt;increments&gt;\n&lt;increment&gt;dataaccess_infrastructure&lt;/increment&gt;\n&lt;increment&gt;daos&lt;/increment&gt;\n&lt;/increments&gt;\n&lt;failOnNothingGenerated&gt;false&lt;/failOnNothingGenerated&gt;\n&lt;/configuration&gt;\n&lt;dependencies&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.devonfw.cobigen&lt;/groupId&gt;\n&lt;artifactId&gt;templates-devon4j&lt;/artifactId&gt;\n&lt;version&gt;2.0.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.devonfw.cobigen&lt;/groupId&gt;\n&lt;artifactId&gt;tempeng-freemarker&lt;/artifactId&gt;\n&lt;version&gt;1.0.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.devonfw.cobigen&lt;/groupId&gt;\n&lt;artifactId&gt;javaplugin&lt;/artifactId&gt;\n&lt;version&gt;1.6.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;/dependencies&gt;\n&lt;/plugin&gt;\n&lt;/plugins&gt;\n&lt;/build&gt;\n&#x2190;&#xA0;Previous:&#xA0;CobiGen&#xA0;| &#x2191;&#xA0;Up:&#xA0;CobiGen&#x2009;&#x2014;&#x2009;Code-based incremental Generator&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Eclipse Integration&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-cobigen.asciidoc_template-development.html","title":"47. Template Development","body":"\n47. Template Development\n47.1. Helpful links for template development\nFreeMarker Root Page\nExpressions Cheat Sheet\nComplete Language reference\nFreeMarker Template Tester\nVariables to access Java source model\n&#x2190;&#xA0;Previous:&#xA0;Eclipse Integration&#xA0;| &#x2191;&#xA0;Up:&#xA0;CobiGen&#x2009;&#x2014;&#x2009;Code-based incremental Generator&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;devonfw testing&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devon4j.asciidoc.html","title":"III. devon4j","body":"\nIII. devon4j\nIntroduction\nCoding\nLayers\nGuides\ndevonfw Development\nTutorials\nFor Core-Developers\n&#x2190;&#xA0;Previous:&#xA0;Integrated Development Environment&#xA0;| &#x2191;&#xA0;Up:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Introduction&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devon4j.asciidoc_coding.html","title":"7. Coding","body":"\n7. Coding\n7.1. Coding Conventions\nThe code should follow general conventions for Java (see Oracle Naming Conventions, Google Java Style, etc.).We consider this as common sense and provide configurations for SonarQube and related tools such as Checkstyle instead of repeating this here.\n7.1.1. Naming\nBesides general Java naming conventions, we follow the additional rules listed here explicitly:\nAlways use short but speaking names (for types, methods, fields, parameters, variables, constants, etc.).\nFor package segments and type names prefer singular forms (CustomerEntity instead of CustomersEntity). Only use plural forms when there is no singular or it is really semantically required (e.g. for a container that contains multiple of such objects).\nAvoid having duplicate type names. The name of a class, interface, enum or annotation should be unique within your project unless this is intentionally desired in a special and reasonable situation.\nAvoid artificial naming constructs such as prefixes (I*) or suffixes (*IF) for interfaces.\nUse CamelCase even for abbreviations (XmlUtil instead of XMLUtil)\nNames of Generics should be easy to understand. Where suitable follow the common rule E=Element, T=Type, K=Key, V=Value but feel free to use longer names for more specific cases such as ID, DTO or ENTITY. The capitalized naming helps to distinguish a generic type from a regular class.\n7.1.2. Packages\nJava Packages are the most important element to structure your code. We use a strict packaging convention to map technical layers and business components (slices) to the code (See technical architecture for further details). By using the same names in documentation and code we create a strong link that gives orientation and makes it easy to find from business requirements, specifications or story tickets into the code and back.\nFor an devon4j based application we use the following Java-Package schema:\n&#xAB;rootpackage&#xBB;.&#xAB;application&#xBB;.&#xAB;component&#xBB;.&#xAB;layer&#xBB;.&#xAB;scope&#xBB;[.&#xAB;detail&#xBB;]*\nE.g. in our example application we find the Spring Data repositories for the ordermanagement component in the package com.devonfw.application.mtsj.ordermanagement.dataaccess.api.repo\nTable 3. Segments of package schema\nSegment\nDescription\nExample\n&#xAB;rootpackage&#xBB;\nIs the basic Java Package name-space of the organization or IT project owning the code following common Java Package conventions. Consists of multiple segments corresponding to the Internet domain of the organization.\ncom.devonfw.application.mtsj\n&#xAB;application&#xBB;\nThe name of the application build in this project.\ndevonfw\n&#xAB;component&#xBB;\nThe (business) component the code belongs to. It is defined by the business architecture and uses terms from the business domain. Use the implicit component general for code not belonging to a specific component (foundation code).\nsalesmanagement\n&#xAB;layer&#xBB;\nThe name of the technical layer (See technical architecture) which is one of the predefined layers (dataaccess, logic, service, batch, gui, client) or common for code not assigned to a technical layer (datatypes, cross-cutting concerns).\ndataaccess\n&#xAB;scope&#xBB;\nThe scope which is one of api (official API to be used by other layers or components),base (basic code to be reused by other implementations) and impl (implementation that should never be imported from outside)\napi\n&#xAB;detail&#xBB;\nHere you are free to further divide your code into sub-components and other concerns according to the size of your component part.\ndao\nPlease note that for library modules where we use com.devonfw.module as &#xAB;basepackage&#xBB; and the name of the module as &#xAB;component&#xBB;. E.g. the API of our beanmapping module can be found in the package com.devonfw.module.beanmapping.common.api.\n7.1.3. Architecture Mapping\nWe combine the above naming and packaging conventions to map the entire architecture to the code.\nThis also allows tools such as CobiGen or sonar-devon4j-plugin to &quot;understand&quot; the code.\nAlso this helps developers going from one devon4j project to the next to quickly understand the code-base.\nIf every developer knows where to find what, the project gets more efficient.\nA long time ago maven standardized the project structure with src/main/java, etc. and turned chaos into structure.\nWith devonfw we experienced the same for the codebase (what is inside src/main/java).\nListing 4. Architecture mapped to code\n&#xAB;rootpackage&#xBB;.&#xAB;application&#xBB;\n&#x251C;&#x2500;&#x2500;.&#xAB;component&#xBB;\n| &#x251C;&#x2500;&#x2500;.common\n| | &#x251C;&#x2500;&#x2500;.api[.&#xAB;detail&#xBB;]\n| | | &#x251C;&#x2500;&#x2500;.datatype\n| | | | &#x2514;&#x2500;&#x2500;.&#xAB;Datatype&#xBB;\n| | | &#x2514;&#x2500;&#x2500;.&#xAB;BusinessObject&#xBB;\n| | &#x2514;&#x2500;&#x2500;.impl[.&#xAB;detail&#xBB;]\n| | &#x251C;&#x2500;&#x2500;.&#xAB;Datatype&#xBB;JsonSerializer\n| | &#x2514;&#x2500;&#x2500;.&#xAB;Datatype&#xBB;JsonDeserializer\n| &#x251C;&#x2500;&#x2500;.dataaccess\n| | &#x251C;&#x2500;&#x2500;.api[.&#xAB;detail&#xBB;]\n| | | &#x251C;&#x2500;&#x2500;.repo\n| | | | &#x2514;&#x2500;&#x2500;.&#xAB;BusinessObject&#xBB;Repository\n| | | &#x251C;&#x2500;&#x2500;.dao (alternative to repo)\n| | | | &#x2514;&#x2500;&#x2500;.&#xAB;BusinessObject&#xBB;Dao (alternative to Repository)\n| | | &#x2514;&#x2500;&#x2500;.&#xAB;BusinessObject&#xBB;Entity\n| | &#x2514;&#x2500;&#x2500;.impl[.&#xAB;detail&#xBB;]\n| | &#x251C;&#x2500;&#x2500;.dao (alternative to repo)\n| | | &#x2514;&#x2500;&#x2500;.&#xAB;BusinessObject&#xBB;DaoImpl (alternative to Repository)\n| | &#x2514;&#x2500;&#x2500;.&#xAB;Datatype&#xBB;AttributeConverter\n| &#x251C;&#x2500;&#x2500;.logic\n| | &#x251C;&#x2500;&#x2500;.api\n| | | &#x251C;&#x2500;&#x2500;.[&#xAB;detail&#xBB;.]to\n| | | | &#x251C;&#x2500;&#x2500;.&#xAB;MyCustom&#xBB;&#xAB;To\n| | | | &#x251C;&#x2500;&#x2500;.&#xAB;DataStructure&#xBB;Embeddable\n| | | | &#x251C;&#x2500;&#x2500;.&#xAB;BusinessObject&#xBB;Eto\n| | | | &#x2514;&#x2500;&#x2500;.&#xAB;BusinessObject&#xBB;&#xAB;Subset&#xBB;Cto\n| | | &#x251C;&#x2500;&#x2500;.[&#xAB;detail&#xBB;.]usecase\n| | | | &#x251C;&#x2500;&#x2500;.UcFind&#xAB;BusinessObject&#xBB;\n| | | | &#x251C;&#x2500;&#x2500;.UcManage&#xAB;BusinessObject&#xBB;\n| | | | &#x2514;&#x2500;&#x2500;.Uc&#xAB;Operation&#xBB;&#xAB;BusinessObject&#xBB;\n| | | &#x2514;&#x2500;&#x2500;.&#xAB;Component&#xBB;\n| | &#x251C;&#x2500;&#x2500;.base\n| | | &#x2514;&#x2500;&#x2500;.[&#xAB;detail&#xBB;.]usecase\n| | | &#x2514;&#x2500;&#x2500;.Abstract&#xAB;BusinessObject&#xBB;Uc\n| | &#x2514;&#x2500;&#x2500;.impl\n| | &#x251C;&#x2500;&#x2500;.[&#xAB;detail&#xBB;.]usecase\n| | | &#x251C;&#x2500;&#x2500;.UcFind&#xAB;BusinessObject&#xBB;Impl\n| | | &#x251C;&#x2500;&#x2500;.UcManage&#xAB;BusinessObject&#xBB;Impl\n| | | &#x2514;&#x2500;&#x2500;.Uc&#xAB;Operation&#xBB;&#xAB;BusinessObject&#xBB;Impl\n| | &#x2514;&#x2500;&#x2500;.&#xAB;Component&#xBB;Impl\n| &#x2514;&#x2500;&#x2500;.service\n| &#x251C;&#x2500;&#x2500;.api[.&#xAB;detail&#xBB;]\n| | &#x251C;&#x2500;&#x2500;.rest\n| | | &#x2514;&#x2500;&#x2500;.&#xAB;Component&#xBB;RestService\n| | &#x2514;&#x2500;&#x2500;.ws\n| | &#x2514;&#x2500;&#x2500;.&#xAB;Component&#xBB;WebService\n| &#x2514;&#x2500;&#x2500;.impl[.&#xAB;detail&#xBB;]\n| &#x251C;&#x2500;&#x2500;.jms\n| | &#x2514;&#x2500;&#x2500;.&#xAB;BusinessObject&#xBB;JmsListener\n| &#x251C;&#x2500;&#x2500;.rest\n| | &#x2514;&#x2500;&#x2500;.&#xAB;Component&#xBB;RestServiceImpl\n| &#x2514;&#x2500;&#x2500;.ws\n| &#x2514;&#x2500;&#x2500;.&#xAB;Component&#xBB;WebServiceImpl\n&#x251C;&#x2500;&#x2500;.general\n&#x2502; &#x251C;&#x2500;&#x2500;.common\n&#x2502; | &#x251C;&#x2500;&#x2500;.api\n| | | &#x251C;&#x2500;&#x2500;.to\n| | | | &#x251C;&#x2500;&#x2500;.AbstractSearchCriteriaTo\n| | | &#x2514;&#x2500;&#x2500;.ApplicationEntity\n&#x2502; | &#x251C;&#x2500;&#x2500;.base\n| | | &#x2514;&#x2500;&#x2500;.AbstractBeanMapperSupport\n&#x2502; | &#x2514;&#x2500;&#x2500;.impl\n&#x2502; | &#x251C;&#x2500;&#x2500;.config\n&#x2502; | | &#x2514;&#x2500;&#x2500;.ApplicationObjectMapperFactory\n&#x2502; | &#x2514;&#x2500;&#x2500;.security\n&#x2502; | &#x2514;&#x2500;&#x2500;.ApplicationWebSecurityConfig\n&#x2502; &#x251C;&#x2500;&#x2500;.dataaccess\n&#x2502; | &#x2514;&#x2500;&#x2500;.api\n| | &#x2514;&#x2500;&#x2500;.ApplicationPersistenceEntity\n&#x2502; &#x251C;&#x2500;&#x2500;.logic\n&#x2502; | &#x2514;&#x2500;&#x2500;.base\n| | &#x251C;&#x2500;&#x2500;.AbstractComponentFacade\n| | &#x251C;&#x2500;&#x2500;.AbstractLogic\n| | &#x2514;&#x2500;&#x2500;.AbstractUc\n| &#x2514;&#x2500;&#x2500;.service\n| &#x2514;&#x2500;&#x2500;...\n&#x2514;&#x2500;&#x2500;.SpringBootApp\n7.1.4. Code Tasks\nCode spots that need some rework can be marked with the following tasks tags. These are already properly pre-configured in your development environment for auto completion and to view tasks you are responsible for. It is important to keep the number of code tasks low. Therefore every member of the team should be responsible for the overall code quality. So if you change a piece of code and hit a code task that you can resolve in a reliable way do this as part of your change and remove the according tag.\nTODO\nUsed to mark a piece of code that is not yet complete (typically because it can not be completed due to a dependency on something that is not ready).\n// TODO &#xAB;author&#xBB; &#xAB;description&#xBB;\nA TODO tag is added by the author of the code who is also responsible for completing this task.\nFIXME\n// FIXME &#xAB;author&#xBB; &#xAB;description&#xBB;\nA FIXME tag is added by the author of the code or someone who found a bug he can not fix right now. The &#xAB;author&#xBB; who added the FIXME is also responsible for completing this task. This is very similar to a TODO but with a higher priority. FIXME tags indicate problems that should be resolved before a release is completed while TODO tags might have to stay for a longer time.\nREVIEW\n// REVIEW &#xAB;responsible&#xBB; (&#xAB;reviewer&#xBB;) &#xAB;description&#xBB;\nA REVIEW tag is added by a reviewer during a code review. Here the original author of the code is responsible to resolve the REVIEW tag and the reviewer is assigning this task to him. This is important for feedback and learning and has to be aligned with a review &quot;process&quot; where people talk to each other and get into discussion. In smaller or local teams a peer-review is preferable but this does not scale for large or even distributed teams.\n7.1.5. Code-Documentation\nAs a general goal the code should be easy to read and understand. Besides clear naming the documentation is important. We follow these rules:\nAPIs (especially component interfaces) are properly documented with JavaDoc.\nJavaDoc shall provide actual value - we do not write JavaDoc to satisfy tools such as checkstyle but to express information not already available in the signature.\nWe make use of {@link} tags in JavaDoc to make it more expressive.\nJavaDoc of APIs describes how to use the type or method and not how the implementation internally works.\nTo document implementation details, we use code comments (e.g. // we have to flush explicitly to ensure version is up-to-date). This is only needed for complex logic.\n7.1.6. Code-Style\nThis section gives you best practices to write better code and avoid pitfalls and mistakes.\nBLOBs\nAvoid using byte[] for BLOBs as this will load them entirely into your memory. This will cause performance issues or out of memory errors. Instead use streams when dealing with BLOBs. For further details see BLOB support.\nClosing Resources\nResources such as streams (InputStream, OutputStream, Reader, Writer) or transactions need to be handled properly. Therefore it is important to follow these rules:\nEach resource has to be closed properly, otherwise you will get out of file handles, TX sessions, memory leaks or the like\nWhere possible avoid to deal with such resources manually. That is why we are recommending @Transactional for transactions in devonfw (see Transaction Handling).\nIn case you have to deal with resources manually (e.g. binary streams) ensure to close them properly. See the example below for details.\nClosing streams and other such resources is error prone. Have a look at the following example:\ntry {\nInputStream in = new FileInputStream(file);\nreadData(in);\nin.close();\n} catch (IOException e) {\nthrow new RuntimeIoException(e, IoMode.READ);\n}\nThe code above is wrong as in case of an IOException the InputStream is not properly closed. In a server application such mistakes can cause severe errors that typically will only occur in production. As such resources implement the AutoCloseable interface you can use the try-with-resource syntax to write correct code. The following code shows a correct version of the example:\ntry (InputStream in = new FileInputStream(file)) {\nreadData(in);\n} catch (IOException e) {\nthrow new RuntimeIoException(e, IoMode.READ);\n}\nLambdas and Streams\nWith Java8 you have cool new features like lambdas and monads like (Stream, CompletableFuture, Optional, etc.).\nHowever, these new features can also be misused or lead to code that is hard to read or debug. To avoid pain, we give you the following best practices:\nLearn how to use the new features properly before using. Often developers are keen on using cool new features. When you do your first experiments in your project code you will cause deep pain and might be ashamed afterwards. Please study the features properly. Even Java8 experts still write for loops to iterate over collections, so only use these features where it really makes sense.\nStreams shall only be used in fluent API calls as a Stream can not be forked or reused.\nEach stream has to have exactly one terminal operation.\nDo not write multiple statements into lambda code:\ncollection.stream().map(x -&gt; {\nFoo foo = doSomething(x);\n...\nreturn foo;\n}).collect(Collectors.toList());\nThis style makes the code hard to read and debug. Never do that! Instead extract the lambda body to a private method with a meaningful name:\ncollection.stream().map(this::convertToFoo).collect(Collectors.toList());\nDo not use parallelStream() in general code (that will run on server side) unless you know exactly what you are doing and what is going on under the hood. Some developers might think that using parallel streams is a good idea as it will make the code faster. However, if you want to do performance optimizations talk to your technical lead (architect). Many features such as security and transactions will rely on contextual information that is associated with the current thread. Hence, using parallel streams will most probably cause serious bugs. Only use them for standalone (CLI) applications or for code that is just processing large amounts of data.\nDo not perform operations on a sub-stream inside a lambda:\nset.stream().flatMap(x -&gt; x.getChildren().stream().filter(this::isSpecial)).collect(Collectors.toList()); // bad\nset.stream().flatMap(x -&gt; x.getChildren().stream()).filter(this::isSpecial).collect(Collectors.toList()); // fine\nOnly use collect at the end of the stream:\nset.stream().collect(Collectors.toList()).forEach(...) // bad\nset.stream().peek(...).collect(Collectors.toList()) // fine\nLambda parameters with Types inference\n(a,b,c) -&gt; a.toString() + Float.toString(b) + Arrays.toString(c) // fine\n(String a, Float b, Byte[] c) -&gt; a.toString() + Float.toString(b) + Arrays.toString(c) //bad\nCollections.sort(personList, (p1, p2) -&gt; p1.getSurName().compareTo(p2.getSurName())); //fine\nCollections.sort(personList, (Person p1, Person p2) -&gt; p1.getSurName().compareTo(p2.getSurName())); //bad\nAvoid Return Braces and Statement\n(a) -&gt; a.toString(); // fine\n(a) -&gt; { return a.toString(); } //bad\nAvoid Parentheses with Single Parameter\na -&gt; a.toString(); // fine\n(a) -&gt; a.toString(); //bad\nAvoid if/else inside foreach method. Use Filter method &amp; comprehension\nBad\nstatic public Iterator&lt;String&gt; TwitterHandles(Iterator&lt;Author&gt; authors, string company) {\nfinal List result = new ArrayList&lt;String&gt; ();\nforeach (Author a : authors) {\nif (a.Company.equals(company)) {\nString handle = a.TwitterHandle;\nif (handle != null)\nresult.Add(handle);\n}\n}\nreturn result;\n}\nFine\npublic List&lt;String&gt; twitterHandles(List&lt;Author&gt; authors, String company) {\nreturn authors.stream()\n.filter(a -&gt; null != a &amp;&amp; a.getCompany().equals(company))\n.map(a -&gt; a.getTwitterHandle())\n.collect(toList());\n}\nOptionals\nWith Optional you can wrap values to avoid a NullPointerException (NPE). However, it is not a good code-style to use Optional for every parameter or result to express that it may be null. For such case use @Nullable or even better instead annotate @NotNull where null is not acceptable.\nHowever, Optional can be used to prevent NPEs in fluent calls (due to the lack of the elvis operator):\nLong id;\nid = fooCto.getBar().getBar().getId(); // may cause NPE\nid = Optional.ofNullable(fooCto).map(FooCto::getBar).map(BarCto::getBar).map(BarEto::getId).orElse(null); // null-safe\nEncoding\nEncoding (esp. Unicode with combining characters and surrogates) is a complex topic. Please study this topic if you have to deal with encodings and processing of special characters. For the basics follow these recommendations:\nWhen you have explicitly decide for an encoding always prefer Unicode (UTF-8 or better). This especially impacts your databases and has to be defined upfront as it typically can not be changed (easily) afterwards.\nDo not cast from byte to char (Unicode characters can be composed of multiple bytes, such cast may only work for ASCII characters)\nNever convert the case of a String using the default locale (esp. when writing generic code like in devonfw). E.g. if you do &quot;HI&quot;.toLowerCase() and your system locale is Turkish, then the output will be &quot;h&#x131;&quot; instead of &quot;hi&quot; what can lead to wrong assumptions and serious problems. If you want to do a &quot;universal&quot; case conversion always use explicitly an according western locale (e.g. toLowerCase(Locale.US)). Consider using a library (https://github.com/m-m-m/util/blob/master/core/src/main/java/net/sf/mmm/util/lang/api/BasicHelper.java) or create your own little static utility for that in your project.\nWrite your code independent from the default encoding (system property file.encoding) - this will most likely differ in JUnit from production environment\nAlways provide an encoding when you create a String from byte[]: new String(bytes, encoding)\nAlways provide an encoding when you create a Reader or Writer : new InputStreamReader(inStream, encoding)\nPrefer general API\nAvoid unnecessary strong bindings:\nDo not bind your code to implementations such as Vector or ArrayList instead of List\nIn APIs for input (=parameters) always consider to make little assumptions:\nprefer Collection over List or Set where the difference does not matter (e.g. only use Set when you require uniqueness or highly efficient contains)\nconsider preferring Collection&lt;? extends Foo&gt; over Collection&lt;Foo&gt; when Foo is an interface or super-class\n7.2. Tools\nTable 4. Development Tools used for devon4j\nTopic\nDetail\nSuggested Tool\nbuild-management\n*\nmaven\nIDE\nIDE\nEclipse\nIDE\nsetup &amp; update\ndevonfw-ide\nIDE\ncode generation\nCobiGen\nTesting\nUnit-Testing\nJUnit\nTesting\nMocking\nMockito &amp; WireMock\nTesting\nIntegration-Testing\nspring-test (arquillian for JEE)\nTesting\nEnd-to-end\nMrChecker\nQuality\nCode-Analysis\nSonarQube\n&#x2190;&#xA0;Previous:&#xA0;Introduction&#xA0;| &#x2191;&#xA0;Up:&#xA0;devon4j&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Layers&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devon4j.asciidoc_devonfw-development.html","title":"10. devonfw Development","body":"\n10. devonfw Development\n10.1. IDE Setup\nThis Tutorial explains how to setup the development environment to work on and contribute to devonfw4j with your Windows computer.\nWe are using a pre-configured devon-ide for development. To get started follow these steps:\nGet a Git client. For Windows use:\nhttps://gitforwindows.org/\nImportant: install with option Use Git from the Windows Command Prompts but without Windows Explorer integration.\nDownload TortoiseGit from https://tortoisegit.org/\nDownload the IDE\nIf you are a member of Capgemini: download devonfw ide package or the higher integrated devonfw distribution (for devonfw please find the setup guide within the devon-dist).\nIf you are not member of Capgemini: We cannot distribute the package. Please consult devon-ide to setup and configure the IDE manually. If you need help, please get in touch.\nChoose a project location for your project (e.g. C:\\projects\\devonfw, referred to with $projectLoc in this setup guides following steps). Avoid long paths and white spaces to prevent trouble. Extract the downloaded ZIP files via Extract Here (e.g. using 7-Zip). Do not use the Windows native ZIP tool to extract as this is not working properly on long paths and filenames.\nRun the script update-all-workspaces.bat in $projectLoc.\nHint: You can use update-all-workspaces.bat whenever you created a new folder in workspaces to separate different workspaces. This update will create new Eclipse start batches allowing to run a number of Eclipse instances using different workspaces in parallel.\nYou should end up having a structure like this in $projectLoc\nOpen console.bat and check out the git repositories you need to work on into workspaces\\main. with the following commands:\ncd workspaces/main\ngit clone --recursive https://github.com/devonfw/my-thai-star.git\nDo another check whether there are files in folder workspaces\\main\\my-thai-star\\!\nRun the script eclipse-main.bat to start the Eclipse IDE.\nIn Eclipse select File &gt; Import &gt; Maven &gt; Existing Maven Projects and then choose the cloned projects from your workspace by clicking the Browse button and select the folder structure (workspaces\\main\\my-thai-star\\java\\MTSJ).\nExecute the application by starting the &#xB4;SpringBootApp&#xB4;. Select the class and click the right mouse button. In the context menu select the entry Run as &#x21D2; Java Application (or Debug as &#x2026;&#x200B;). The application starts up and creates log entries in the Eclipse Console Tab.\nOnce started, the backend part of the application runs on http://localhost:8081/mythaistar. This is protected by Spring Security, so an additional frontend needs to be started, that will be able to get the needed access tokens.\nNow switch within command line to workspaces\\main\\my-thai-star\\angular and run yarn install followed by yarn start. Finally\nlogin with waiter/waiter at http://localhost:4200/restaurant/.\n10.2. Issue creation and resolution\n10.2.1. Issue creation\nYou can create an issue here. Please consider the following points:\nIf your issue is related to a specific building block (like e.g. devon4ng), open an issue on that specific issue tracker. If you&#x2019;re unsure which building block is causing your problem open an issue on this repository.\nPut a label on the issue to mark whether you suggest an enhancement, report an error or something else.\nWhen reporting errors:\nInclude the version of devon4j you are using.\nInclude screenshots, stack traces.\nInclude the behavior you expected.\nusing a debugger you might be able to find the cause of the problem and you could be the one to contribute a bug-fix.\n10.2.2. Preparation for issue resolution\nBefore you get started working on an issue, check out the following points:\ntry to complete all other issues you are working on before. Only postpone issues where you are stuck and consider giving them back in the queue (backlog).\ncheck that no-one else is already assigned or working on the issue\nread through the issue and check that you understand the task completely. Collect any remaining questions and clarify them with the one responsible for the topic.\nensure that you are aware on which branch the issue shall be fixed and start your work in the corresponding workspace.\nif you are using git perform your changes on a feature branch.\n10.2.3. Definition of Done\nactual issue is implemented (bug fixed, new feature implemented, etc.)\nnew situation is covered by tests (according to test strategy of the project e.g. for bugs create a unit test first proving the bug and running red, then fix the bug and check that the test gets green, for new essential features create new tests, for GUI features do manual testing)\ncheck the code-style with sonar-qube in eclipse. If there are anomalies in the new or modified code, please rework.\ncheck out the latest code from the branch you are working on (svn update, git pull after git commit)\ntest that all builds and tests are working (mvn clean install)\ncommit your code (svn commit, git push) - for all your commits ensure you stick to the conventions for code contributions (see code contribution) and provide proper comments (see coding conventions).\nif no milestone was assigned please assign suitable milestone\nset the issue as done\n10.3. Code contribution\nWe are looking forward to your contribution to devon4j. This page describes the few conventions to follow. Please note that this is an open and international project and all content has to be in (American) English language.\nFor contributions to the code please consider:\nWe are working issue-based so check if there is already an issue in our tracker for the task you want to work on or create a new issue for it.\nIn case of more complex issues please get involved with the community and ensure that there is a common understanding of what and how to do it. You do not want to invest into something that will later be rejected by the community.\nBefore you get started ensure that you comment the issue accordingly and you are the person assigned to the issue. If there is already someone else assigned get in contact with him if you still want to contribute to the same issue. You do not want to invest into something that is already done by someone else.\nCreate a fork of the repository on github to your private github space.\nClone this fork.\nBefore doing any change choose the branch you want to add your feature to. In most cases this will be the develop branch to add new features. However, if you want to fix a bug, check if an according maintenance branch develop-x.y already exists and switch to that one before.\nThen the first step is to create a local feature branch (named by the feature you are planning so `feature/&#xAB;issue-id&#xBB;-&#xAB;keyword&#xBB;) and checkout this branch.\nStart your modifications.\nEnsure to stick to our coding-conventions.\nCheck in features or fixes as individual commits associated with an issue using the commit message format:\n#&lt;issueId&gt;: &lt;describe your change&gt;\nThen github will automatically link the commit in the issue. In case you worked on an issue from a different repository (e.g. change in devon4j-sample due to issue in devon4j) we use this commit message format:\ndevonfw/&lt;repository&gt;#&lt;issueId&gt;: &lt;describe your change&gt;\nSo as an example:\ndevonfw/devon4j#1: added REST service for tablemanagement\nIf you completed your feature (bug-fix, improvement, etc.) use a pull request to give it back to the community.\nYour pull request will automatically be checked if it builds correctly (no compile or test errors), can be merged without conflicts, and CLA has been signed. Please ensure to do the required tasks and reworks unless all checks are satisfied.\nFrom here a reviewer should take over and give feedback. In the best case, your contribution gets merged and everything is completed.\nIn case you should not get feedback for weeks, do not hesitate to ask the community.\nIf one (typically the reviewer) has to change the base branch (because the wrong develop branch was used, see above) onto which the changes will be merged, one can do the same by following the instructions at here.\nsee also the documentation guidelines.\n10.4. devonfw Documentation\nWe are using the github AsciiDoc feature to create and maintain the documentation for devonfw. Also the documentation PDFs are generated from these AsciiDoc files.\nThe source of the documentation is always located in the documentation folder of the main git repository (see Code tab and then click on documentation). These files are automatically synchronized to the wiki. This is for pure usability reasons as people typically go to the Wiki tab on github repositories to look for documentation. However, the wiki is a read-only copy of the documentation folder from the Code repo.\n10.4.1. Contribution to devon4j documentation\nContributions and improvements to the documentation are welcome. However, you should be aware of the following aspects:\nYour contributions will become part of the devon4j documentation and is licensed under creative commons (see footer).\nIf you want to contribute larger changes (beyond fixing a typo or a link) please consider to get in contact with the community (by creating an issue) before getting started. You do not want to write complete chapters and then get your work rejected afterwards.\nPlease consult the DocGen manual as we are using DocGen\nto generate the documentation starting from devon4j-doc.\nIf you consider all the aspects above you can start editing the documentation if you have a github-account. For small and simple changes just go to the AsciiDoc file in the documentation folder. Then click on the pencil-icon (you have to be signed in). Now github will allow you to edit the raw AsciiDoc text. Do your changes and preview them (using the Preview tab). Once complete, commit the changes as a new branch. From there click on compare and pull-request and finally confirm your pull-request.\nFor larger changes, you should create a fork just as for code-contributions. Often larger changes imply changes to documentation and code.\n&#x2190;&#xA0;Previous:&#xA0;Guides&#xA0;| &#x2191;&#xA0;Up:&#xA0;devon4j&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Tutorials&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devon4j.asciidoc_for-core-developers.html","title":"12. For Core-Developers","body":"\n12. For Core-Developers\n12.1. Creating a Release\nThis page documents how to create and publish a release of devon4j.\nFor each release there is a milestone that contains an issue for creating the release itself (the github issue of that issue is referred as &#xAB;issue&#xBB;). The release version is referred as &#xAB;x.y.z&#xBB;.\n12.1.1. Releasing the code\nTo release the code follow these steps.\nCreate a clean clone of the repository:\ngit clone https://github.com/devonfw/devon4j.git\nIn case you want to build a (bug fix) release from a different branch, switch to that branch:\ngit checkout -b develop-&#xAB;x.y&#xBB; origin/develop-&#xAB;x.y&#xBB;\nEnsure your branch is up-to-date:\ngit pull\nEnsure that the result is what you want to release (mvn clean install).\nBump the release version by removing the -SNAPSHOT from devon4j.version property in top-level pom.xml.\nCreate an annotated tag for your release:\ngit tag -a release/x.y.z -m &quot;#&#xAB;issue&#xBB;: tagged x.y.z&quot;\ne.g For release 2.5.0 the command would look like\ngit tag -a release/2.5.0 -m &quot;#618: tagged 2.5.0&quot;\nwhere #618 is the issue number created for release itself under release milestone.\nYou can confirm if the tag is created by listing out the tags with the following command\ngit tag\nConfigure OSSRH\nFor publishing artifacts to OSSRH, we need an OSSRH account with necessary rights for publishing and managing staging repositories. And configure this account in devonfw distribution to create connection and deploy to OSSRH.\nIf you do not already have an account on OSSRH, create an account on the link below\nhttps://issues.sonatype.org/secure/Signup!default.jspa\nYou need manager access to deploy artifacts to OSSRH. For same contact devonfw administrators for OSSRH.\nOpen file conf/.m2/setting.xml in your devon distribution (devon-ide) and add a new server with following details\n&lt;server&gt;\n&lt;id&gt;ossrh&lt;/id&gt;\n&lt;username&gt;&#xAB;ossrh_username&#xBB;&lt;/username&gt;\n&lt;password&gt;&#xAB;ossrh_password&#xBB;&lt;/password&gt;\n&lt;/server&gt;\nHere &#xAB;ossrh_username&#xBB; and &#xAB;ossrh_password&#xBB; are the account details used to login into OSSRH and should have rights to publish artifacts to OSSRH for groupId name com.devonfw (and its children).\nPlease use password encryption and prevent\nstoring passwords in plain text.\nThe id ossrh points to the OSSRH repository for snaphost and release declared in the &lt;distributionManagement&gt; section of the devon4j/pom.xml.\nOptionally you may want to explicitly define PGP key via the associated email-address:\n&lt;profile&gt;\n&lt;id&gt;devon.ossrh&lt;/id&gt;\n&lt;activation&gt;\n&lt;activeByDefault&gt;true&lt;/activeByDefault&gt;\n&lt;/activation&gt;\n&lt;properties&gt;\n&lt;gpg.keyname&gt;your.email@address.com&lt;/gpg.keyname&gt;\n&lt;/properties&gt;\n&lt;/profile&gt;\nConfigure PGP\nArtifacts should be PGP signed before they can be deployed to OSSRH. Artifacts can be signed either by using command line tool GnuPG or GUI based tool Gpg4win Kleopetra (preferred). Follow the steps below to sign artifacts using either of the two tools.\nDownload tools\nGnuPg - https://www.gnupg.org/download/\ngpg4win - https://www.gpg4win.org/download.html\nInstallation\nInstallation is self explanatory for GnuPG and gpg4win. To verify installation of GnuPg, open windows command line and run &quot;gpg --version or gpg2 --version&quot;\nGenerate PGP key pair for signing artifacts.\nNote\nRemember the passphrase set for PGP keys as it will be used later for authentication during signing of artifacts by maven.\nUsing GnuPg follow either of the link below\nhttp://central.sonatype.org/pages/working-with-pgp-signatures.html#generating-a-key-pair\nhttps://www.youtube.com/watch?v=DE3FVty3NgE&amp;feature=youtu.be\nUsing Kleopetra follow link below\nhttps://www.deepdotweb.com/2015/02/21/pgp-tutorial-for-windows-kleopatra-gpg4win/\nExporting PGP key to public key-server\nUsing GnuPg - http://central.sonatype.org/pages/working-with-pgp-signatures.html#distributing-your-public-key\nUsing Kleopetra, click on the certificate entry you want to publish to OpenPGP certificate servers and select File &gt; Publish on Server as shown below. These instructions are as per Kleopatra 3.0.1-gpg4win-3.0.2, for latest versions there might be some variation.\nDeploy to OSSRH\nGo to the root of devon4j project and run following command. Make sure there are no spaces between comma separated profiles.\nmvn clean deploy -P deploy\nA pop will appear asking for passphrase for PGP key. Enter the passphrase and press &quot;OK&quot;.\nNote\nIf you face the error below, contact one of the people who have access to the repository for access rights.\nOpen OSSRH, login and open staging repositories.\nFind your deployment repository as comdevonfw-NNNN and check its Content.\nThen click on Close to close the repository and wait a minute.\nRefresh the repository and copy the URL.\nCreate a vote for the release and paste the URL of the staging repository.\nAfter the vote has passed with success go back to OSSRH and and click on Release to publish the release and stage to maven central.\nEdit the top-level pom.xml and change devon4j.version property to the next planned release version including the -SNAPSHOT suffix.\nCommit and push the changes:\ngit commit -m &quot;#&#xAB;issue&#xBB;: open next snapshot version&quot;\ngit push\nIn case you build the release from a branch other that develop ensure to follow the next steps. Otherwise you are done here and can continue to the next section. To merge the changes (bug fixes) onto develop do:\ngit checkout develop\ngit merge develop-&#xAB;x.y&#xBB;\nYou most probably will have a conflict in the top-level pom.xml. Then resolve this conflict. In any case edit this pom.xml and ensure that it is still pointing to the latest planned SNAPSHOT for the develop branch.\nIf there are local changes to the top-level pom.xml, commit them:\ngit commit -m &quot;#&#xAB;issue&#xBB;: open next snapshot version&quot;\nPush the changes of your develop branch:\ngit push\n12.1.2. Releasing the maven-site\nCreate a new folder for your version in your checkout of devonfw.github.io/devon4j (as &#xAB;x.y.z&#xBB;).\nCopy the just generated devon4j-doc.pdf into the new release version folder.\nCopy the index.html from the previous release to the new release version folder.\nEdit the new copy of index.html and replace all occurrences of the version to the new release as well as the release date.\nGenerate the maven site from the devon4j release checkout (see code release):\nmvn site\nmvn site:deploy\nReview that the maven site is intact and copy it to the new release version folder (from devon4j/target/devon4j/maven to devonfw.github.io/devon4j/&#xAB;x.y.z&#xBB;/maven).\nUpdate the link in the devon4j/index.html to the latest stable documentation.\nAdd, commit and push the new release version folder.\ngit add &#xAB;x.y.z&#xBB;\ngit commit -m &quot;devonfw/devon4j#&#xAB;issue&#xBB;: released documentation&quot;\ngit push\n12.1.3. Finalize the Release\nClose the issue of the release.\nClose the milestone of the release (if necessary correct the release date).\nEnsure that the new release is available in maven central.\nWrite an announcement for the new release.\n&#x2190;&#xA0;Previous:&#xA0;Tutorials&#xA0;| &#x2191;&#xA0;Up:&#xA0;devon4j&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;devon4ng&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devon4j.asciidoc_guides.html","title":"9. Guides","body":"\n9. Guides\n9.1. Dependency Injection\nDependency injection is one of the most important design patterns and is a key principle to a modular and component based architecture. The Java Standard for dependency injection is javax.inject (JSR330) that we use in combination with JSR250.\nThere are many frameworks which support this standard including all recent Java EE application servers. We recommend to use Spring (also known as springframework) that we use in our example application. However, the modules we provide typically just rely on JSR330 and can be used with any compliant container.\n9.1.1. Key Principles\nA Bean in CDI (Contexts and Dependency-Injection) or Spring is typically part of a larger component and encapsulates some piece of logic that should in general be replaceable. As an example we can think of a Use-Case, Data-Access-Object (DAO), etc. As best practice we use the following principles:\nSeparation of API and implementation\nWe create a self-contained API documented with JavaDoc. Then we create an implementation of this API that we annotate with @Named. This implementation is treated as secret. Code from other components that wants to use the implementation shall only rely on the API. Therefore we use dependency injection via the interface with the @Inject annotation.\nStateless implementation\nBy default implementations (CDI-Beans) shall always be stateless. If you store state information in member variables you can easily run into concurrency problems and nasty bugs. This is easy to avoid by using local variables and separate state classes for complex state-information. Try to avoid stateful CDI-Beans wherever possible. Only add state if you are fully aware of what you are doing and properly document this as a warning in your JavaDoc.\nUsage of JSR330\nWe use javax.inject (JSR330) and JSR250 as a common standard that makes our code portable (works in any modern Java EE environment). However, we recommend to use the springframework as container. But we never use proprietary annotations such as @Autowired instead of standardized annotations like @Inject. Generally we avoid proprietary annotations in business code (common and logic layer).\nSimple Injection-Style\nIn general you can choose between constructor, setter or field injection. For simplicity we recommend to do private field injection as it is very compact and easy to maintain. We believe that constructor injection is bad for maintenance especially in case of inheritance (if you change the dependencies you need to refactor all sub-classes). Private field injection and public setter injection are very similar but setter injection is much more verbose (often you are even forced to have javadoc for all public methods). If you are writing re-usable library code setter injection will make sense as it is more flexible. In a business application you typically do not need that and can save a lot of boiler-plate code if you use private field injection instead. Nowadays you are using container infrastructure also for your tests (see spring integration tests) so there is no need to inject manually (what would require a public setter).\nKISS\nTo follow the KISS (keep it small and simple) principle we avoid advanced features (e.g. AOP, non-singleton beans) and only use them where necessary.\n9.1.2. Example Bean\nHere you can see the implementation of an example bean using JSR330 and JSR250:\n@Named\npublic class MyBeanImpl implements MyBean {\n@Inject\nprivate MyOtherBean myOtherBean;\n@PostConstruct\npublic void init() {\n// initialization if required (otherwise omit this method)\n}\n@PreDestroy\npublic void dispose() {\n// shutdown bean, free resources if required (otherwise omit this method)\n}\n}\nIt depends on MyOtherBean that should be the interface of an other component that is injected into the field because of the @Inject annotation. To make this work there must be exactly one implementation of MyOtherBean in the container (in our case spring). In order to put a Bean into the container we use the @Named annotation so in our example we put MyBeanImpl into the container. Therefore it can be injected into all setters that take the interface MyBean as argument and are annotated with @Inject.\nIn some situations you may have an Interface that defines a kind of &quot;plugin&quot; where you can have multiple implementations in your container and want to have all of them. Then you can request a list with all instances of that interface as in the following example:\n@Inject\nprivate List&lt;MyConverter&gt; converters;\nPlease note that when writing library code instead of annotating implementation with @Named it is better to provide @Configuration classes that choose the implementation via @Bean methods (see @Bean documentation). This way you can better &quot;export&quot; specific features instead of relying library users to do a component-scan to your library code and loose control on upgrades.\n9.1.3. Bean configuration\nWiring and Bean configuration can be found in configuration guide.\n9.2. Configuration\nAn application needs to be configurable in order to allow internal setup (like CDI) but also to allow externalized configuration of a deployed package (e.g. integration into runtime environment). Using Spring Boot (must read: Spring Boot reference) we rely on a comprehensive configuration approach following a &quot;convention over configuration&quot; pattern. This guide adds on to this by detailed instructions and best-practices how to deal with configurations.\nIn general we distinguish the following kinds of configuration that are explained in the following sections:\nInternal Application configuration maintained by developers\nExternalized Environment configuration maintained by operators\nExternalized Business configuration maintained by business administrators\n9.2.1. Internal Application Configuration\nThe application configuration contains all internal settings and wirings of the application (bean wiring, database mappings, etc.) and is maintained by the application developers at development time. There usually is a main configuration registered with main Spring Boot App, but differing configurations to support automated test of the application can be defined using profiles (not detailed in this guide).\nSpring Boot Application\nThe devonfw recommends using spring-boot to build web applications.\nFor a complete documentation see the Spring Boot Reference Guide.\nWith spring-boot you provide a simple main class (also called starter class) like this:\ncom.devonfw.mtsj.application\n@SpringBootApplication(exclude = { EndpointAutoConfiguration.class })\n@EntityScan(basePackages = { &quot;com.devonfw.mtsj.application&quot; }, basePackageClasses = { AdvancedRevisionEntity.class })\n@EnableGlobalMethodSecurity(jsr250Enabled = true)\n@ComponentScan(basePackages = { &quot;com.devonfw.mtsj.application.general&quot;, &quot;com.devonfw.mtsj.application&quot; })\npublic class SpringBootApp {\n/**\n* Entry point for spring-boot based app\n*\n* @param args - arguments\n*/\npublic static void main(String[] args) {\nSpringApplication.run(SpringBootApp.class, args);\n}\n}\nIn an devonfw application this main class is always located in the &lt;basepackage&gt; of the application package namespace (see package-conventions). This is because a spring boot application will automatically do a classpath scan for components (spring-beans) and entities in the package where the application main class is located including all sub-packages. You can use the @ComponentScan and @EntityScan annotations to customize this behaviour.\nStandard beans configuration\nFor basic bean configuration we rely on spring boot using mainly configuration classes and only occasionally XML configuration files. Some key principle to understand Spring Boot auto-configuration features:\nSpring Boot auto-configuration attempts to automatically configure your Spring application based on the jar dependencies and annotated components found in your source code.\nAuto-configuration is non-invasive, at any point you can start to define your own configuration to replace specific parts of the auto-configuration by redefining your identically named bean (see also exclude attribute of @SpringBootApplication in example code above).\nBeans are configured via annotations in your java code (see dependency-injection).\nFor technical configuration you will typically write additional spring config classes annotated with @Configuration that provide bean implementations via methods annotated with @Bean. See spring @Bean documentation for further details. Like in XML you can also use @Import to make a @Configuration class include other configurations.\nXML-based beans configuration\nIt is still possible and allowed to provide (bean-) configurations using XML, though not recommended. These configuration files are no more bundled via a main xml config file but loaded individually from their respective owners, e.g. for unit-tests:\n@SpringApplicationConfiguration(classes = { SpringBootApp.class }, locations = { &quot;classpath:/config/app/batch/beans-productimport.xml&quot; })\npublic class ProductImportJobTest extends AbstractSpringBatchIntegrationTest {\n...\nConfiguration XML-files reside in an adequately named subfolder of:\nsrc/main/resources/app\nBatch configuration\nIn the directory src/main/resources/config/app/batch we place the configuration for the batch jobs. Each file within this directory represents one batch job. See batch-layer for further details.\nBeanMapper Configuration\nIn the directory src/main/resources/config/app/common we place the configuration for the bean-mapping.\nSee bean-mapper configuration for further details.\nSecurity configuration\nThe abstract base class BaseWebSecurityConfig should be extended to configure web application security thoroughly.\nA basic and secure configuration is provided which can be overridden or extended by subclasses.\nSubclasses must use the @Profile annotation to further discriminate between beans used in production and testing scenarios. See the following example:\nListing 5. How to extend BaseWebSecurityConfig for Production and Test\n@Configuration\n@EnableWebSecurity\n@Profile(SpringProfileConstants.JUNIT)\npublic class TestWebSecurityConfig extends BaseWebSecurityConfig {...}\n@Configuration\n@EnableWebSecurity\n@Profile(SpringProfileConstants.NOT_JUNIT)\npublic class WebSecurityConfig extends BaseWebSecurityConfig {...}\nSee WebSecurityConfig.\nWebSocket configuration\nA websocket endpoint is configured within the business package as a Spring configuration class. The annotation @EnableWebSocketMessageBroker makes Spring Boot registering this endpoint.\npackage your.path.to.the.websocket.config;\n...\n@Configuration\n@EnableWebSocketMessageBroker\npublic class WebSocketConfig extends AbstractWebSocketMessageBrokerConfigurer {\n...\nDatabase Configuration\nTo choose database of your choice , set spring.profiles.active=XXX in src/main/resources/config/application.properties. Also, one has to set all the active spring profiles in this application.properties and not in any of the other application.properties.\n9.2.2. Externalized Configuration\nExternalized configuration is a configuration that is provided separately to a deployment package and can be maintained undisturbed by re-deployments.\nEnvironment Configuration\nThe environment configuration contains configuration parameters (typically port numbers, host names, passwords, logins, timeouts, certificates, etc.) specific for the different environments. These are under the control of the operators responsible for the application.\nThe environment configuration is maintained in application.properties files, defining various properties (see common application properties for a list of properties defined by the spring framework).\nThese properties are explained in the corresponding configuration sections of the guides for each topic:\npersistence configuration\nservice configuration\nlogging guide\nFor a general understanding how spring-boot is loading and boostrapping your application.properties see spring-boot external configuration.\nThe following properties files are used in every devonfw application:\nsrc/main/resources/application.properties providing a default configuration - bundled and deployed with the application package. It further acts as a template to derive a tailored minimal environment-specific configuration.\nsrc/main/resources/config/application.properties providing additional properties only used at development time (for all local deployment scenarios). This property file is excluded from all packaging.\nsrc/test/resources/config/application.properties providing additional properties only used for testing (JUnits based on spring test).\nFor other environments where the software gets deployed such as test, acceptance and production you need to provide a tailored copy of application.properties. The location depends on the deployment strategy:\nstandalone run-able Spring Boot App using embedded tomcat: config/application.properties under the installation directory of the spring boot application.\ndedicated tomcat (one tomcat per app): $CATALINA_BASE/lib/config/application.properties\ntomcat serving a number of apps (requires expanding the wars): $CATALINA_BASE/webapps/&lt;app&gt;/WEB-INF/classes/config\nIn this application.properties you only define the minimum properties that are environment specific and inherit everything else from the bundled src/main/resources/application.properties. In any case, make very sure that the classloader will find the file.\nMake sure your properties are thoroughly documented by providing a comment to each property. This inline documentation is most valuable for your operating department.\nBusiness Configuration\nThe business configuration contains all business configuration values of the application, which can be edited by administrators through the GUI. The business configuration values are stored in the database in key/value pairs.\nThe database table business_configuration has the following columns:\nID\nProperty name\nProperty type (Boolean, Integer, String)\nProperty value\nDescription\nAccording to the entries in this table, the administrative GUI shows a generic form to change business configuration. The hierarchy of the properties determines the place in the GUI, so the GUI bundles properties from the same hierarchy level and name. Boolean values are shown as checkboxes, integer and string values as text fields. The properties are read and saved in a typed form, an error is raised if you try to save a string in an integer property for example.\nWe recommend the following base layout for the hierarchical business configuration:\ncomponent.[subcomponent].[subcomponent].propertyname\n9.2.3. Security\nOften you need to have passwords (for databases, third-party services, etc.) as part of your configuration. These are typically environment specific (see above). However, with DevOps and continuous-deployment you might be tempted to commit such configurations into your version-control (e.g. git). Doing that with plain text passwords is a severe problem especially for production systems. Never do that! Instead we offer some suggestions how to deal with sensible configurations:\nPassword Encryption\nA simple but reasonable approach is to configure the passwords encrypted with a master-password. The master-password should be a strong secret that is specific for each environment. It must never be committed to version-control.\nIn order to support encrypted passwords in spring-boot application.properties all you need to do is to add jasypt-spring-boot as dependency in your pom.xml(please check for recent version):\n&lt;dependency&gt;\n&lt;groupId&gt;com.github.ulisesbocchio&lt;/groupId&gt;\n&lt;artifactId&gt;jasypt-spring-boot-starter&lt;/artifactId&gt;\n&lt;version&gt;1.17&lt;/version&gt;\n&lt;/dependency&gt;\nThis will smoothly integrate jasypt into your spring-boot application. Read this HOWTO to learn how to encrypt and decrypt passwords using jasypt. Here is a simple example output of an encrypted password (of course you have to use strong passwords instead of secret and postgres - this is only an example):\n----ARGUMENTS-------------------\ninput: postgres\npassword: secret\n----OUTPUT----------------------\njd5ZREpBqxuN9ok0IhnXabgw7V3EoG2p\nThe master-password can be configured on your target environment via the property jasypt.encryptor.password. As system properties given on the command-line are visible in the process list, we recommend to use an config/application.yml file only for this purpose (as we recommended to use application.properties for regular configs):\njasypt:\nencryptor:\npassword: secret\n(of course you will replace secret with a strong password). In case you happen to have multiple apps on the same machine, you can symlink the application.yml from a central place.\nNow you are able to put encrypted passwords into your application.properties\nspring.datasource.password=ENC(jd5ZREpBqxuN9ok0IhnXabgw7V3EoG2p)\nTo prevent jasypt to throw an exception in dev or test scenarios simply put this in your local config (src/main/config/application.properties and same for test, see above for details):\njasypt.encryptor.password=none\nIs this Security by Obscurity?\nYes, from the point of view to protect the passwords on the target environment this is nothing but security by obscurity. If an attacker somehow got full access to the machine this will only cause him to spend some more time.\nNo, if someone only gets the configuration file. So all your developers might have access to the version-control where the config is stored. Others might have access to the software releases that include this configs. But without the master-password that should only be known to specific operators none else can decrypt the password (except with brute-force what will take a very long time, see jasypt for details).\n9.3. Java Persistence API\nFor mapping java objects to a relational database we use the Java Persistence API (JPA).\nAs JPA implementation we recommend to use hibernate. For general documentation about JPA and hibernate follow the links above as we will not replicate the documentation. Here you will only find guidelines and examples how we recommend to use it properly. The following examples show how to map the data of a database to an entity. As we use JPA we abstract from SQL here. However, you will still need a DDL script for your schema and during maintenance also database migrations. Please follow our SQL guide for such artifacts.\n9.3.1. Entity\nEntities are part of the persistence layer and contain the actual data. They are POJOs (Plain Old Java Objects) on which the relational data of a database is mapped and vice versa. The mapping is configured via JPA annotations (javax.persistence). Usually an entity class corresponds to a table of a database and a property to a column of that table. A persistent entity instance then represents a row of the database table.\nA Simple Entity\nThe following listing shows a simple example:\n@Entity\n@Table(name=&quot;TEXTMESSAGE&quot;)\npublic class MessageEntity extends ApplicationPersistenceEntity implements Message {\nprivate String text;\npublic String getText() {\nreturn this.text;\n}\npublic void setText(String text) {\nthis.text = text;\n}\n}\nThe @Entity annotation defines that instances of this class will be entities which can be stored in the database. The @Table annotation is optional and can be used to define the name of the corresponding table in the database. If it is not specified, the simple name of the entity class is used instead.\nIn order to specify how to map the attributes to columns we annotate the corresponding getter methods (technically also private field annotation is also possible but approaches can not be mixed).\nThe @Id annotation specifies that a property should be used as primary key.\nWith the help of the @Column annotation it is possible to define the name of the column that an attribute is mapped to as well as other aspects such as nullable or unique. If no column name is specified, the name of the property is used as default.\nNote that every entity class needs a constructor with public or protected visibility that does not have any arguments. Moreover, neither the class nor its getters and setters may be final.\nEntities should be simple POJOs and not contain business logic.\nEntities and Datatypes\nStandard datatypes like Integer, BigDecimal, String, etc. are mapped automatically by JPA. Custom datatypes are mapped as serialized BLOB by default what is typically undesired.\nIn order to map atomic custom datatypes (implementations of`+SimpleDatatype`) we implement an AttributeConverter. ere is a simple example:\n@Converter(autoApply = true)\npublic class MoneyAttributeConverter implements AttributeConverter&lt;Money, BigDecimal&gt; {\npublic BigDecimal convertToDatabaseColumn(Money attribute) {\nreturn attribute.getValue();\n}\npublic Money convertToEntityAttribute(BigDecimal dbData) {\nreturn new Money(dbData);\n}\n}\nThe annotation @Converter is detected by the JPA vendor if the annotated class is in the packages to scan. Further, autoApply = true implies that the converter is automatically used for all properties of the handled datatype. Therefore all entities with properties of that datatype will automatically be mapped properly (in our example Money is mapped as BigDecimal).\nIn case you have a composite datatype that you need to map to multiple columns the JPA does not offer a real solution. As a workaround you can use a bean instead of a real datatype and declare it as @Embeddable. If you are using hibernate you can implement CompositeUserType. Via the @TypeDef annotation it can be registered to hibernate. If you want to annotate the CompositeUserType implementation itself you also need another annotation (e.g. MappedSuperclass tough not technically correct) so it is found by the scan.\nEnumerations\nBy default JPA maps Enums via their ordinal. Therefore the database will only contain the ordinals (0, 1, 2, etc.) . So , inside the database you can not easily understand their meaning. Using @Enumerated with EnumType.STRING allows to map the enum values to their name (Enum.name()). Both approaches are fragile when it comes to code changes and refactoring (if you change the order of the enum values or rename them) after the application is deployed to production. If you want to avoid this and get a robust mapping you can define a dedicated string in each enum value for database representation that you keep untouched. Then you treat the enum just like any other custom datatype.\nBLOB\nIf binary or character large objects (BLOB/CLOB) should be used to store the value of an attribute, e.g. to store an icon, the @Lob annotation should be used as shown in the following listing:\n@Lob\npublic byte[] getIcon() {\nreturn this.icon;\n}\nWarning\nUsing a byte array will cause problems if BLOBs get large because the entire BLOB is loaded into the RAM of the server and has to be processed by the garbage collector. For larger BLOBs the type Blob and streaming should be used.\npublic Blob getAttachment() {\nreturn this.attachment;\n}\nDate and Time\nTo store date and time related values, the temporal annotation can be used as shown in the listing below:\n@Temporal(TemporalType.TIMESTAMP)\npublic java.util.Date getStart() {\nreturn start;\n}\nUntil Java8 the java data type java.util.Date (or Jodatime) has to be used.\nTemporalType defines the granularity. In this case, a precision of nanoseconds is used. If this granularity is not wanted, TemporalType.DATE can be used instead, which only has a granularity of milliseconds.\nMixing these two granularities can cause problems when comparing one value to another. This is why we only use TemporalType.TIMESTAMP.\nQueryDSL and Custom Types\nUsing the Aliases API of QueryDSL might result in an InvalidDataAccessApiUsageException when using custom datatypes in entity properties. This can be circumvented in two steps:\nEnsure you have the following maven dependencies in your project (core module) to support custom types via the Aliases API:\n&lt;dependency&gt;\n&lt;groupId&gt;org.ow2.asm&lt;/groupId&gt;\n&lt;artifactId&gt;asm&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;cglib&lt;/groupId&gt;\n&lt;artifactId&gt;cglib&lt;/artifactId&gt;\n&lt;/dependency&gt;\nMake sure, that all your custom types used in entities provide a non-argument constructor with at least visibility level protected.\nIdRef\nIdRef can be used to reference the other entities in TOs\nIdRef is just a wrapper for reference ID as Long. so instead of\nprivate Long orderId;\nyou can write\nprivate IdRef&lt;Order&gt; orderId;\nWhen using IdRef make sure that the Junit tests contains the following\nJunit test should be derived from DbTest (e.g. ComponentDbTest and SubsystemDbTest).\nPrimary Keys\nWe only use simple Long values as primary keys (IDs). By default it is auto generated (@GeneratedValue(strategy=GenerationType.AUTO)). This is already provided by the class com.devonfw.&lt;projectName&gt;.general.dataaccess.api.AbstractPersistenceEntity that you can extend.\nIn case you have business oriented keys (often as String), you can define an additional property for it and declare it as unique (@Column(unique=true)).\nBe sure to include &quot;AUTO_INCREMENT&quot; in your sql table field ID to be able to persist data (or similar for other databases).\n9.3.2. Relationships\nn:1 and 1:1 Relationships\nEntities often do not exist independently but are in some relation to each other. For example, for every period of time one of the StaffMember&#x2019;s of the restaurant example has worked, which is represented by the class WorkingTime, there is a relationship to this StaffMember.\nThe following listing shows how this can be modeled using JPA:\n...\n@Entity\npublic class WorkingTimeEntity {\n...\nprivate StaffMemberEntity staffMember;\n@ManyToOne\n@JoinColumn(name=&quot;STAFFMEMBER&quot;)\npublic StaffMemberEntity getStaffMember() {\nreturn this.staffMember;\n}\npublic void setStaffMember(StaffMemberEntity staffMember) {\nthis.staffMember = staffMember;\n}\n}\nTo represent the relationship, an attribute of the type of the corresponding entity class that is referenced has been introduced. The relationship is a n:1 relationship, because every WorkingTime belongs to exactly one StaffMember, but a StaffMember usually worked more often than once.\nThis is why the @ManyToOne annotation is used here. For 1:1 relationships the @OneToOne annotation can be used which works basically the same way. To be able to save information about the relation in the database, an additional column in the corresponding table of WorkingTime is needed which contains the primary key of the referenced StaffMember. With the name element of the @JoinColumn annotation it is possible to specify the name of this column.\n1:n and n:m Relationships\nThe relationship of the example listed above is currently an unidirectional one, as there is a getter method for retrieving the StaffMember from the WorkingTime object, but not vice versa.\nTo make it a bidirectional one, the following code has to be added to StaffMember:\nprivate Set&lt;WorkingTimeEntity&gt; workingTimes;\n@OneToMany(mappedBy=&quot;staffMember&quot;)\npublic Set&lt;WorkingTimeEntity&gt; getWorkingTimes() {\nreturn this.workingTimes;\n}\npublic void setWorkingTimes(Set&lt;WorkingTimeEntity&gt; workingTimes) {\nthis.workingTimes = workingTimes;\n}\nTo make the relationship bidirectional, the tables in the database do not have to be changed. Instead the column that corresponds to the attribute staffMember in class WorkingTime is used, which is specified by the mappedBy element of the @OneToMany annotation. Hibernate will search for corresponding WorkingTime objects automatically when a StaffMember is loaded.\nThe problem with bidirectional relationships is that if a WorkingTime object is added to the set or list workingTimes in StaffMember, this does not have any effect in the database unless\nthe staffMember attribute of that WorkingTime object is set. That is why the devon4j advices not to use bidirectional relationships but to use queries instead. How to do this is shown here. If a bidirectional relationship should be used nevertheless, appropriate add and remove methods must be used.\nFor 1:n and n:m relations, the devon4j demands that (unordered) Sets and no other collection types are used, as shown in the listing above. The only exception is whenever an ordering is really needed, (sorted) lists can be used.\nFor example, if WorkingTime objects should be sorted by their start time, this could be done like this:\nprivate List&lt;WorkingTimeEntity&gt; workingTimes;\n@OneToMany(mappedBy = &quot;staffMember&quot;)\n@OrderBy(&quot;startTime asc&quot;)\npublic List&lt;WorkingTimeEntity&gt; getWorkingTimes() {\nreturn this.workingTimes;\n}\npublic void setWorkingTimes(List&lt;WorkingTimeEntity&gt; workingTimes) {\nthis.workingTimes = workingTimes;\n}\nThe value of the @OrderBy annotation consists of an attribute name of the class followed by asc (ascending) or desc (descending).\nTo store information about a n:m relationship, a separate table has to be used, as one column cannot store several values (at least if the database schema is in first normal form).\nFor example if one wanted to extend the example application so that all ingredients of one FoodDrink can be saved and to model the ingredients themselves as entities (e.g. to store additional information about them), this could be modeled as follows (extract of class FoodDrink):\nprivate Set&lt;IngredientEntity&gt; ingredients;\n@ManyToMany()\n@JoinTable\npublic Set&lt;IngredientEntity&gt; getIngredients() {\nreturn this.ingredients;\n}\npublic void setOrders(Set&lt;IngredientEntity&gt; ingredients) {\nthis.ingredients = ingredients;\n}\nInformation about the relation is stored in a table called BILL_ORDER that has to have two columns, one for referencing the Bill, the other one for referencing the Order. Note that the @JoinTable annotation is not needed in this case because a separate table is the default solution here (same for n:m relations) unless there is a mappedBy element specified.\nFor 1:n relationships this solution has the disadvantage that more joins (in the database system) are needed to get a Bill with all the Orders it refers to. This might have a negative impact on performance so that the solution to store a reference to the Bill row/entity in the Order&#x2019;s table is probably the better solution in most cases.\nNote that bidirectional n:m relationships are not allowed for applications based on the devon4j. Instead a third entity has to be introduced, which &quot;represents&quot; the relationship (it has two n:1 relationships).\nEager vs. Lazy Loading\nUsing JPA it is possible to use either lazy or eager loading. Eager loading means that for entities retrieved from the database, other entities that are referenced by these entities are also retrieved, whereas lazy loading means that this is only done when they are actually needed, i.e. when the corresponding getter method is invoked.\nApplication based on the devon are strongly advised to always use lazy loading. The JPA defaults are:\n@OneToMany: LAZY\n@ManyToMany: LAZY\n@ManyToOne: EAGER\n@OneToOne: EAGER\nSo at least for @ManyToOne and @OneToOne you always need to override the default by providing fetch = FetchType.LAZY.\nIMPORTANT: Please read the performance guide.\nCascading Relationships\nFor relations it is also possible to define whether operations are cascaded (like a recursion) to the related entity.\nBy default, nothing is done in these situations. This can be changed by using the cascade property of the annotation that specifies the relation type (@OneToOne, @ManyToOne, @OneToMany, @ManyToOne). This property accepts a CascadeType that offers the following options:\nPERSIST (for EntityManager.persist, relevant to inserted transient entities into DB)\nREMOVE (for EntityManager.remove to delete entity from DB)\nMERGE (for EntityManager.merge)\nREFRESH (for EntityManager.refresh)\nDETACH (for EntityManager.detach)\nALL (cascade all of the above operations)\nSee here for more information.\n9.3.3. Embeddable\nAn embeddable Object is a way to group properties of an entity into a separate Java (child) object. Unlike with implement relationships the embeddable is not a separate entity and its properties are stored (embedded) in the same table together with the entity. This is helpful to structure and reuse groups of properties.\nThe following example shows an Address implemented as an embeddable class:\n@Embeddable\npublic class AddressEmbeddable {\nprivate String street;\nprivate String number;\nprivate Integer zipCode;\nprivate String city;\n@Column(name=&quot;STREETNUMBER&quot;)\npublic String getNumber() {\nreturn number;\n}\npublic void setNumber(String number) {\nthis.number = number;\n}\n... // other getter and setter methods, equals, hashCode\n}\nAs you can see an embeddable is similar to an entity class, but with an @Embeddable annotation instead of the @Entity annotation and without primary key or modification counter.\nAn Embeddable does not exist on its own but in the context of an entity.\nAs a simplification Embeddables do not require a separate interface and ETO as the bean-mapper will create a copy automatically when converting the owning entity to an ETO.\nHowever, in this case the embeddable becomes part of your api module that therefore needs a dependency on the JPA.\nIn addition to that the methods equals(Object) and hashCode() need to be implemented as this is required by Hibernate (it is not required for entities because they can be unambiguously identified by their primary key). For some hints on how to implement the hashCode() method please have a look here.\nUsing this AddressEmbeddable inside an entity class can be done like this:\nprivate AddressEmbeddable address;\n@Embedded\npublic AddressEmbeddable getAddress() {\nreturn this.address;\n}\npublic void setAddress(AddressEmbeddable address) {\nthis.address = address;\n}\n}\nThe @Embedded annotation needs to be used for embedded attributes. Note that if in all columns of the embeddable (here Address) are null, then the embeddable object itself is also null inside the entity. This has to be considered to avoid NullPointerException&#x2019;s. Further this causes some issues with primitive types in embeddable classes that can be avoided by only using object types instead.\n9.3.4. Inheritance\nJust like normal java classes, entity classes can inherit from others. The only difference is that you need to specify how to map a class hierarchy to database tables. Generic abstract super-classes for entities can simply be annotated with @MappedSuperclass.\nFor all other cases the JPA offers the annotation @Inheritance with the property strategy talking an InheritanceType that has the following options:\nSINGLE_TABLE: This strategy uses a single table that contains all columns needed to store all entity-types of the entire inheritance hierarchy. If a column is not needed for an entity because of its type, there is a null value in this column. An additional column is introduced, which denotes the type of the entity (called dtype).\nTABLE_PER_CLASS: For each concrete entity class there is a table in the database that can store such an entity with all its attributes. An entity is only saved in the table corresponding to its most concrete type. To get all entities of a super type, joins are needed.\nJOINED: In this case there is a table for every entity class including abstract classes, which contains only the columns for the persistent properties of that particular class. Additionally there is a primary key column in every table. To get an entity of a class that is a subclass of another one, joins are needed.\nEach of the three approaches has its advantages and drawbacks, which are discussed in detail here. In most cases, the first one should be used, because it is usually the fastest way to do the mapping, as no joins are needed when retrieving, searching or persisting entities. Moreover it is rather simple and easy to understand.\nOne major disadvantage is that the first approach could lead to a table with a lot of null values, which might have a negative impact on the database size.\nThe inheritance strategy has to be annotated to the top-most entity of the class hierarchy (where `@MappedSuperclass`es are not considered) like in the following example:\n@Entity\n@Inheritance(strategy=InheritanceType.SINGLE_TABLE)\npublic abstract class MyParentEntity extends ApplicationPersistenceEntity implements MyParent {\n...\n}\n@Entity\npublic class MyChildEntity extends MyParentEntity implements MyChild {\n...\n}\n@Entity\npublic class MyOtherEntity extends MyParentEntity implements MyChild {\n...\n}\nAs a best practice we advise you to avoid entity hierarchies at all where possible and otherwise to keep the hierarchy as small as possible. In order to just ensure reuse or establish a common API you can consider a shared interface, a @MappedSuperclass or an @Embeddable instead of an entity hierarchy.\n9.3.5. Repositories and DAOs\nFor each entity a code unit is created that groups all database operations for that entity. We recommend to use spring-data repositories for that as it is most efficient for developers. As an alternative there is still the classic approach using DAOs.\nConcurrency Control\nThe concurrency control defines the way concurrent access to the same data of a database is handled. When several users (or threads of application servers) concurrently access a database, anomalies may happen, e.g. a transaction is able to see changes from another transaction although that one did, not yet commit these changes. Most of these anomalies are automatically prevented by the database system, depending on the isolation level (property hibernate.connection.isolation in the jpa.xml, see here).\nAnother anomaly is when two stakeholders concurrently access a record, do some changes and write them back to the database. The JPA addresses this with different locking strategies (see here).\nAs a best practice we are using optimistic locking for regular end-user services (OLTP) and pessimistic locking for batches.\nOptimistic Locking\nThe class com.devonfw.module.jpa.persistence.api.AbstractPersistenceEntity already provides optimistic locking via a modificationCounter with the @Version annotation. Therefore JPA takes care of optimistic locking for you. When entities are transferred to clients, modified and sent back for update you need to ensure the modificationCounter is part of the game. If you follow our guides about transfer-objects and services this will also work out of the box.\nYou only have to care about two things:\nHow to deal with optimistic locking in relationships?\nAssume an entity A contains a collection of B entities. Should there be a locking conflict if one user modifies an instance of A while another user in parallel modifies an instance of B that is contained in the other instance? To address this , take a look at GenericDao.forceIncrementModificationCounter.\nWhat should happen in the UI if an OptimisticLockException occurred?\nAccording to KISS our recommendation is that the user gets an error displayed that tells him to do his change again on the recent data. Try to design your system and the work processing in a way to keep such conflicts rare and you are fine.\nPessimistic Locking\nFor back-end services and especially for batches optimistic locking is not suitable. A human user shall not cause a large batch process to fail because he was editing the same entity. Therefore such use-cases use pessimistic locking what gives them a kind of priority over the human users.\nIn your DAO implementation you can provide methods that do pessimistic locking via EntityManager operations that take a LockModeType. Here is a simple example:\ngetEntityManager().lock(entity, LockModeType.READ);\nWhen using the lock(Object, LockModeType) method with LockModeType.READ, Hibernate will issue a SELECT &#x2026;&#x200B; FOR UPDATE. This means that no one else can update the entity (see here for more information on the statement). If LockModeType.WRITE is specified, Hibernate issues a SELECT &#x2026;&#x200B; FOR UPDATE NOWAIT instead, which has has the same meaning as the statement above, but if there is already a lock, the program will not wait for this lock to be released. Instead, an exception is raised.\nUse one of the types if you want to modify the entity later on, for read only access no lock is required.\nAs you might have noticed, the behavior of Hibernate deviates from what one would expect by looking at the LockModeType (especially LockModeType.READ should not cause a SELECT &#x2026;&#x200B; FOR UPDATE to be issued). The framework actually deviates from what is specified in the JPA for unknown reasons.\n9.3.6. Database Auditing\nSee auditing guide.\n9.3.7. Testing Entities and DAOs\nSee testing guide.\n9.3.8. Principles\nWe strongly recommend these principles:\nUse the JPA where ever possible and use vendor (hibernate) specific features only for situations when JPA does not provide a solution. In the latter case consider first if you really need the feature.\nCreate your entities as simple POJOs and use JPA to annotate the getters in order to define the mapping.\nKeep your entities simple and avoid putting advanced logic into entity methods.\n9.3.9. Database Configuration\nThe configuration for spring and hibernate is already provided by devonfw in our sample application and the application template. So you only need to worry about a few things to customize.\nDatabase System and Access\nObviously you need to configure which type of database you want to use as well as the location and credentials to access it. The defaults are configured in application-default.properties that is bundled and deployed with the release of the software. It should therefore contain the properties as in the given example:\ndatabase.url=jdbc:postgresql://database.enterprise.com/app\ndatabase.user.login=appuser01\ndatabase.hibernate.dialect = org.hibernate.dialect.PostgreSQLDialect\ndatabase.hibernate.hbm2ddl.auto=validate\nThe environment specific settings (especially passwords) are configured by the operators in application.properties. For further details consult the configuration guide. It can also override the default values. The relevant configuration properties can be seen by the following example for the development environment (located in src/test/resources):\ndatabase.url=jdbc:postgresql://localhost/app\ndatabase.user.password=************\ndatabase.hibernate.hbm2ddl.auto=create\nFor further details about database.hibernate.hbm2ddl.auto please see here. For production and acceptance environments we use the value validate that should be set as default. In case you want to use Oracle RDBMS you can find additional hints here.\nDatabase Migration\nSee database migration.\nDatabase Logging\nAdd the following properties to application.properties to enable logging of database queries for debugging purposes.\nspring.jpa.properties.hibernate.show_sql=true\nspring.jpa.properties.hibernate.use_sql_comments=true\nspring.jpa.properties.hibernate.format_sql=true\nPooling\nYou typically want to pool JDBC connections to boost performance by recycling previous connections. There are many libraries available to do connection pooling. We recommend to use HikariCP. For Oracle RDBMS see here.\n9.3.10. Security\nSQL-Injection\nA common security threat is SQL-injection. Never build queries with string concatenation or your code might be vulnerable as in the following example:\nString query = &quot;Select op from OrderPosition op where op.comment = &quot; + userInput;\nreturn getEntityManager().createQuery(query).getResultList();\nVia the parameter userInput an attacker can inject SQL (JPQL) and execute arbitrary statements in the database causing extreme damage. In order to prevent such injections you have to strictly follow our rules for queries: Use named queries for static queries and QueryDSL for dynamic queries. Please also consult the SQL Injection Prevention Cheat Sheet.\nLimited Permissions for Application\nWe suggest that you operate your application with a database user that has limited permissions so he can not modify the SQL schema (e.g. drop tables). For initializing the schema (DDL) or to do schema migrations use a separate user that is not used by the application itself.\n9.3.11. Queries\nThe Java Persistence API (JPA) defines its own query language, the java persistence query language (JPQL) (see also JPQL tutorial), which is similar to SQL but operates on entities and their attributes instead of tables and columns.\nThe simplest CRUD-Queries (e.g. find an entity by its ID) are already build in the devonfw CRUD functionality (via Repository or DAO). For other cases you need to write your own query. We distinguish between static and dynamic queries. Static queries have a fixed JPQL query string that may only use parameters to customize the query at runtime. Instead, dynamic queries can change their clauses (WHERE, ORDER BY, JOIN, etc.) at runtime depending on the given search criteria.\nStatic Queries\nE.g. to find all DishEntries (from MTS sample app) that have a price not exceeding a given maxPrice we write the following JPQL query:\nSELECT dish FROM DishEntity dish WHERE dish.price &lt;= :maxPrice\nHere dish is used as alias (variable name) for our selected DishEntity (what refers to the simple name of the Java entity class). With dish.price we are referring to the Java property price (getPrice()/setPrice(&#x2026;&#x200B;)) in DishEntity. A named variable provided from outside (the search criteria at runtime) is specified with a colon (:) as prefix. Here with :maxPrice we reference to a variable that needs to be set via query.setParameter(&quot;maxPrice&quot;, maxPriceValue). JPQL also supports indexed parameters (?) but they are discouraged because they easily cause confusion and mistakes.\nUsing Queries to Avoid Bidirectional Relationships\nWith the usage of queries it is possible to avoid exposing relationships or modelling bidirectional relationships, which have some disadvantages (see relationships). This is especially desired for relationships between entities of different business components.\nSo for example to get all OrderLineEntities for a specific OrderEntity without using the orderLines relation from OrderEntity the following query could be used:\nSELECT line FROM OrderLineEntity line WHERE line.order.id = :orderId\nDynamic Queries\nFor dynamic queries we use QueryDSL. It allows to implement queries in a powerful but readable and type-safe way (unlike Criteria API). If you already know JPQL you will quickly be able to read and write QueryDSL code. It feels like JPQL but implemented in Java instead of plain text.\nPlease be aware that code-generation can be painful especially with large teams. We therefore recommend to use QueryDSL without code-generation. Here is an example from our sample application:\npublic List&lt;DishEntity&gt; findOrders(DishSearchCriteriaTo criteria) {\nDishEntity dish = Alias.alias(DishEntity.class);\nJPAQuery&lt;OrderEntity&gt; query = newDslQuery(alias); // new JPAQuery&lt;&gt;(getEntityManager()).from(Alias.$(dish));\nRange&lt;BigDecimal&gt; priceRange = criteria.getPriceRange();\nif (priceRange != null) {\nBigDecimal min = priceRange.getMin();\nif (min != null) {\nquery.where(Alias.$(order.getPrice()).ge(min));\n}\nBigDecimal max = priceRange.getMax();\nif (max != null) {\nquery.where(Alias.$(order.getPrice()).le(max));\n}\n}\nString name = criteria.getName();\nif ((name != null) &amp;&amp; (!name.isEmpty())) {\n// query.where(Alias.$(alias.getName()).eq(name));\nQueryUtil.get().whereString(query, Alias.$(alias.getName()), name, criteria.getNameOption());\n}\nreturn query.fetch();\n}\nUsing Wildcards\nFor flexible queries it is often required to allow wildcards (especially in dynamic queries). While users intuitively expect glob syntax the SQL and JPQL standards work different. Therefore a mapping is required. devonfw provides this on a lower level by LikePatternSyntax and on a high level by QueryUtil (see QueryHelper.newStringClause(&#x2026;&#x200B;)).\nPagination\ndevonfw provides pagination support. If you are using spring-data repositories you will get that directly from spring for static queries. Otherwise for dynamic or generally handwritten queries we provide this via QueryUtil.findPaginated(&#x2026;&#x200B;):\nboolean determineTotalHitCount = ...;\nreturn QueryUtil.get().findPaginated(criteria.getPageable(), query, determineTotalHitCount);\nPagination example\nFor the table entity we can make a search request by accessing the REST endpoint with pagination support like in the following examples:\nPOST mythaistar/services/rest/tablemanagement/v1/table/search\n{\n&quot;pagination&quot;: {\n&quot;size&quot;:2,\n&quot;total&quot;:true\n}\n}\n//Response\n{\n&quot;pagination&quot;: {\n&quot;size&quot;: 2,\n&quot;page&quot;: 1,\n&quot;total&quot;: 11\n},\n&quot;result&quot;: [\n{\n&quot;id&quot;: 101,\n&quot;modificationCounter&quot;: 1,\n&quot;revision&quot;: null,\n&quot;waiterId&quot;: null,\n&quot;number&quot;: 1,\n&quot;state&quot;: &quot;OCCUPIED&quot;\n},\n{\n&quot;id&quot;: 102,\n&quot;modificationCounter&quot;: 1,\n&quot;revision&quot;: null,\n&quot;waiterId&quot;: null,\n&quot;number&quot;: 2,\n&quot;state&quot;: &quot;FREE&quot;\n}\n]\n}\nNote\nAs we are requesting with the total property set to true the server responds with the total count of rows for the query.\nFor retrieving a concrete page, we provide the page attribute with the desired value. Here we also left out the total property so the server doesn&#x2019;t incur on the effort to calculate it:\nPOST mythaistar/services/rest/tablemanagement/v1/table/search\n{\n&quot;pagination&quot;: {\n&quot;size&quot;:2,\n&quot;page&quot;:2\n}\n}\n//Response\n{\n&quot;pagination&quot;: {\n&quot;size&quot;: 2,\n&quot;page&quot;: 2,\n&quot;total&quot;: null\n},\n&quot;result&quot;: [\n{\n&quot;id&quot;: 103,\n&quot;modificationCounter&quot;: 1,\n&quot;revision&quot;: null,\n&quot;waiterId&quot;: null,\n&quot;number&quot;: 3,\n&quot;state&quot;: &quot;FREE&quot;\n},\n{\n&quot;id&quot;: 104,\n&quot;modificationCounter&quot;: 1,\n&quot;revision&quot;: null,\n&quot;waiterId&quot;: null,\n&quot;number&quot;: 4,\n&quot;state&quot;: &quot;FREE&quot;\n}\n]\n}\nQuery Meta-Parameters\nQueries can have meta-parameters and that are provided via SearchCriteriaTo. Besides paging (see above) we also get timeout support.\nAdvanced Queries\nWriting queries can sometimes get rather complex. The current examples given above only showed very simple basics. Within this topic a lot of advanced features need to be considered like:\nJoins\nConstructor queries\nOrder By (Sorting)\nGrouping\nHaving\nUnions\nSub-Queries\nAggregation functions like e.g. count/avg/sum\nDistinct selections\nSQL Hints (see e.g. Oracle hints or SQL-Server hints) - only when required for ultimate performance tuning\nThis list is just containing the most important aspects. As we can not cover all these topics here, they are linked to external documentation that can help and guide you.\n9.3.12. Spring-Data\nIf you are using the Spring Framework and have no restrictions regarding that, we recommend to use spring-data-jpa via devon4j-starter-spring-data-jpa that brings advanced integration (esp. for QueryDSL).\nMotivation\nThe benefits of spring-data are (for examples and explanations see next sections):\nAll you need is one single repository interface for each entity. No need for a separate implementation or other code artifacts like XML descriptors, NamedQueries class, etc.\nYou have all information together in one place (the repository interface) that actually belong together (where as in the classic approach you have the static queries in an XML file, constants to them in NamedQueries class and referencing usages in DAO implementation classes).\nStatic queries are most simple to realize as you do not need to write any method body. This means you can develop faster.\nSupport for paging is already build-in. Again for static query method the is nothing you have to do except using the paging objects in the signature.\nStill you have the freedom to write custom implementations via default methods within the repository interface (e.g. for dynamic queries).\nRepository\nFor each entity &#xAB;Entity&#xBB;Entity an interface is created with the name &#xAB;Entity&#xBB;Repository extending DefaultRepository.\nSuch repository is the analogy to a Data-Access-Object (DAO) used in the classic approach or when spring-data is not an option.\nExample\nThe following example shows how to write such a repository:\npublic interface ExampleRepository extends DefaultRepository&lt;ExampleEntity&gt; {\n@Query(&quot;SELECT example FROM ExampleEntity example&quot; //\n+ &quot; WHERE example.name = :name&quot;)\nList&lt;ExampleEntity&gt; findByName(@Param(&quot;name&quot;) String name);\n@Query(&quot;SELECT example FROM ExampleEntity example&quot; //\n+ &quot; WHERE example.name = :name&quot;)\nPage&lt;ExampleEntity&gt; findByNamePaginated(@Param(&quot;name&quot;) String name, Pageable pageable);\ndefault Page&lt;ExampleEntity&gt; findByCriteria(ExampleSearchCriteriaTo criteria) {\nExampleEntity alias = newDslAlias();\nJPAQuery&lt;ExampleEntity&gt; query = newDslQuery(alias);\nString name = criteria.getName();\nif ((name != null) &amp;&amp; !name.isEmpty()) {\nQueryUtil.get().whereString(query, $(alias.getName()), name, criteria.getNameOption());\n}\nreturn QueryUtil.get().findPaginated(criteria.getPageable(), query, false);\n}\n}\nThis ExampleRepository has the following features:\nCRUD support from spring-data (see JavaDoc for details).\nSupport for QueryDSL integration, paging and more as well as locking via GenericRepository\nA static query method findByName to find all ExampleEntity instances from DB that have the given name. Please note the @Param annotation that links the method parameter with the variable inside the query (:name).\nThe same with pagination support via findByNamePaginated method.\nA dynamic query method findByCriteria showing the QueryDSL and paging integration into spring-data provided by devon.\nFurther examples\nYou can also read the JUnit test-case DefaultRepositoryTest that is testing an example\nFooRepository.\nAuditing\nIn case you need auditing, you only need to extend DefaultRevisionedRepository instead of DefaultRepository. The auditing methods can be found in GenericRevisionedRepository.\nDependency\nIn case you want to switch to or add spring-data support to your devon application all you need is this maven dependency:\n&lt;!-- Starter for consuming REST services --&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.devonfw.java.starters&lt;/groupId&gt;\n&lt;artifactId&gt;devon4j-starter-spring-data-jpa&lt;/artifactId&gt;\n&lt;/dependency&gt;\nDrawbacks\nSpring-data also has some drawbacks:\nSome kind of magic behind the scenes that are not so easy to understand. So in case you want to extend all your repositories without providing the implementation via a default method in a parent repository interface you need to deep-dive into spring-data. We assume that you do not need that and hope what spring-data and devon already provides out-of-the-box is already sufficient.\nThe spring-data magic also includes guessing the query from the method name. This is not easy to understand and especially to debug. Our suggestion is not to use this feature at all and either provide a @Query annotation or an implementation via default method.\n9.3.13. Data Access Object\nThe Data Access Objects (DAOs) are part of the persistence layer.\nThey are responsible for a specific entity and should be named &#xAB;Entity&#xBB;Dao and &#xAB;Entity&#xBB;DaoImpl.\nThe DAO offers the so called CRUD-functionalities (create, retrieve, update, delete) for the corresponding entity.\nAdditionally a DAO may offer advanced operations such as query or locking methods.\nDAO Interface\nFor each DAO there is an interface named &#xAB;Entity&#xBB;Dao that defines the API. For CRUD support and common naming we derive it from the ApplicationDao interface that comes with the devon application template:\npublic interface MyEntityDao extends ApplicationDao&lt;MyEntity&gt; {\nList&lt;MyEntity&gt; findByCriteria(MyEntitySearchCriteria criteria);\n}\nAll CRUD operations are inherited from ApplicationDao so you only have to declare the additional methods.\nDAO Implementation\nImplementing a DAO is quite simple. We create a class named &#xAB;Entity&#xBB;DaoImpl that extends ApplicationDaoImpl and implements your &#xAB;Entity&#xBB;Dao interface:\npublic class MyEntityDaoImpl extends ApplicationDaoImpl&lt;MyEntity&gt; implements MyEntityDao {\npublic List&lt;MyEntity&gt; findByCriteria(MyEntitySearchCriteria criteria) {\nTypedQuery&lt;MyEntity&gt; query = createQuery(criteria, getEntityManager());\nreturn query.getResultList();\n}\n...\n}\nAgain you only need to implement the additional non-CRUD methods that you have declared in your &#xAB;Entity&#xBB;Dao interface.\nIn the DAO implementation you can use the method getEntityManager() to access the EntityManager from the JPA. You will need the EntityManager to create and execute queries.\nStatic queries for DAO Implementation\nAll static queries are declared in the file src\\main\\resources\\META-INF\\orm.xml:\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n&lt;entity-mappings version=&quot;1.0&quot; xmlns=&quot;http://java.sun.com/xml/ns/persistence/orm&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;\nxsi:schemaLocation=&quot;http://java.sun.com/xml/ns/persistence/orm http://java.sun.com/xml/ns/persistence/orm_1_0.xsd&quot;&gt;\n&lt;named-query name=&quot;find.dish.with.max.price&quot;&gt;\n&lt;query&gt;&lt;![SELECT dish FROM DishEntity dish WHERE dish.price &lt;= :maxPrice]]&gt;&lt;/query&gt;\n&lt;/named-query&gt;\n...\n&lt;/hibernate-mapping&gt;\nWhen your application is started, all these static queries will be created as prepared statements. This allows better performance and also ensures that you get errors for invalid JPQL queries when you start your app rather than later when the query is used.\nTo avoid redundant occurrences of the query name (get.open.order.positions.for.order) we define a constant for each named query:\npublic class NamedQueries {\npublic static final String FIND_DISH_WITH_MAX_PRICE = &quot;find.dish.with.max.price&quot;;\n}\nNote that changing the name of the java constant (FIND_DISH_WITH_MAX_PRICE) can be done easily with refactoring. Further you can trace where the query is used by searching the references of the constant.\nThe following listing shows how to use this query:\npublic List&lt;DishEntity&gt; findDishByMaxPrice(BigDecimal maxPrice) {\nQuery query = getEntityManager().createNamedQuery(NamedQueries.FIND_DISH_WITH_MAX_PRICE);\nquery.setParameter(&quot;maxPrice&quot;, maxPrice);\nreturn query.getResultList();\n}\nVia EntityManager.createNamedQuery(String) we create an instance of Query for our predefined static query.\nNext we use setParameter(String, Object) to provide a parameter (maxPrice) to the query. This has to be done for all parameters of the query.\nNote that using the createQuery(String) method, which takes the entire query as string (that may already contain the parameter) is not allowed to avoid SQL injection vulnerabilities.\nWhen the method getResultList() is invoked, the query is executed and the result is delivered as List. As an alternative, there is a method called getSingleResult(), which returns the entity if the query returned exactly one and throws an exception otherwise.\n9.3.14. JPA Performance\nWhen using JPA the developer sometimes does not see or understand where and when statements to the database are triggered.\nEstablishing expectations Developers shouldn&#x2019;t expect to sprinkle magic pixie dust on POJOs in hopes they will become persistent.\n&#x2014; Dan Allen\nhttps://epdf.tips/seam-in-action.html\nSo in case you do not understand what is going on under the hood of JPA, you will easily run into performance issues due to lazy loading and other effects.\nN plus 1 Problem\nThe most prominent phenomena is call the N+1 Problem.\nWe use entities from our MTS demo app as an example to explain the problem.\nThere is a DishEntity that has a @ManyToMany relation to\nIngredientEntity.\nNow we assume that we want to iterate all ingredients for a dish like this:\nDishEntity dish = dao.findDishById(dishId);\nBigDecimal priceWithAllExtras = dish.getPrice();\nfor (IngredientEntity ingredient : dish.getExtras()) {\npriceWithAllExtras = priceWithAllExtras.add(ingredient.getPrice());\n}\nNow dish.getExtras() is loaded lazy. Therefore the JPA vendor will provide a list with lazy initialized instances of IngredientEntity that only contain the ID of that entity. Now with every call of ingredient.getPrice() we technically trigger an SQL query statement to load the specific IngredientEntity by its ID from the database.\nNow findDishById caused 1 initial query statement and for any number N of ingredients we are causing an additional query statement. This makes a total of N+1 statements. As causing statements to the database is an expensive operation with a lot of overhead (creating connection, etc.) this ends in bad performance and is therefore a problem (the N+1 Problem).\nSolving N plus 1 Problem\nTo solve the N+1 Problem you need to change your code to only trigger a single statement instead. This can be archived in various ways. The most universal solution is to use FETCH JOIN`s in order to pre-load the nested `N child entities into the first level cache of the JPA vendor implementation. This will behave as if the @ManyToMany relation to IngredientEntity was having FetchType.EAGER but only for the that specific query and not in general. Because changing @ManyToMany to FetchType.EAGER would cause bad performance for other usecases where only the dish but not its extra ingredients are needed. For this reason all relations, including @OneToOne should always be FetchType.LAZY. Back to our example we simply replace dao.findDishById(dishId) with dao.findDishWithExtrasById(dishId) that we implement by the following JPQL query:\nSELECT dish FROM DishEntity dish\nLEFT JOIN FETCH dish.extras\nWHERE dish.id = :dishId\nThe rest of the code does not have to be changed but now dish.getExtras() will get the IngredientEntity from the first level cache where is was fetched by the initial query above.\nPlease note that if you only need the sum of the prices from the extras you can also create a query using an aggregator function:\nSELECT sum(dish.extras.price) FROM DishEntity dish\nAs you can see you need to understand the concepts in order to get good performance.\nThere are many advanced topics such as creating database indexes or calculating statistics for the query optimizer to get the best performance. For such advanced topics we recommend to have a database expert in your team that cares about such things. However, understanding the N+1 Problem and its solutions is something that every Java developer in the team needs to understand.\n9.4. Auditing\nFor database auditing we use hibernate envers. If you want to use auditing ensure you have the following dependency in your pom.xml:\n&lt;dependency&gt;\n&lt;groupId&gt;com.devonfw.java.modules&lt;/groupId&gt;\n&lt;artifactId&gt;devon4j-jpa-envers&lt;/artifactId&gt;\n&lt;/dependency&gt;\nMake sure that entity manager also scans the package from the devon4j-jpa[-envers] module in order to work properly. And make sure that correct Repository Factory Bean Class is chosen.\n@EntityScan(basePackages = { &quot;&#xAB;my.base.package&#xBB;&quot; }, basePackageClasses = { AdvancedRevisionEntity.class })\n...\n@EnableJpaRepositories(repositoryFactoryBeanClass = GenericRevisionedRepositoryFactoryBean.class)\n...\npublic class SpringBootApp {\n...\n}\nNow let your [Entity]Repository extend from DefaultRevisionedRepository instead of DefaultRepository.\nThe repository now has a method getRevisionHistoryMetadata(id) and getRevisionHistoryMetadata(id, boolean lazy) available to get a list of revisions for a given entity and a method find(id, revision) to load a specific revision of an entity with the given ID or getLastRevisionHistoryMetadata(id) to load last revision.\nTo enable auditing for a entity simply place the @Audited annotation to your entity and all entity classes it extends from.\n@Entity(name = &quot;Drink&quot;)\n@Audited\npublic class DrinkEntity extends ProductEntity implements Drink {\n...\nWhen auditing is enabled for an entity an additional database table is used to store all changes to the entity table and a corresponding revision number. This table is called &lt;ENTITY_NAME&gt;_AUD per default. Another table called REVINFO is used to store all revisions. Make sure that these tables are available. They can be generated by hibernate with the following property (only for development environments).\ndatabase.hibernate.hbm2ddl.auto=create\nAnother possibility is to put them in your database migration scripts like so.\nCREATE CACHED TABLE PUBLIC.REVINFO(\nid BIGINT NOT NULL generated by default as identity (start with 1),\ntimestamp BIGINT NOT NULL,\nuser VARCHAR(255)\n);\n...\nCREATE CACHED TABLE PUBLIC.&lt;TABLE_NAME&gt;_AUD(\n&lt;ALL_TABLE_ATTRIBUTES&gt;,\nrevtype TINYINT,\nrev BIGINT NOT NULL\n);\n9.5. Transaction Handling\nTransactions are technically processed by the data access layer. However, the transaction control has to be performed in upper layers. To avoid dependencies on persistence layer and technical code in upper layers, we use AOP to add transaction control via annotations as aspect.\nWe recommend using the @Transactional annotation (the JEE standard javax.transaction.Transactional rather than org.springframework.transaction.annotation.Transactional). We use this annotation in the logic layer to annotate business methods that participate in transactions (what typically applies to most up to all business components):\n@Transactional\npublic MyDataTo getData(MyCriteriaTo criteria) {\n...\n}\nIn case a service operation should invoke multiple use-cases, you would end up with multiple transactions what is undesired (what if the first TX succeeds and then the second TX fails?). Therefore you would then also annotate the service operation. This is not proposed as a pattern in any case as in some rare cases you need to handle constraint-violations from the database to create a specific business exception (with specified message). In such case you have to surround the transaction with a try {} catch statement what is not working if that method itself is @Transactional.\n9.5.1. Batches\nTransaction control for batches is a lot more complicated and is described in the batch layer.\n9.6. SQL\nFor general guides on dealing or avoiding SQL, preventing SQL-injection, etc. you should study data-access layer.\n9.6.1. Naming Conventions\nHere we define naming conventions that you should follow whenever you write SQL files:\nAll SQL-Keywords in UPPER CASE\nTable names in upper CamelCase (e.g. RestaurantOrder)\nColumn names in CamelCase (e.g. drinkState)\nIndentation should be 2 spaces as suggested by devonfw for every format.\nDDL\nFor DDLs follow these additional guidelines:\nID column names without underscore (e.g. tableId)\nDefine columns and constraints inline in the statement to create the table\nIndent column types so they all start in the same text column\nConstraints should be named explicitly (to get a reasonable hint error messages) with:\nPK_{table} for primary key (name optional here as PK constraint are fundamental)\nFK_{table}_{property} for foreign keys ({table} and {property} are both on the source where the foreign key is defined)\nUC_{table}_{property}[_{propertyN}]* for unique constraints\nCK_{table}_{check} for check constraints ({check} describes the check, if it is defined on a single property it should start with the property).\nDatabases have hard limitations for names (e.g. 30 characters). If you have to shorten names try to define common abbreviations in your project for according (business) terms. Especially do not just truncate the names at the limit.\nIf possible add comments on table and columns to help DBAs understanding your schema. This is also honored by many tools (not only DBA-tools).\nHere is a brief example of a DDL:\nCREATE SEQUENCE HIBERNATE_SEQUENCE START WITH 1000000;\n-- *** Table ***\nCREATE TABLE Table (\nid BIGINT NOT NULL AUTO_INCREMENT,\nmodificationCounter INTEGER NOT NULL,\nseatsNumber INTEGER NOT NULL,\nCONSTRAINT PK_Table PRIMARY KEY(id)\n);\n-- *** UserRole ***\nCREATE TABLE UserRole (\nid BIGINT NOT NULL AUTO_INCREMENT,\nmodificationCounter INTEGER NOT NULL,\nname VARCHAR (255),\nactive BOOLEAN,\nCONSTRAINT PK_UserRole PRIMARY KEY(id)\n-- *** User ***\nCREATE TABLE User (\nid BIGINT NOT NULL AUTO_INCREMENT,\nmodificationCounter INTEGER NOT NULL,\nusername VARCHAR (255) NULL,\npassword VARCHAR (255) NULL,\nemail VARCHAR (120) NULL,\nidRole BIGINT NOT NULL,\nCONSTRAINT PK_User PRIMARY KEY(id),\nCONSTRAINT PK_User_idRole FOREIGN KEY(idRole) REFERENCES UserRole(id) NOCHECK\n);\nCOMMENT ON TABLE User is &apos;The users of the restaurant site&apos;;\n...\nData\nFor insert, update, delete, etc. of data SQL scripts should additionally follow these guidelines:\nInserts always with the same order of columns in blocks for each table.\nInsert column values always starting with id, modificationCounter, [dtype, ] &#x2026;&#x200B;\nList columns with fixed length values (boolean, number, enums, etc.) before columns with free text to support alignment of multiple insert statements\nPro Tip: Get familiar with column mode of notepad++ when editing large blocks of similar insert statements.\nINSERT INTO UserRole(id, modificationCounter, name, active) VALUES (0, 1, &apos;Customer&apos;, true);\nINSERT INTO UserRole(id, modificationCounter, name, active) VALUES (1, 1, &apos;Waiter&apos;, true);\nINSERT INTO User(id, modificationCounter, username, password, email, idRole) VALUES (0, 1, &apos;user0&apos;, &apos;password&apos;, &apos;user0@mail.com&apos;, 0);\nINSERT INTO User(id, modificationCounter, username, password, email, idRole) VALUES (1, 1, &apos;waiter&apos;, &apos;waiter&apos;, &apos;waiter@mail.com&apos;, 1);\nINSERT INTO Table(id, modificationCounter, seatsNumber) VALUES (0, 1, 4);\nINSERT INTO Table(id, modificationCounter, seatsNumber) VALUES (1, 1, 4);\nINSERT INTO Table(id, modificationCounter, seatsNumber) VALUES (2, 1, 4);\nINSERT INTO Table(id, modificationCounter, seatsNumber) VALUES (3, 1, 4);\nINSERT INTO Table(id, modificationCounter, seatsNumber) VALUES (4, 1, 6);\nINSERT INTO Table(id, modificationCounter, seatsNumber) VALUES (5, 1, 6);\nINSERT INTO Table(id, modificationCounter, seatsNumber) VALUES (6, 1, 6);\nINSERT INTO Table(id, modificationCounter, seatsNumber) VALUES (7, 1, 8);\nINSERT INTO Table(id, modificationCounter, seatsNumber) VALUES (8, 1, 8);\n...\nSee also Database Migrations.\n9.7. Database Migration\nFor database migrations we use Flyway.\nAs illustrated here database migrations have three advantages:\nRecreate a database from scratch\nMake it clear at all times what state a database is in\nMigrate in a deterministic way from your current version of the database to a newer one\nFlyway can be used standalone or can be integrated via its API to make sure the database migration takes place on startup.\n9.7.1. Organizational Advice\nA few considerations with respect to project organization will help to implement maintainable Flyway migrations.\nAt first, testing and production environments must be clearly and consistently distinguished. Use the following directory structure to achieve this distinction:\nsrc/main/resources/db\nsrc/test/resources/db\nAlthough this structure introduces redundancies, the benefit outweighs this disadvantage.\nAn even more fine-grained production directory structure which contains one sub folder per release should be implemented:\nsrc/main/resources/db/migration/releases/X.Y/x.sql\nEmphasizing that migration scripts below the current version must never be changed will aid the second advantage of migrations: it will always be clearly reproducible in which state the database currently is.\nHere, it is important to mention that, if test data is required, it must be managed separately from the migration data in the following directory:\nsrc/test/resources/db/migration/\nThe migration directory is added to aid easy usage of Flyway defaults.\nOf course, test data should also be managed per release as like production data.\nWith regard to content, separation of concerns (SoC) is an important goal. SoC can be achieved by distinguishing and writing multiple scripts with respect to business components/use cases (or database tables in case of large volumes of master data [1]. Comprehensible file names aid this separation.\nIt is important to have clear responsibilities regarding the database, the persistence layer (JPA), and migrations. Therefore a dedicated database expert should be in charge of any migrations performed or she should at least be informed before any change to any of the mentioned parts is applied.\n9.7.2. Technical Configuration\nDatabase migrations can be SQL based or Java based.\nTo enable auto migration on startup (not recommended for productive environment) set the following property in the application.properties file for an environment.\nflyway.enabled=true\nflyway.clean-on-validation-error=false\nFor development environment it is helpful to set both properties to true in order to simplify development. For regular environments flyway.clean-on-validation-error should be false.\nIf you want to use Flyway set the following property in any case to prevent Hibernate from doing changes on the database (pre-configured by default in devonfw):\nspring.jpa.hibernate.ddl-auto=validate\nThe setting must be communicated to and coordinated with the customer and their needs.\nIn acceptance testing the same configuration as for the production environment should be enabled.\nSince migration scripts will also be versioned the end-of-line (EOL) style must be fixated according to this issue. This is however solved in flyway 4.0+ and the latest devonfw release.\nAlso, the version numbers of migration scripts should not consist of simple ascending integer numbers like V0001&#x2026;&#x200B;, V0002&#x2026;&#x200B;, &#x2026;&#x200B; This naming may lead to problems when merging branches. Instead the usage of timestamps as version numbers will help to avoid such problems.\n9.7.3. Naming Conventions\nDatabase migrations should follow this naming convention:\nV&lt;version&gt;__&lt;description&gt; (e.g.: V12345__Add_new_table.sql).\nIt is also possible to use Flyway for test data. To do so place your test data migrations in src/main/resources/db/testdata/ and set property\nflyway.locations=classpath:db/migration/releases,classpath:db/migration/testdata\nThen Flyway scans the additional location for migrations and applies all in the order specified by their version. If migrations V0001__... and V0002__... exist and a test data migration should be applied in between you can name it V0001_1__....\n9.8. SAP HANA\nThis section contains hints for those who use SAP HANA, a very powerful and fast RDBMS. If you have chosen a different persistence technology on purpose you can simply ignore this guide. Besides general hints about the driver there are tips for more tight integration with other SAP features or products.\n9.8.1. Driver\nThe hana JDBC driver is available in Maven Central what makes your life very easy. All you need is the following maven dependency:\n&lt;dependency&gt;\n&lt;groupId&gt;com.sap.cloud.db.jdbc&lt;/groupId&gt;\n&lt;artifactId&gt;ngdbc&lt;/artifactId&gt;\n&lt;version&gt;${hana.driver.version}&lt;/version&gt;\n&lt;/dependency&gt;\nThe variable hana.driver.version may be 2.3.55, but check yourself at http://central.maven.org/maven2/com/sap/cloud/db/jdbc/ngdbc/ for the proper or most recent version.\n9.8.2. Developer Usage\nFor your local development environment you will love the free SAP HANA, Express Edition.\nYou can run HANA in several ways:\nOn-premise\nVia a Docker image (Linux only)\nVia a pre-configured virtual machine (Windows, Linux, OS X)\nInstalled natively on your local machine (Linux only)\nIn the cloud\nVia a pre-configured machine on the Google Cloud Platform\nVia a pre-configured machine in the Microsoft Azure Cloud\nVia a pre-configured machine on Amazon Web Services\nTo get started with SAP HANA, Express Edition you can check out the tutorials at the SAP Developer Center.\n9.8.3. Pooling\nTODO\n9.8.4. Fuzzy Search\nSee https://blogs.sap.com/2015/08/28/dynamism-of-fuzzy-search-in-sap-hana/ or the SAP HANA Search Developer Guide\n9.9. Oracle RDBMS\nThis section contains hints for those who use Oracle RDBMS. If you use a different persistence technology you can simply ignore it. Besides general hints about the driver there are tips for more tight integration with other Oracle features or products. However, if you work for a project where Oracle RDBMS is settled and not going to be replaced (you are in a vendor lock-in anyway), you might want to use even more from Oracle technology to take advantage from a closer integration.\n9.9.1. XE\nFor local development you should setup Oracle XE (eXpress Edition).\nYou need an oracle account, then you can download it from here.\nThe most comfortable way to run it as needed is using docker. You can build your own docker image from the downloaded RPM using the instructions and dockerfile from oracle. (In case the build of the docker-image fails reproducibly and you want to give up with the Dockerfiles from Oracle you can also try this inofficial docker-oracle-xe solution).\nTo connect to your local XE database you need to use xe as the SID of your main database that can not be changed. The hostname should be localhost and the port is by default 1521 if you did not remap it with docker to something else. However, starting with XE 18c you need to be aware that oracle introduced a multi-tenant architecture. Hence xe refers to the root CDB while you typically want to connect to the PDB (pluggable database) and XE ships with exactly one of this called xepdb1. To connect with SQL Developer switch Connection Type from Basic to Advanced and enter the Custom JDBC URL like e.g.\njdbc:oracle:thin:@//localhost:1521/xepdb1\nThe same way you can also connect from your devon4j app via JDBC.\n9.9.2. Driver\nThe oracle JDBC driver is not available in maven central. Depending on the Oracle DB version and the Java version, you can use either the 11g/ojdbc6, 12c/ojdbc7, or 12c/ojdbc8 version of the driver. Oracle JDBC drivers usually are backward and forward compatible so you should be able to use the 12c/ojdbc8 driver with an 11g DB etc. As a rule of thumb, use the 12c/ojdbc8 driver unless you must use Java7. All JDBC drivers can be downloaded without registration: 11g/ojdbc6, 12c/ojdbc7, and 12c/ojdbc8. Your project should use a maven repository server such as nexus or artifactory.\nYour dependency for the oracle driver should look as follows (use artifactId &quot;ojdbc6&quot; or &quot;ojdbc7&quot; for the older drivers):\n&lt;dependency&gt;\n&lt;groupId&gt;com.oracle&lt;/groupId&gt;\n&lt;artifactId&gt;ojdbc8&lt;/artifactId&gt;\n&lt;version&gt;${oracle.driver.version}&lt;/version&gt;\n&lt;/dependency&gt;\noracle.driver.version being 11.2.0.4 for 11g/ojdbc6, or 12.1.0.1 for 12c/ojdbc7, or 12.2.0.1 for 12c/ojdbc8 or newer\n9.9.3. Pooling\nIn order to boost performance JDBC connections should be pooled and reused. If you are using Oracle RDBMS and do not plan to change that you can use the Oracle specific connection pool &quot;Universal Connection Pool (UCP)&quot; that is perfectly integrated with the Oracle driver. According to the documentation, UCP can even be used to manage third party data sources. The 11g version of UCP can be downloaded without registration here, the 12c version of UCP is available at the same download locations as the 12c JDBC driver (see above). As a rule of thumb, use the version that is the same as the JDBC driver version.\nAgain, you have to upload the artefact manually to your maven repository. The dependency should look like this:\n&lt;dependency&gt;\n&lt;groupId&gt;com.oracle&lt;/groupId&gt;\n&lt;artifactId&gt;ucp&lt;/artifactId&gt;\n&lt;version&gt;${oracle.ucp.version}&lt;/version&gt;\n&lt;/dependency&gt;\nwith oracle.ucp.version being 11.2.0.4 or 12.2.0.1 or newer.\nConfiguration is done via application.properties like this (example):\n#Oracle UCP\n# Datasource for accessing the database\nspring.datasource.url=jdbc:oracle:thin:@192.168.58.2:1521:xe\nspring.jpa.database-platform=org.hibernate.dialect.Oracle12cDialect\nspring.datasource.user=MyUser\nspring.datasource.password=ThisIsMyPassword\nspring.datasource.driver-class-name=oracle.jdbc.OracleDriver\nspring.datasource.schema=MySchema\nspring.datasource.type=oracle.ucp.jdbc.PoolDataSourceImpl\nspring.datasource.factory=oracle.ucp.jdbc.PoolDataSourceFactory\nspring.datasource.factory-method=getPoolDataSource\nspring.datasource.connectionFactoryClassName=oracle.jdbc.pool.OracleDataSource\nspring.datasource.validateConnectionOnBorrow=true\nspring.datasource.connectionPoolName=MyPool\nspring.datasource.jmx-enabled=true\n# Optional: Set the log level to INTERNAL_ERROR, SEVERE, WARNING, INFO, CONFIG, FINE, TRACE_10, FINER, TRACE_20, TRACE_30, or FINEST\n# logging.level.oracle.ucp=INTERNAL_ERROR\n# Optional: activate tracing\n# logging.level.oracle.ucp.jdbc.oracle.OracleUniversalPooledConnection=TRACE\n#Optional: Configures pool size manually\n#spring.datasource.minPoolSize=10\n#spring.datasource.maxPoolSize=40\n#spring.datasource.initialPoolSize=20\nResources: FAQ, developer&#x2019;s guide, Java API Reference. For an in-depth discussion on how to use JDBC and UCP, see the Oracle documentation Connection Management Strategies for Java Applications using JDBC and UCP.\nNote: there is a bug in UCP 12.1.0.2 that results in the creation of thousands of java.lang.Timer threads over hours or days of system uptime (see article on stackoverflow). Also, Oracle has a strange bug fixing / patching policy: instead of producing a fixed version 12.1.0.3 or 12.1.0.2.x, Oracle publishes collections of *.class files that must be manually patched into the ucp.jar! Therefore, use the newest versions only.\n9.9.4. Messaging\nIn case you want to do messaging based on JMS you might consider the Oracle JMS also called Oracle Streams Advanced Queuing, or Oracle Advanced Queuing, or OAQ or AQ for short. OAQ is a JMS provider based on the Oracle RDBMS and included in the DB product for no extra fee. OAQ has some features that exceed the JMS standard like a retention time (i.e. a built-in backup mechanism that allows to make messages &quot;unread&quot; within a configurable period of time so that these messages do not have to be resent by the sending application). Also, OAQ messages are stored in relational tables so they can easily be observed by a test driver in a system test scenario.\nCapgemini has used the Spring Data JDBC Extension in order to process OAQ messages within the same technical transaction as the resulting Oracle RDBMS data changes without using 2PC and an XA-compliant transaction manager - which is not available out of the box in Tomcat. This is possible only due to the fact that OAQ queues and RDBMS tables actually reside in the same database. However, this is higher magic and should only be tried if high transaction rates must be achieved by avoiding 2PC.\n9.9.5. General Notes on the use of Oracle products\nOracle sells commercial products and receives licence fees for them. This includes access to a support organization. Therefore, at an early stage of your project, prepare for contacting oracle support in case of technical problems. You will need the Oracle support ID of your customer [i.e. the legal entity who pays the licence fee and runs the RDBMS] and your customer must grant you permission to use it in a service request - it is not legal to use a your own support ID in a customer-related project. Your customer pays for that service anyway, so use it in case of a problem!\nSoftware components like the JDBC driver or the UCP may be available without a registration or fee but they are protected by the Oracle Technology Network (OTN) License Agreement. The most important aspect of this licence agreement is the fact that an IT service provider is not allowed to simply download the Oracle software component, bundle it in a software artefact and deliver it to the customer. Instead, the Oracle software component must be (from a legal point of view) provided by the owner of the Oracle DB licence (i.e. your customer). This can be achieved in two ways: Advise your customer to install the Oracle software component in the application server as a library that can be used by your custom built system. Or, in cases where this is not feasible, e.g. in a OpenShift environment where the IT service provider delivers complete Docker images, you must advise your customer to (legally, i.e. documented in a written form) provide the Oracle software component to you, i.e. you don&#x2019;t download the software component from the Oracle site but receive it from your customer.\n9.10. JEE\nThis section is about Java Enterprise Edition (JEE). Regarding to our key principles we focus on open standards. For Java this means that we consider official standards from Java Standard and Enterprise Edition as first choice for considerations. Therefore we also decided to recommend JAX-RS over SpringMVC as the latter is proprietary. Only if an existing Java standard is not suitable for current demands such as Java Server Faces (JSF), we do not officially recommend it (while you are still free to use it if you have good reasons to do so). In all other cases we officially suggest the according standard and use it in our guides, code-samples, sample application, modules, templates, etc. Examples for such standards are JPA, JAX-RS, JAX-WS, JSR330, JSR250, JAX-B, etc.\n9.10.1. Application-Server\nWe designed everything based on standards to work with different technology stacks and servlet containers. However, we strongly encourage to use spring and spring-boot. You are free to decide for something else but here is a list of good reasons for our decision:\nUp-to-date\nWith spring you easily keep up to date with evolving technologies (microservices, reactive, NoSQL, etc.). Most application servers put you in a jail with old legacy technology. In many cases you are even forced to use a totally outdated version of java (JVM/JDK). This may even cause severe IT-Security vulnerabilities but with expensive support you might get updates. Also with spring and open-source you need to be aware that for IT-security you need to update recently what can cost quite a lot of additional maintenance effort.\nDevelopment speed\nWith spring-boot you can implement and especially test your individual logic very fast. Starting the app in your IDE is very easy, fast, and realistic (close to production). You can easily write JUnit tests that startup your server application to e.g. test calls to your remote services via HTTP fast and easy. For application servers you need to bundle and deploy your app what takes more time and limits you in various ways. We are aware that this has improved in the past but also spring continuously improves and is always way ahead in this area. Further, with spring you have your configurations bundled together with the code in version control (still with ability to handle different environments) while with application servers these are configured externally and can not be easily tested during development.\nDocumentation\nSpring has an extremely open and active community. There is documentation for everything available for free on the web. You will find solutions to almost any problem on platforms like stackoverflow. If you have a problem you are only a google search away from your solution. This is very much different for proprietary application server products.\nHelpful Exception Messages\nSpring is really great for developers on exception messages. If you do something wrong you get detailed and helpful messages that guide you to the problem or even the solution. This is not as great in application servers.\nFuture-proof\nSpring has evolved really awesome over time. Since its 1.0 release in 2004 spring has continuously been improved and always caught up with important trends and innovations. Even in critical situations, when the company behind it (interface21) was sold, spring went on perfectly.\nJEE went through a lot of trouble and crisis. Just look at the EJB pain stories. This happened often in the past and also recent. See JEE 8 in crisis.\nFree\nSpring and its ecosystem is free and open-source. It still perfectly integrates with commercial solutions for specific needs. Most application servers are commercial and cost a lot of money. As of today the ROI for this is of question.\nFun\nIf you go to conferences or ask developers you will see that spring is popular and fun. If new developers are forced to use an old application server product they will be less motivated or even get frustrated. Especially in today&#x2019;s agile projects this is a very important aspect. In the end you will get into trouble with maintenance on the long run if you rely on a proprietary application server.\nOf course the vendors of application servers will tell you a different story. This is simply because they still make a lot of money from their products. We do not get paid from application servers nor from spring. We are just developers who love to build great systems. A good reason for application servers is that they combine a set of solutions to particular aspects to one product that helps to standardize your IT. However, devonfw fills exactly this gap for the spring ecosystem in a very open and flexible way. However, there is one important aspect that you need to understand and be aware of:\nSome big companies decided for a specific application server as their IT strategy. They may have hundreds of apps running with this application server. All their operators and developers have learned a lot of specific skills for this product and are familiar with it. If you are implementing yet another (small) app in this context it can make sense to stick with this application server. However, also they have to be aware that with every additional app they increase their technical debt.\n9.11. Logging\nWe use SLF4J as API for logging. The recommended implementation is Logback for which we provide additional value such as configuration templates and an appender that prevents log-forging and reformatting of stack-traces for operational optimizations.\n9.11.1. Usage\nMaven Integration\nIn the pom.xml of your application add this dependency (that also adds transitive dependencies to SLF4J and logback):\n&lt;dependency&gt;\n&lt;groupId&gt;com.devonfw.java&lt;/groupId&gt;\n&lt;artifactId&gt;devon4j-logging&lt;/artifactId&gt;\n&lt;/dependency&gt;\nConfiguration\nThe configuration file is logback.xml and is to put in the directory src/main/resources of your main application. For details consult the logback configuration manual. devon4j provides a production ready configuration here. Simply copy this configuration into your application in order to benefit from the provided operational and security aspects. We do not include the configuration into the devon4j-logging module to give you the freedom of customizations (e.g. tune log levels for components and integrated products and libraries of your application).\nThe provided logback.xml is configured to use variables defined on the config/application.properties file. On our example, the log files path point to ../logs/ in order to log to tomcat log directory when starting tomcat on the bin folder. Change it according to your custom needs.\nListing 6. config/application.properties\nlog.dir=../logs/\nLogger Access\nThe general pattern for accessing loggers from your code is a static logger instance per class. We pre-configured the development environment so you can just type LOG and hit [ctrl][space] (and then [arrow up]) to insert the code pattern line into your class:\npublic class MyClass {\nprivate static final Logger LOG = LoggerFactory.getLogger(MyClass.class);\n...\n}\nPlease note that in this case we are not using injection pattern but use the convenient static alternative. This is already a common solution and also has performance benefits.\nHow to log\nWe use a common understanding of the log-levels as illustrated by the following table. This helps for better maintenance and operation of the systems by combining both views.\nTable 7. Log-levels\nLog-level\nDescription\nImpact\nActive Environments\nFATAL\nOnly used for fatal errors that prevent the application to work at all (e.g. startup fails or shutdown/restart required)\nOperator has to react immediately\nall\nERROR\nAn abnormal error indicating that the processing failed due to technical problems.\nOperator should check for known issue and otherwise inform development\nall\nWARNING\nA situation where something worked not as expected. E.g. a business exception or user validation failure occurred.\nNo direct reaction required. Used for problem analysis.\nall\nINFO\nImportant information such as context, duration, success/failure of request or process\nNo direct reaction required. Used for analysis.\nall\nDEBUG\nDevelopment information that provides additional context for debugging problems.\nNo direct reaction required. Used for analysis.\ndevelopment and testing\nTRACE\nLike DEBUG but exhaustive information and for code that is run very frequently. Will typically cause large log-files.\nNo direct reaction required. Used for problem analysis.\nnone (turned off by default)\nExceptions (with their stacktrace) should only be logged on FATAL or ERROR level. For business exceptions typically a WARNING including the message of the exception is sufficient.\n9.11.2. Operations\nLog Files\nWe always use the following log files:\nError Log: Includes log entries to detect errors.\nInfo Log: Used to analyze system status and to detect bottlenecks.\nDebug Log: Detailed information for error detection.\nThe log file name pattern is as follows:\n&#xAB;LOGTYPE&#xBB;_log_&#xAB;HOST&#xBB;_&#xAB;APPLICATION&#xBB;_&#xAB;TIMESTAMP&#xBB;.log\nTable 8. Segments of Logfilename\nElement\nValue\nDescription\n&#xAB;LOGTYPE&#xBB;\ninfo, error, debug\nType of log file\n&#xAB;HOST&#xBB;\ne.g. mywebserver01\nName of server, where logs are generated\n&#xAB;APPLICATION&#xBB;\ne.g. myapp\nName of application, which causes logs\n&#xAB;TIMESTAMP&#xBB;\nYYYY-MM-DD_HH00\ndate of log file\nExample:\nerror_log_mywebserver01_myapp_2013-09-16_0900.log\nError log from mywebserver01 at application myapp at 16th September 2013 9pm.\nOutput format\nWe use the following output format for all log entries to ensure that searching and filtering of log entries work consistent for all logfiles:\n[D: &#xAB;timestamp&#xBB;] [P: &#xAB;priority&#xBB;] [C: &#xAB;NDC&#xBB;][T: &#xAB;thread&#xBB;][L: &#xAB;logger&#xBB;]-[M: &#xAB;message&#xBB;]\nD: Date (Timestamp in ISO8601 format e.g. 2013-09-05 16:40:36,464)\nP: Priority (the log level)\nC: Correlation ID (ID to identify users across multiple systems, needed when application is distributed)\nT: Thread (Name of thread)\nL: Logger name (use class name)\nM: Message (log message)\nExample:\n[D: 2013-09-05 16:40:36,464] [P: DEBUG] [C: 12345] [T: main] [L: my.package.MyClass]-[M: My message...]\n9.11.3. Security\nIn order to prevent log forging attacks we provide a special appender for logback in devonfw-logging. If you use it (see configuration) you are safe from such attacks.\n9.11.4. Correlation ID\nIn order to correlate separate HTTP requests to services belonging to the same user / session, we provide a servlet filter called DiagnosticContextFilter. This filter takes a provided correlation ID from the HTTP header X-Correlation-Id. If none was found, it will generate a new correlation id as UUID. This correlation ID is added as MDC to the logger. Therefore, it will then be included to any log message of the current request (thread). Further concepts such as service invocations will pass this correlation ID to subsequent calls in the application landscape. Hence you can find all log messages related to an initial request simply via the correlation ID even in highly distributed systems.\n9.11.5. Monitoring\nIn highly distributed systems (from clustering up to microservices) it might get tedious to search for problems and details in log files. Therefore, we recommend to setup a central log and analysis server for your application landscape. Then you feed the logs from all your applications (using logstash) into that central server that adds them to a search index to allow fast searches (using elasticsearch). This should be completed with a UI that allows dashboards and reports based on data aggregated from all the logs.\nThis is addressed by ELK or Graylog.\n9.12. Security\nSecurity is todays most important cross-cutting concern of an application and an enterprise IT-landscape. We seriously care about security and give you detailed guides to prevent pitfalls, vulnerabilities, and other disasters. While many mistakes can be avoided by following our guidelines you still have to consider security and think about it in your design and implementation. The security guide will not only automatically prevent you from any harm, but will provide you hints and best practices already used in different software products.\nAn important aspect of security is proper authentication and authorization as described in access-control. In the following we discuss about potential vulnerabilities and protection to prevent them.\n9.12.1. Vulnerabilities and Protection\nIndependent from classical authentication and authorization mechanisms there are many common pitfalls that can lead to vulnerabilities and security issues in your application such as XSS, CSRF, SQL-injection, log-forging, etc. A good source of information about this is the OWASP.\nWe address these common threats individually in security sections of our technological guides as a concrete solution to prevent an attack typically depends on the according technology. The following table illustrates common threats and contains links to the solutions and protection-mechanisms provided by the devonfw:\nTable 9. Security threats and protection-mechanisms\nThreat\nProtection\nLink to details\nA1 Injection\nvalidate input, escape output, use proper frameworks\nSQL Injection\nA2 Broken Authentication and Session Management\nencrypt all channels, use a central identity management with strong password-policy\nAuthentication\nA3 XSS\nprevent injection (see A1) for HTML, JavaScript and CSS and understand same-origin-policy\nclient-layer\nA4 Insecure Direct Object References\nUsing direct object references (IDs) only with appropriate authorization\nlogic-layer\nA5 Security Misconfiguration\nUse devonfw application template and guides to avoid\napplication template and sensitive configuration\nA6 Sensitive Data Exposure\nUse secured exception facade, design your data model accordingly\nREST exception handling\nA7 Missing Function Level Access Control\nEnsure proper authorization for all use-cases, use @DenyAll as default to enforce\nMethod authorization\nA8 CSRF\nsecure mutable service operations with an explicit CSRF security token sent in HTTP header and verified on the server\nservice-layer security\nA9 Using Components with Known Vulnerabilities\nsubscribe to security newsletters, recheck products and their versions continuously, use devonfw dependency management\nCVE newsletter and dependency check\nA10 Unvalidated Redirects and Forwards\nAvoid using redirects and forwards, in case you need them do a security audit on the solution.\ndevonfw proposes to use rich-clients (SPA/RIA). We only use redirects for login in a safe way.\nLog-Forging\nEscape newlines in log messages\nlogging security\n9.12.2. Tools\nDependency Check\nTo address A9 Using Components with Known Vulnerabilities we integrated OWASP dependency check into the devonfw maven build. If you build an devonfw application (sample or any app created from our app-template) you can activate dependency check with the security profile:\nmvn clean install -P security\nThis does not run by default as it causes a huge overhead for the build performance. However , consider to build this in your CI at least nightly.\nAfter the dependency check is performed , you will find the results in target/dependency-check-report.html of each module. The report will also always be generated when the site is build (mvn site).\nPenetration Testing\nFor penetration testing (testing for vulnerabilities) of your web application, we recommend the following tools:\nZAP (OWASP Zed Attack Proxy Project)\nsqlmap (or HQLmap)\nnmap\nSee the marvelous presentation Toolbox of a security professional from Christian Schneider.\n9.13. Access-Control\nAccess-Control is a central and important aspect of Security. It consists of two major aspects:\nAuthentication (Who tries to access?)\nAuthorization (Is the one accessing allowed to do what he wants to do?)\n9.13.1. Authentication\nDefinition:\nAuthentication is the verification that somebody interacting with the system is the actual subject for whom he claims to be.\nThe one authenticated is properly called subject or principal. However, for simplicity we use the common term user even though it may not be a human (e.g. in case of a service call from an external system).\nTo prove his authenticity the user provides some secret called credentials. The most simple form of credentials is a password.\nNote\nPlease never implement your own authentication mechanism or credential store. You have to be aware of implicit demands such as salting and hashing credentials, password life-cycle with recovery, expiry, and renewal including email notification confirmation tokens, central password policies, etc. This is the domain of access managers and identity management systems. In a business context you will typically already find a system for this purpose that you have to integrate (e.g. via LDAP). Otherwise you should consider establishing such a system e.g. using keycloak.\nWe use spring-security as a framework for authentication purposes.\nTherefore you need to provide an implementation of WebSecurityConfigurerAdapter:\n@Configuration\n@EnableWebSecurity\npublic class MyWebSecurityConfig extends WebSecurityConfigurerAdapter {\n@Inject\nprivate UserDetailsService userDetailsService;\n...\npublic void configure(HttpSecurity http) throws Exception {\nhttp.userDetailsService(this.userDetailsService)\n.authorizeRequests().antMatchers(&quot;/public/**&quot;).permitAll()\n.anyRequest().authenticated().and()\n...\n}\n}\nAs you can see spring-security offers a fluent API for easy configuration. You can simply add invocations like formLogin().loginPage(&quot;/public/login&quot;) or httpBasic().realmName(&quot;MyApp&quot;). Also CSRF protection can be configured by invoking csrf().\nFor further details see spring Java-config for HTTP security.\nFurther, you need to provide an implementation of the UserDetailsService interface.\nA good starting point comes with our application template.\nPreserve original request anchors after form login redirect\nSpring Security will automatically redirect any unauthorized access to the defined login-page. After successful login, the user will be redirected to the original requested URL. The only pitfall is, that anchors in the request URL will not be transmitted to server and thus cannot be restored after successful login. Therefore the devon4j-security module provides the RetainAnchorFilter, which is able to inject javascript code to the source page and to the target page of any redirection. Using javascript this filter is able to retrieve the requested anchors and store them into a cookie. Heading the target URL this cookie will be used to restore the original anchors again.\nTo enable this mechanism you have to integrate the RetainAnchorFilter as follows:\nFirst, declare the filter with\nstoreUrlPattern: an regular expression matching the URL, where anchors should be stored\nrestoreUrlPattern: an regular expression matching the URL, where anchors should be restored\ncookieName: the name of the cookie to save the anchors in the intermediate time\nYou can easily configure this as code in your WebSecurityConfig as following:\nRetainAnchorFilter filter = new RetainAnchorFilter();\nfilter.setStoreUrlPattern(&quot;http://[^/]+/[^/]+/login.*&quot;);\nfilter.setRestoreUrlPattern(&quot;http://[^/]+/[^/]+/.*&quot;);\nfilter.setCookieName(&quot;TARGETANCHOR&quot;);\nhttp.addFilterBefore(filter, UsernamePasswordAuthenticationFilter.class);\nUsers vs. Systems\nIf we are talking about authentication we have to distinguish two forms of principals:\nhuman users\nautonomous systems\nWhile e.g. a Kerberos/SPNEGO Single-Sign-On makes sense for human users it is pointless for authenticating autonomous systems. So always keep this in mind when you design your authentication mechanisms and separate access for human users from access for systems.\nMixed Authentication\nIn rare cases you might need to mix multiple authentication mechanisms (form based, basic-auth, SAMLv2, OAuth, etc.) within the same app (for different URLs). For KISS this should be avoided where possible. However, when needed, you can find a solution\nhere.\n9.13.2. Authorization\nDefinition:\nAuthorization is the verification that an authenticated user is allowed to perform the operation he intends to invoke.\nClarification of terms\nFor clarification we also want to give a common understanding of related terms that have no unique definition and consistent usage in the wild.\nTable 10. Security terms related to authorization\nTerm\nMeaning and comment\nPermission\nA permission is an object that allows a principal to perform an operation in the system. This permission can be granted (give) or revoked (taken away). Sometimes people also use the term right what is actually wrong as a right (such as the right to be free) can not be revoked.\nGroup\nWe use the term group in this context for an object that contains permissions. A group may also contain other groups. Then the group represents the set of all recursively contained permissions.\nRole\nWe consider a role as a specific form of group that also contains permissions. A role identifies a specific function of a principal. A user can act in a role.\nFor simple scenarios a principal has a single role associated. In more complex situations a principal can have multiple roles but has only one active role at a time that he can choose out of his assigned roles. For KISS it is sometimes sufficient to avoid this by creating multiple accounts for the few users with multiple roles. Otherwise at least avoid switching roles at run-time in clients as this may cause problems with related states. Simply restart the client with the new role as parameter in case the user wants to switch his role.\nAccess Control\nAny permission, group, role, etc., which declares a control for access management.\nSuggestions on the access model\nFor the access model we give the following suggestions:\nEach Access Control (permission, group, role, &#x2026;&#x200B;) is uniquely identified by a human readable string.\nWe create a unique permission for each use-case.\nWe define groups that combine permissions to typical and useful sets for the users.\nWe define roles as specific groups as required by our business demands.\nWe allow to associate users with a list of Access Controls.\nFor authorization of an implemented use case we determine the required permission. Furthermore, we determine the current user and verify that the required permission is contained in the tree spanned by all his associated Access Controls. If the user does not have the permission we throw a security exception and thus abort the operation and transaction.\nWe avoid negative permissions, that is a user has no permission by default and only those granted to him explicitly give him additional permission for specific things. Permissions granted can not be reduced by other permissions.\nTechnically we consider permissions as a secret of the application. Administrators shall not fiddle with individual permissions but grant them via groups. So the access management provides a list of strings identifying the Access Controls of a user. The individual application itself contains these Access Controls in a structured way, whereas each group forms a permission tree.\nNaming conventions\nAs stated above each Access Control is uniquely identified by a human readable string. This string should follow the naming convention:\n&#xAB;app-id&#xBB;.&#xAB;local-name&#xBB;\nFor Access Control Permissions the &#xAB;local-name&#xBB; again follows the convention:\n&#xAB;verb&#xBB;&#xAB;object&#xBB;\nThe segments are defined by the following table:\nTable 11. Segments of Access Control Permission ID\nSegment\nDescription\nExample\n&#xAB;app-id&#xBB;\nIs a unique technical but human readable string of the application (or microservice). It shall not contain special characters and especially no dot or whitespace. We recommend to use lower-train-case-ascii-syntax. The identity and access management should be organized on enterprise level rather than application level. Therefore permissions of different apps might easily clash (e.g. two apps might both define a group ReadMasterData but some user shall get this group for only one of these two apps). Using the &#xAB;app-id&#xBB;. prefix is a simple but powerful namespacing concept that allows you to scale and grow. You may also reserve specific &#xAB;app-id&#xBB;s for cross-cutting concerns that do not actually reflect a single app e.g to grant access to a geographic region.\nshop\n&#xAB;verb&#xBB;\nThe action that is to be performed on &#xAB;object&#xBB;. We use Find for searching and reading data. Save shall be used both for create and update. Only if you really have demands to separate these two you may use Create in addition to Save. Finally, Delete is used for deletions. For non CRUD actions you are free to use additional verbs such as Approve or Reject.\nFind\n&#xAB;object&#xBB;\nThe affected object or entity. Shall be named according to your data-model\nProduct\nSo as an example shop.FindProduct will reflect the permission to search and retrieve a Product in the shop application. The group shop.ReadMasterData may combine all permissions to read master-data from the shop. However, also a group shop.Admin may exist for the Admin role of the shop application. Here the &#xAB;local-name&#xBB; is Admin that does not follow the &#xAB;verb&#xBB;&#xAB;object&#xBB; schema.\ndevon4j-security\nThe devonfw provides a ready to use module devon4j-security that is based on spring-security and makes your life a lot easier.\nFigure 24. devon4j Security Model\nThe diagram shows the model of devon4j-security that separates two different aspects:\nThe Identity- and Access-Management is provided by according products and typically already available in the enterprise landscape (e.g. an active directory). It provides a hierarchy of primary access control objects (roles and groups) of a user. An administrator can grant and revoke permissions (indirectly) via this way.\nThe application security defines a hierarchy of secondary access control objects (groups and permissions). This is done by configuration owned by the application (see following section). The &quot;API&quot; is defined by the IDs of the primary access control objects that will be referenced from the Identity- and Access-Management.\nAccess Control Config\nIn your application simply extend AccessControlConfig to configure your access control objects as code and reference it from your use-cases. An example config may look like this:\n@Named\npublic class ApplicationAccessControlConfig extends AccessControlConfig {\npublic static final String APP_ID = &quot;MyApp&quot;;\nprivate static final String PREFIX = APP_ID + &quot;.&quot;;\npublic static final String PERMISSION_FIND_OFFER = PREFIX + &quot;FindOffer&quot;;\npublic static final String PERMISSION_SAVE_OFFER = PREFIX + &quot;SaveOffer&quot;;\npublic static final String PERMISSION_DELETE_OFFER = PREFIX + &quot;DeleteOffer&quot;;\npublic static final String PERMISSION_FIND_PRODUCT = PREFIX + &quot;FindProduct&quot;;\npublic static final String PERMISSION_SAVE_PRODUCT = PREFIX + &quot;SaveProduct&quot;;\npublic static final String PERMISSION_DELETE_PRODUCT = PREFIX + &quot;DeleteProduct&quot;;\npublic static final String GROUP_READ_MASTER_DATA = PREFIX + &quot;ReadMasterData&quot;;\npublic static final String GROUP_MANAGER = PREFIX + &quot;Manager&quot;;\npublic static final String GROUP_ADMIN = PREFIX + &quot;Admin&quot;;\npublic ApplicationAccessControlConfig() {\nsuper();\nAccessControlGroup readMasterData = group(GROUP_READ_MASTER_DATA, PERMISSION_FIND_OFFER, PERMISSION_FIND_PRODUCT);\nAccessControlGroup manager = group(GROUP_MANAGER, readMasterData, PERMISSION_SAVE_OFFER, PERMISSION_SAVE_PRODUCT);\nAccessControlGroup admin = group(GROUP_ADMIN, manager, PERMISSION_DELETE_OFFER, PERMISSION_DELETE_PRODUCT);\n}\n}\nConfiguration on Java Method level\nIn your use-case you can now reference a permission like this:\n@Named\npublic class UcSafeOfferImpl extends ApplicationUc implements UcSafeOffer {\n@Override\n@RolesAllowed(ApplicationAccessControlConfig.PERMISSION_SAVE_OFFER)\npublic OfferEto save(OfferEto offer) { ... }\n...\n}\nCheck Data-Permissions\nSee data permissions\nAccess Control Schema (deprecated)\nThe access-control-schema.xml approach is deprecated. The documentation can still be found in access control schema.\n9.14. Data-permissions\nIn some projects there are demands for permissions and authorization that is dependent on the processed data. E.g. a user may only be allowed to read or write data for a specific region. This is adding some additional complexity to your authorization. If you can avoid this it is always best to keep things simple. However, in various cases this is a requirement. Therefore the following sections give you guidance and patterns how to solve this properly.\n9.14.1. Structuring your data\nFor all your business objects (entities) that have to be secured regarding to data permissions we recommend that you create a separate interface that provides access to the relevant data required to decide about the permission. Here is a simple example:\npublic interface SecurityDataPermissionCountry {\n/**\n* @return the 2-letter ISO code of the country this object is associated with. Users need\n* a data-permission for this country in order to read and write this object.\n*/\nString getCountry();\n}\nNow related business objects (entities) can implement this interface. Often such data-permissions have to be applied to an entire object-hierarchy. For security reasons we recommend that also all child-objects implement this interface. For performance reasons we recommend that the child-objects redundantly store the data-permission properties (such as country in the example above) and this gets simply propagated from the parent, when a child object is created.\n9.14.2. Permissions for processing data\nWhen saving or processing objects with a data-permission, we recommend to provide dedicated methods to verify the permission in an abstract base-class such as AbstractUc and simply call this explicitly from your business code. This makes it easy to understand and debug the code. Here is a simple example:\nprotected void verifyPermission(SecurityDataPermissionCountry entity) throws AccessDeniedException;\nBeware of AOP\nFor simple but cross-cutting data-permissions you may also use AOP. This leads to programming aspects that reflectively scan method arguments and magically decide what to do. Be aware that this quickly gets tricky:\nWhat if multiple of your method arguments have data-permissions (e.g. implement SecurityDataPermission*)?\nWhat if the object to authorize is only provided as reference (e.g. Long or IdRef) and only loaded and processed inside the implementation where the AOP aspect does not apply?\nHow to express advanced data-permissions in annotations?\nWhat we have learned is that annotations like @PreAuthorize from spring-security easily lead to the &quot;programming in string literals&quot; anti-pattern. We strongly discourage to use this anti-pattern. In such case writing your own verifyPermission methods that you manually call in the right places of your business-logic is much better to understand, debug and maintain.\n9.14.3. Permissions for reading data\nWhen it comes to restrictions on the data to read it becomes even more tricky. In the context of a user only entities shall be loaded from the database he is permitted to read. This is simple for loading a single entity (e.g. by its ID) as you can load it and then if not permitted throw an exception to secure your code. But what if the user is performing a search query to find many entities? For performance reasons we should only find data the user is permitted to read and filter all the rest already via the database query. But what if this is not a requirement for a single query but needs to be applied cross-cutting to tons of queries? Therefore we have the following pattern that solves your problem:\nFor each data-permission attribute (or set of such) we create an abstract base entity:\n@MappedSuperclass\n@EntityListeners(PermissionCheckListener.class)\n@FilterDef(name = &quot;country&quot;, parameters = {@ParamDef(name = &quot;countries&quot;, type = &quot;string&quot;)})\n@Filter(name = &quot;country&quot;, condition = &quot;country in (:countries)&quot;)\npublic abstract class SecurityDataPermissionCountryEntity extends ApplicationPersistenceEntity\nimplements SecurityDataPermissionCountry {\nprivate String country;\n@Override\npublic String getCountry() {\nreturn this.country;\n}\npublic void setCountry(String country) {\nthis.country = country;\n}\n}\nThere are some special hibernate annotations @EntityListeners, @FilterDef, and @Filter used here allowing to apply a filter on the country for any (non-native) query performed by hibernate. The entity listener may look like this:\npublic class PermissionCheckListener {\n@PostLoad\npublic void read(SecurityDataPermissionCountryEntity entity) {\nPermissionChecker.getInstance().requireReadPermission(entity);\n}\n@PrePersist\n@PreUpdate\npublic void write(SecurityDataPermissionCountryEntity entity) {\nPermissionChecker.getInstance().requireWritePermission(entity);\n}\n}\nThis will ensure that hibernate implicitly will call these checks for every such entity when it is read from or written to the database. Further to avoid reading entities from the database the user is not permitted to (and ending up with exceptions), we create an AOP aspect that automatically activates the above declared hibernate filter:\n@Named\npublic class PermissionCheckerAdvice implements MethodBeforeAdvice {\n@Inject\nprivate PermissionChecker permissionChecker;\n@PersistenceContext\nprivate EntityManager entityManager;\n@Override\npublic void before(Method method, Object[] args, Object target) {\nCollection&lt;String&gt; permittedCountries = this.permissionChecker.getPermittedCountriesForReading();\nif (permittedCountries != null) { // null is returned for admins that may access all countries\nif (permittedCountries.isEmpty()) {\nthrow new AccessDeniedException(&quot;Not permitted for any country!&quot;);\n}\nSession session = this.entityManager.unwrap(Session.class);\nsession.enableFilter(&quot;country&quot;).setParameterList(&quot;countries&quot;, permittedCountries.toArray());\n}\n}\n}\nFinally to apply this aspect to all Repositories (can easily be changed to DAOs) implement the following advisor:\n@Named\npublic class PermissionCheckerAdvisor implements PointcutAdvisor, Pointcut, ClassFilter, MethodMatcher {\n@Inject\nprivate PermissionCheckerAdvice advice;\n@Override\npublic Advice getAdvice() {\nreturn this.advice;\n}\n@Override\npublic boolean isPerInstance() {\nreturn false;\n}\n@Override\npublic Pointcut getPointcut() {\nreturn this;\n}\n@Override\npublic ClassFilter getClassFilter() {\nreturn this;\n}\n@Override\npublic MethodMatcher getMethodMatcher() {\nreturn this;\n}\n@Override\npublic boolean matches(Method method, Class&lt;?&gt; targetClass) {\nreturn true; // apply to all methods\n}\n@Override\npublic boolean isRuntime() {\nreturn false;\n}\n@Override\npublic boolean matches(Method method, Class&lt;?&gt; targetClass, Object... args) {\nthrow new IllegalStateException(&quot;isRuntime()==false&quot;);\n}\n@Override\npublic boolean matches(Class&lt;?&gt; clazz) {\n// when using DAOs simply change to some class like ApplicationDao\nreturn DefaultRepository.class.isAssignableFrom(clazz);\n}\n}\n9.14.4. Managing and granting the data-permissions\nFollowing our authorization guide we can simply create a permission for each country. We might simply reserve a prefix (as virtual &#xAB;app-id&#xBB;) for each data-permission to allow granting data-permissions to end-users across all applications of the IT landscape. In our example we could create access controls country.DE, country.US, country.ES, etc. and assign those to the users. The method permissionChecker.getPermittedCountriesForReading() would then scan for these access controls and only return the 2-letter country code from it.\nCaution\nBefore you make your decisions how to design your access controls please clarify the following questions:\nDo you need to separate data-permissions independent of the functional permissions? E.g. may it be required to express that a user can read data from the countries ES and PL but is only permitted to modify data from PL? In such case a single assignment of &quot;country-permissions&quot; to users is insufficient.\nDo you want to grant data-permissions individually for each application (higher flexibility and complexity) or for the entire application landscape (simplicity, better maintenance for administrators)? In case of the first approach you would rather have access controls like app1.country.GB and app2.country.GB.\nDo your data-permissions depend on objects that can be created dynamically inside your application?\nIf you want to grant data-permissions on other business objects (entities), how do you want to reference them (primary keys, business keys, etc.)? What reference is most stable? Which is most readable?\n9.15. Validation\nValidation is about checking syntax and semantics of input data. Invalid data is rejected by the application.\nTherefore validation is required in multiple places of an application. E.g. the GUI will do validation for usability reasons to assist the user, early feedback and to prevent unnecessary server requests.\nOn the server-side validation has to be done for consistency and security.\nIn general we distinguish these forms of validation:\nstateless validation will produce the same result for given input at any time (for the same code/release).\nstateful validation is dependent on other states and can consider the same input data as valid in once case and as invalid in another.\n9.15.1. Stateless Validation\nFor regular, stateless validation we use the JSR303 standard that is also called bean validation (BV).\nDetails can be found in the specification.\nAs implementation we recommend hibernate-validator.\nExample\nA description of how to enable BV can be found in the relevant Spring documentation. For a quick summary follow these steps:\nMake sure that hibernate-validator is located in the classpath by adding a dependency to the pom.xml.\n&lt;dependency&gt;\n&lt;groupId&gt;org.hibernate&lt;/groupId&gt;\n&lt;artifactId&gt;hibernate-validator&lt;/artifactId&gt;\n&lt;/dependency&gt;\nAdd the @Validated annotation to the implementation (spring bean) to be validated. The standard use case is to annotate the logic layer implementation, i.e. the use case implementation or component facade in case of simple logic layer pattern. Thus, the validation will be executed for service requests as well as batch processing. For methods to validate go to their declaration and add constraint annotations to the method parameters.\n@Valid annotation to the arguments to validate (if that class itself is annotated with constraints to check).\n@NotNull for required arguments.\nOther constraints (e.g. @Size) for generic arguments (e.g. of type String or Integer). However, consider to create custom datatypes and avoid adding too much validation logic (especially redundant in multiple places).\n.BookingmanagementRestServiceImpl.java\n@Validated\npublic class BookingmanagementRestServiceImpl implements BookingmanagementRestService {\n...\npublic BookingEto saveBooking(@Valid BookingCto booking) {\n...\nFinally add appropriate validation constraint annotations to the fields of the ETO class.\n.BookingCto.java\n@Valid\nprivate BookingEto booking;\nListing 7. BookingEto.java\n@NotNull\n@Future\nprivate Timestamp bookingDate;\nA list with all bean validation constraint annotations available for hibernate-validator can be found here. In addition it is possible to configure custom constraints. Therefore it is necessary to implement a annotation and a corresponding validator. A description can also be found in the Spring documentation or with more details in the hibernate documentation.\nNote\nBean Validation in Wildfly &gt;v8: Wildfly v8 is the first version of Wildfly implementing the JEE7 specification. It comes with bean validation based on hibernate-validator out of the box. In case someone is running Spring in Wildfly for whatever reasons, the spring based annotation @Validated would duplicate bean validation at runtime and thus should be omitted.\nGUI-Integration\nTODO\nCross-Field Validation\nBV has poor support for this. Best practice is to create and use beans for ranges, etc. that solve this. A bean for a range could look like so:\npublic class Range&lt;V extends Comparable&lt;V&gt;&gt; {\nprivate V min;\nprivate V max;\npublic Range(V min, V max) {\nsuper();\nif ((min != null) &amp;&amp; (max != null)) {\nint delta = min.compareTo(max);\nif (delta &gt; 0) {\nthrow new ValueOutOfRangeException(null, min, min, max);\n}\n}\nthis.min = min;\nthis.max = max;\n}\npublic V getMin() ...\npublic V getMax() ...\n9.15.2. Stateful Validation\nFor complex and stateful business validations we do not use BV (possible with groups and context, etc.) but follow KISS and just implement this on the server in a straight forward manner.\nAn example is the deletion of a table in the example application. Here the state of the table must be checked first:\nBookingmanagementImpl.java\nprivate void sendConfirmationEmails(BookingEntity booking) {\nif (!booking.getInvitedGuests().isEmpty()) {\nfor (InvitedGuestEntity guest : booking.getInvitedGuests()) {\nsendInviteEmailToGuest(guest, booking);\n}\n}\nsendConfirmationEmailToHost(booking);\n}\nImplementing this small check with BV would be a lot more effort.\n9.16. Aspect Oriented Programming (AOP)\nAOP is a powerful feature for cross-cutting concerns. However, if used extensive and for the wrong things an application can get unmaintainable. Therefore we give you the best practices where and how to use AOP properly.\n9.16.1. AOP Key Principles\nWe follow these principles:\nWe use spring AOP based on dynamic proxies (and fallback to cglib).\nWe avoid AspectJ and other mighty and complex AOP frameworks whenever possible\nWe only use AOP where we consider it as necessary (see below).\n9.16.2. AOP Usage\nWe recommend to use AOP with care but we consider it established for the following cross cutting concerns:\nTransaction-Handling\nAuthorization\nValidation\nTrace-Logging (for testing and debugging)\nException facades for services but only if no other solution is possible (use alternatives such as JAX-RS provider instead).\n9.16.3. AOP Debugging\nWhen using AOP with dynamic proxies the debugging of your code can get nasty. As you can see by the red boxes in the call stack in the debugger there is a lot of magic happening while you often just want to step directly into the implementation skipping all the AOP clutter. When using Eclipse this can easily be archived by enabling step filters. Therefore you have to enable the feature in the Eclipse tool bar (highlighted in read).\nIn order to properly make this work you need to ensure that the step filters are properly configured:\nEnsure you have at least the following step-filters configured and active:\nch.qos.logback.*\ncom.devonfw.module.security.*\njava.lang.reflect.*\njava.security.*\njavax.persistence.*\norg.apache.commons.logging.*\norg.apache.cxf.jaxrs.client.*\norg.apache.tomcat.*\norg.h2.*\norg.springframework.*\n9.17. Exception Handling\n9.17.1. Exception Principles\nFor exceptions we follow these principles:\nWe only use exceptions for exceptional situations and not for programming control flows, etc. Creating an exception in Java is expensive and hence you should not do it just for testing if something is present, valid or permitted. In the latter case design your API to return this as a regular result.\nWe use unchecked exceptions (RuntimeException)\nWe distinguish internal exceptions and user exceptions:\nInternal exceptions have technical reasons. For unexpected and exotic situations it is sufficient to throw existing exceptions such as IllegalStateException. For common scenarios a own exception class is reasonable.\nUser exceptions contain a message explaining the problem for end users. Therefore we always define our own exception classes with a clear, brief but detailed message.\nOur own exceptions derive from an exception base class supporting\nunique ID per instance\nError code per class\nmessage templating (see I18N)\ndistinguish between user exceptions and internal exceptions\nAll this is offered by mmm-util-core that we propose as solution.\n9.17.2. Exception Example\nHere is an exception class from our sample application:\npublic class IllegalEntityStateException extends ApplicationBusinessException {\nprivate static final long serialVersionUID = 1L;\npublic IllegalEntityStateException(Object entity, Object state) {\nthis((Throwable) null, entity, state);\n}\npublic IllegalEntityStateException(Object entity, Object currentState, Object newState) {\nthis(null, entity, currentState, newState);\n}\npublic IllegalEntityStateException(Throwable cause, Object entity, Object state) {\nsuper(cause, createBundle(NlsBundleApplicationRoot.class).errorIllegalEntityState(entity, state));\n}\npublic IllegalEntityStateException(Throwable cause, Object entity, Object currentState, Object newState) {\nsuper(cause, createBundle(NlsBundleApplicationRoot.class).errorIllegalEntityStateChange(entity, currentState,\nnewState));\n}\n}\nThe message templates are defined in the interface NlsBundleRestaurantRoot as following:\npublic interface NlsBundleApplicationRoot extends NlsBundle {\n@NlsBundleMessage(&quot;The entity {entity} is in state {state}!&quot;)\nNlsMessage errorIllegalEntityState(@Named(&quot;entity&quot;) Object entity, @Named(&quot;state&quot;) Object state);\n@NlsBundleMessage(&quot;The entity {entity} in state {currentState} can not be changed to state {newState}!&quot;)\nNlsMessage errorIllegalEntityStateChange(@Named(&quot;entity&quot;) Object entity, @Named(&quot;currentState&quot;) Object currentState,\n@Named(&quot;newState&quot;) Object newState);\n@NlsBundleMessage(&quot;The property {property} of object {object} can not be changed!&quot;)\nNlsMessage errorIllegalPropertyChange(@Named(&quot;object&quot;) Object object, @Named(&quot;property&quot;) Object property);\n@NlsBundleMessage(&quot;There is currently no user logged in&quot;)\nNlsMessage errorNoActiveUser();\n9.17.3. Handling Exceptions\nFor catching and handling exceptions we follow these rules:\nWe do not catch exceptions just to wrap or to re-throw them.\nIf we catch an exception and throw a new one, we always have to provide the original exception as cause to the constructor of the new exception.\nAt the entry points of the application (e.g. a service operation) we have to catch and handle all throwables. This is done via the exception-facade-pattern via an explicit facade or aspect. The devon4j already provides ready-to-use implementations for this such as RestServiceExceptionFacade. The exception facade has to&#x2026;&#x200B;\nlog all errors (user errors on info and technical errors on error level)\nconvert the error to a result appropriable for the client and secure for Sensitive Data Exposure. Especially for security exceptions only a generic security error code or message may be revealed but the details shall only be logged but not be exposed to the client. All internal exceptions are converted to a generic error with a message like:\nAn unexpected technical error has occurred. We apologize any inconvenience. Please try again later.\n9.17.4. Common Errors\nThe following errors may occur in any devon application:\nTable 12. Common Exceptions\nCode\nMessage\nLink\nTechnicalError\nAn unexpected error has occurred! We apologize any inconvenience. Please try again later.\nTechnicalErrorUserException.java\nServiceInvoke\n&#xAB;original message of the cause&#xBB;\nServiceInvocationFailedException.java\n9.18. Internationalization\nInternationalization (I18N) is about writing code independent from locale-specific information.\nFor I18N of text messages we are suggesting\nmmm native-language-support.\nIn devonfw we have developed a solution to manage text internationalization. devonfw solution comes into two aspects:\nBind locale information to the user.\nGet the messages in the current user locale.\n9.18.1. Binding locale information to the user\nWe have defined two different points to bind locale information to user, depending on user is authenticated or not.\nUser not authenticated: devonfw intercepts unsecured request and extract locale from it. At first, we try to extract a language parameter from the request and if it is not possible, we extract locale from &#xC0;ccept-language` header.\nUser authenticated. During login process, applications developers are responsible to fill language parameter in the UserProfile class. This language parameter could be obtain from DB, LDAP, request, etc. In devonfw sample we get the locale information from database.\nThis image shows the entire process:\n9.18.2. Getting internationalizated messages\ndevonfw has a bean that manage i18n message resolution, the ApplicationLocaleResolver. This bean is responsible to get the current user and extract locale information from it and read the correct properties file to get the message.\nThe i18n properties file must be called ApplicationMessages_la_CO.properties where la=language and CO=country. This is an example of a i18n properties file for English language to translate devonfw sample user roles:\nApplicationMessages_en_US.properties\nwaiter=Waiter\nchief=Chief\ncook=Cook\nbarkeeper=Barkeeper\nYou should define an ApplicationMessages_la_CO.properties file for every language that your application needs.\nApplicationLocaleResolver bean is injected in AbstractComponentFacade class so you have available this bean in logic layer so you only need to put this code to get an internationalized message:\nString msg = getApplicationLocaleResolver().getMessage(&quot;mymessage&quot;);\n9.19. XML\nXML (Extensible Markup Language) is a W3C standard format for structured information. It has a large eco-system of additional standards and tools.\nIn Java there are many different APIs and frameworks for accessing, producing and processing XML. For the devonfw we recommend to use JAXB for mapping Java objects to XML and vice-versa. Further there is the popular DOM API for reading and writing smaller XML documents directly. When processing large XML documents StAX is the right choice.\n9.19.1. JAXB\nWe use JAXB to serialize Java objects to XML or vice-versa.\nJAXB and Inheritance\nTODO @XmlSeeAlso\nhttp://stackoverflow.com/questions/7499735/jaxb-how-to-create-xml-from-polymorphic-classes\nJAXB Custom Mapping\nIn order to map custom datatypes or other types that do not follow the Java bean conventions, you need to define a custom mapping. If you create dedicated objects dedicated for the XML mapping you can easily avoid such situations. When this is not suitable follow these instructions to define the mapping: TODO\nhttps://weblogs.java.net/blog/kohsuke/archive/2005/09/using_jaxb_20s.html\n9.20. JSON\nJSON (JavaScript Object Notation) is a popular format to represent and exchange data especially for modern web-clients. For mapping Java objects to JSON and vice-versa there is no official standard API. We use the established and powerful open-source solution Jackson.\nDue to problems with the wiki of fasterxml you should try this alternative link: Jackson/AltLink.\n9.20.1. Configure JSON Mapping\nIn order to avoid polluting business objects with proprietary Jackson annotations (e.g. @JsonTypeInfo, @JsonSubTypes, @JsonProperty) we propose to create a separate configuration class. Every devonfw application (sample or any app created from our app-template) therefore has a class called ApplicationObjectMapperFactory that extends ObjectMapperFactory from the devon4j-rest module. It looks like this:\n@Named(&quot;ApplicationObjectMapperFactory&quot;)\npublic class ApplicationObjectMapperFactory extends ObjectMapperFactory {\npublic RestaurantObjectMapperFactory() {\nsuper();\n// JSON configuration code goes here\n}\n}\n9.20.2. JSON and Inheritance\nIf you are using inheritance for your objects mapped to JSON then polymorphism can not be supported out-of-the box. So in general avoid polymorphic objects in JSON mapping. However, this is not always possible.\nHave a look at the following example from our sample application:\nFigure 25. Transfer-Objects using Inheritance\nNow assume you have a REST service operation as Java method that takes a ProductEto as argument. As this is an abstract class the server needs to know the actual sub-class to instantiate.\nWe typically do not want to specify the classname in the JSON as this should be an implementation detail and not part of the public JSON format (e.g. in case of a service interface). Therefore we use a symbolic name for each polymorphic subtype that is provided as virtual attribute @type within the JSON data of the object:\n{ &quot;@type&quot;: &quot;Drink&quot;, ... }\nTherefore you add configuration code to the constructor of ApplicationObjectMapperFactory. Here you can see an example from the sample application:\nsetBaseClasses(ProductEto.class);\naddSubtypes(new NamedType(MealEto.class, &quot;Meal&quot;), new NamedType(DrinkEto.class, &quot;Drink&quot;),\nnew NamedType(SideDishEto.class, &quot;SideDish&quot;));\nWe use setBaseClasses to register all top-level classes of polymorphic objects. Further we declare all concrete polymorphic sub-classes together with their symbolic name for the JSON format via addSubtypes.\n9.20.3. Custom Mapping\nIn order to map custom datatypes or other types that do not follow the Java bean conventions, you need to define a custom mapping. If you create objects dedicated for the JSON mapping you can easily avoid such situations. When this is not suitable follow these instructions to define the mapping:\nAs an example, the use of JSR354 (javax.money) is appreciated in order to process monetary amounts properly. However, without custom mapping, the default mapping of Jackson will produce the following JSON for a MonetaryAmount:\n&quot;currency&quot;: {&quot;defaultFractionDigits&quot;:2, &quot;numericCode&quot;:978, &quot;currencyCode&quot;:&quot;EUR&quot;},\n&quot;monetaryContext&quot;: {...},\n&quot;number&quot;:6.99,\n&quot;factory&quot;: {...}\nAs clearly can be seen, the JSON contains too much information and reveals implementation secrets that do not belong here. Instead the JSON output expected and desired would be:\n&quot;currency&quot;:&quot;EUR&quot;,&quot;amount&quot;:&quot;6.99&quot;\nEven worse, when we send the JSON data to the server, Jackson will see that MonetaryAmount is an interface and does not know how to instantiate it so the request will fail.\nTherefore we need a customized Serializer.\nWe implement MonetaryAmountJsonSerializer to define how a MonetaryAmount is serialized to JSON:\npublic final class MonetaryAmountJsonSerializer extends JsonSerializer&lt;MonetaryAmount&gt; {\npublic static final String NUMBER = &quot;amount&quot;;\npublic static final String CURRENCY = &quot;currency&quot;;\npublic void serialize(MonetaryAmount value, JsonGenerator jgen, SerializerProvider provider) throws ... {\nif (value != null) {\njgen.writeStartObject();\njgen.writeFieldName(MonetaryAmountJsonSerializer.CURRENCY);\njgen.writeString(value.getCurrency().getCurrencyCode());\njgen.writeFieldName(MonetaryAmountJsonSerializer.NUMBER);\njgen.writeString(value.getNumber().toString());\njgen.writeEndObject();\n}\n}\nFor composite datatypes it is important to wrap the info as an object (writeStartObject() and writeEndObject()). MonetaryAmount provides the information we need by the getCurrency() and getNumber(). So that we can easily write them into the JSON data.\nNext, we implement MonetaryAmountJsonDeserializer to define how a MonetaryAmount is deserialized back as Java object from JSON:\npublic final class MonetaryAmountJsonDeserializer extends AbstractJsonDeserializer&lt;MonetaryAmount&gt; {\nprotected MonetaryAmount deserializeNode(JsonNode node) {\nBigDecimal number = getRequiredValue(node, MonetaryAmountJsonSerializer.NUMBER, BigDecimal.class);\nString currencyCode = getRequiredValue(node, MonetaryAmountJsonSerializer.CURRENCY, String.class);\nMonetaryAmount monetaryAmount =\nMonetaryAmounts.getAmountFactory().setNumber(number).setCurrency(currencyCode).create();\nreturn monetaryAmount;\n}\n}\nFor composite datatypes we extend from AbstractJsonDeserializer as this makes our task easier. So we already get a JsonNode with the parsed payload of our datatype. Based on this API it is easy to retrieve individual fields from the payload without taking care of their order, etc.\nAbstractJsonDeserializer also provides methods such as getRequiredValue to read required fields and get them converted to the desired basis datatype. So we can easily read the amount and currency and construct an instance of MonetaryAmount via the official factory API.\nFinally we need to register our custom (de)serializers with the following configuration code in the constructor of ApplicationObjectMapperFactory:+\nSimpleModule module = getExtensionModule();\nmodule.addDeserializer(MonetaryAmount.class, new MonetaryAmountJsonDeserializer());\nmodule.addSerializer(MonetaryAmount.class, new MonetaryAmountJsonSerializer());\nNow we can read and write MonetaryAmount from and to JSON as expected.\n9.21. REST\nREST (REpresentational State Transfer) is an inter-operable protocol for services that is more lightweight than SOAP.\nHowever, it is no real standard and can cause confusion. Therefore we define best practices here to guide you.\nATTENTION:\nREST and RESTful often implies very strict and specific rules and conventions. However different people will often have different opinions of such rules. We learned that this leads to &quot;religious discussions&quot; (starting from PUT vs. POST and IDs in path vs. payload up to Hypermedia and HATEOAS). These &quot;religious discussions&quot; waste a lot of time and money without adding real value in case of common business applications (if you publish your API on the internet to billions of users this is a different story). Therefore we give best practices that lead to simple, easy and pragmatic &quot;HTTP APIs&quot; (to avoid the term &quot;REST services&quot; and end &quot;religious discussions&quot;). Please also note that we do not want to assault anybody nor force anyone to follow our guidelines. Please read the following best practices carefully and be aware that they might slightly differ from what your first hit on the web will say about REST (see e.g. RESTful cookbook).\n9.21.1. URLs\nURLs are not case sensitive. Hence, we follow the best practice to use only lower-case-letters-with-hyphen-to-separate-words.\nFor operations in REST we distinguish the following types of URLs:\nA collection URL is build from the rest service URL by appending the name of a collection. This is typically the name of an entity. Such URI identifies the entire collection of all elements of this type. Example: https://mydomain.com/myapp/services/rest/mycomponent/v1/myentity\nAn element URL is build from a collection URL by appending an element ID. It identifies a single element (entity) within the collection. Example: https://mydomain.com/myapp/services/rest/mycomponent/v1/myentity/42\nA search URL is build from a collection URL by appending the segment search. The search criteria is send as POST. Example: https://mydomain.com/myapp/services/rest/mycomponent/v1/myentity/search\nThis fits perfect for CRUD operations. For business operations (processing, calculation, etc.) we simply create a collection URL with the name of the business operation instead of the entity name (use a clear naming convention to avoid collisions). Then we can POST the input for the business operation and get the result back.\nIf you want to provide an entity with a different structure do not append further details to an element URL but create a separate collection URL as base.\nSo use https://mydomain.com/myapp/services/rest/mycomponent/v1/myentity-with-details/42 instead of https://mydomain.com/myapp/services/rest/mycomponent/v1/myentity/42/with-details.\nFor offering a CTO simply append -cto to the collection URL (e.g. &#x2026;&#x200B;/myentity-cto/).\n9.21.2. HTTP Methods\nWhile REST was designed as a pragmatical approach it sometimes leads to &quot;religious discussions&quot; e.g. about using PUT vs. POST (see ATTENTION notice above).\nAs the devonfw has a string focus on usual business applications it proposes a more &quot;pragmatic&quot; approach to REST services.\nOn the next table we compare the main differences between the &quot;canonical&quot; REST approach (or RESTful) and the devonfw proposal.\nTable 13. Usage of HTTP methods\nHTTP Method\nRESTful Meaning\ndevonfw\nGET\nRead single element.\nSearch on an entity (with parametrized url)\nRead a single element.\nPUT\nReplace entity data.\nReplace entire collection (typically not supported)\nNot used\nPOST\nCreate a new element in the collection\nCreate or update an element in the collection.\nSearch on an entity (parametrized post body)\nBulk deletion.\nDELETE\nDelete an entity.\nDelete an entire collection (typically not supported)\nDelete an entity.\nDelete an entire collection (typically not supported)\nPlease consider these guidelines and rationales:\nWe use POST on the collection URL to save an entity (create if no ID provided in payload otherwise update). This avoids pointless discussions in distinctions between PUT and POST and what to do if a create contains an ID in the payload or if an update is missing the ID property or contains a different ID in payload than in URL.\nHence, we do NOT use PUT but always use POST for write operations. As we always have a technical ID for each entity, we can simply distinguish create and update by the presence of the ID property.\nPlease also note that for (large) bulk deletions you may be forced to used POST instead of DELETE as according to the HTTP standard DELETE must not have payload and URLs are limited in length.\n9.21.3. HTTP Status Codes\nFurther we define how to use the HTTP status codes for REST services properly. In general the 4xx codes correspond to an error on the client side and the 5xx codes to an error on the server side.\nTable 14. Usage of HTTP status codes\nHTTP Code\nMeaning\nResponse\nComment\n200\nOK\nrequested result\nResult of successful GET\n204\nNo Content\nnone\nResult of successful POST, DELETE, or PUT (void return)\n400\nBad Request\nerror details\nThe HTTP request is invalid (parse error, validation failed)\n401\nUnauthorized\nnone (security)\nAuthentication failed\n403\nForbidden\nnone (security)\nAuthorization failed\n404\nNot found\nnone\nEither the service URL is wrong or the requested resource does not exist\n500\nServer Error\nerror code, UUID\nInternal server error occurred (used for all technical exceptions)\n9.21.4. Metadata\ndevonfw has support for the following metadata in REST service invocations:\nName\nDescription\nFurther information\nX-Correlation-Id\nHTTP header for a correlation ID that is a unique identifier to associate different requests belonging to the same session / action\nLogging guide\nValidation errors\nStandardized format for a service to communicate validation errors to the client\nServer-side validation is documented in the Validation guide.\nThe protocol to communicate these validation errors to the client is worked on at https://github.com/oasp/oasp4j/issues/218\nPagination\nStandardized format for a service to offer paginated access to a list of entities\nServer-side support for pagination is documented in the Repository Guide.\n9.21.5. JAX-RS\nFor implementing REST services we use the JAX-RS standard. As an implementation we recommend CXF. For JSON bindings we use Jackson while XML binding works out-of-the-box with JAXB.\nTo implement a service you write an interface with JAX-RS annotations for the API and a regular implementation class annotated with @Named to make it a spring-bean. Here is a simple example:\ncom.devonfw.application.mtsj.dishmanagement.service.impl.rest\n@Path(&quot;/imagemanagement/v1&quot;)\n@Consumes(MediaType.APPLICATION_JSON)\n@Produces(MediaType.APPLICATION_JSON)\npublic interface ImagemanagementRestService {\n@GET\n@Path(&quot;/image/{id}/&quot;)\npublic ImageEto getImage(@PathParam(&quot;id&quot;) long id);\n}\n@Named(&quot;ImagemanagementRestService&quot;)\npublic class ImagemanagementRestServiceImpl implements ImagemanagementRestService {\n@Inject\nprivate Imagemanagement imagemanagement;\n@Override\npublic ImageEto getImage(long id) {\nreturn this.imagemanagement.findImage(id);\n}\n}\nHere we can see a REST service for the business component imagemanagement. The method getImage can be accessed via HTTP GET (see @GET) under the URL path imagemanagement/image/{id} (see @Path annotations) where {id} is the ID of the requested table and will be extracted from the URL and provided as parameter id to the method getImage. It will return its result (ImageEto) as JSON (see @Produces - should already be defined as defaults in RestService marker interface). As you can see it delegates to the logic component imagemanagement that contains the actual business logic while the service itself only exposes this logic via HTTP. The REST service implementation is a regular CDI bean that can use dependency injection.\nThe separation of the API as a Java interface allows to use it for service client calls.\nNote\nWith JAX-RS it is important to make sure that each service method is annotated with the proper HTTP method (@GET,@POST,etc.) to avoid unnecessary debugging. So you should take care not to forget to specify one of these annotations.\nJAX-RS Configuration\nStarting from CXF 3.0.0 it is possible to enable the auto-discovery of JAX-RS roots.\nWhen the jaxrs server is instantiated all the scanned root and provider beans (beans annotated with javax.ws.rs.Path and javax.ws.rs.ext.Provider) are configured.\n9.21.6. REST Exception Handling\nFor exceptions a service needs to have an exception fa&#xE7;ade that catches all exceptions and handles them by writing proper log messages and mapping them to a HTTP response with an according HTTP status code. Therefore the devonfw provides a generic solution via RestServiceExceptionFacade. You need to follow the exception guide so that it works out of the box because the fa&#xE7;ade needs to be able to distinguish between business and technical exceptions.\nNow your service may throw exceptions but the fa&#xE7;ade with automatically handle them for you.\n9.21.7. Recommendations for REST requests and responses\nThe devonfw proposes, for simplicity, a deviation from the common REST pattern:\nUsing POST for updates (instead of PUT)\nUsing the payload for addressing resources on POST (instead of identifier on the URL)\nUsing parametrized POST for searches\nThis use of REST will lead to simpler code both on client and on server. We discuss this use on the next points.\nThe following table specifies how to use the HTTP methods (verbs) for collection and element URIs properly (see wikipedia).\nUnparameterized loading of a single resource\nHTTP Method: GET\nURL example: /products/123\nFor loading of a single resource, embed the identifier of the resource in the URL (for example /products/123).\nThe response contains the resource in JSON format, using a JSON object at the top-level, for example:\n{\n&quot;name&quot;: &quot;Steak&quot;,\n&quot;color&quot;: &quot;brown&quot;\n}\nUnparameterized loading of a collection of resources\nHTTP Method: GET\nURL example: /products\nFor loading of a collection of resources, make sure that the size of the collection can never exceed a reasonable maximum size. For parameterized loading (searching, pagination), see below.\nThe response contains the collection in JSON format, using a JSON object at the top-level, and the actual collection underneath a result key, for example:\n{\n&quot;result&quot;: [\n{\n&quot;name&quot;: &quot;Steak&quot;,\n&quot;color&quot;: &quot;brown&quot;\n},\n{\n&quot;name&quot;: &quot;Broccoli&quot;,\n&quot;color&quot;: &quot;green&quot;\n}\n]\n}\nSaving a resource\nHTTP Method: POST\nURL example: /products\nThe resource will be passed via JSON in the request body. If updating an existing resource, include the resource&#x2019;s identifier in the JSON and not in the URL, in order to avoid ambiguity.\nIf saving was successful, an empty HTTP 204 response is generated.\nIf saving was unsuccessful, refer below for the format to return errors to the client.\nParameterized loading of a resource\nHTTP Method: POST\nURL example: /products/search\nIn order to differentiate from an unparameterized load, a special subpath (for example search) is introduced. The parameters are passed via JSON in the request body. An example of a simple, paginated search would be:\n{\n&quot;status&quot;: &quot;OPEN&quot;,\n&quot;pagination&quot;: {\n&quot;page&quot;: 2,\n&quot;size&quot;: 25\n}\n}\nThe response contains the requested page of the collection in JSON format, using a JSON object at the top-level, the actual page underneath a result key, and additional pagination information underneath a pagination key, for example:\n{\n&quot;pagination&quot;: {\n&quot;page&quot;: 2,\n&quot;size&quot;: 25,\n&quot;total&quot;: null\n},\n&quot;result&quot;: [\n{\n&quot;name&quot;: &quot;Steak&quot;,\n&quot;color&quot;: &quot;brown&quot;\n},\n{\n&quot;name&quot;: &quot;Broccoli&quot;,\n&quot;color&quot;: &quot;green&quot;\n}\n]\n}\nCompare the code needed on server side to accept this request:\ncom.devonfw.application.mtsj.dishmanagement.service.api.rest\n@Path(&quot;/category/search&quot;)\n@POST\npublic PaginatedListTo&lt;CategoryEto&gt; findCategorysByPost(CategorySearchCriteriaTo searchCriteriaTo) {\nreturn this.dishmanagement.findCategoryEtos(searchCriteriaTo);\n}\nWith the equivalent code required if doing it the RESTful way by issuing a GET request:\n@Path(&quot;/category/search&quot;)\n@POST @Path(&quot;/order&quot;)\n@GET\npublic PaginatedListTo&lt;CategoryEto&gt; findCategorysByPost( @Context UriInfo info) {\nRequestParameters parameters = RequestParameters.fromQuery(info);\nCategorySearchCriteriaTo criteria = new CategorySearchCriteriaTo();\ncriteria.setName(parameters.get(&quot;name&quot;, Long.class, false));\ncriteria.setDescription(parameters.get(&quot;description&quot;, OrderState.class, false));\ncriteria.setShowOrder(parameters.get(&quot;showOrder&quot;, OrderState.class, false));\nreturn this.dishmanagement.findCategoryEtos(criteria);\n}\nPagination details\nThe client can choose to request a count of the total size of the collection, for example to calculate the total number of available pages. It does so, by specifying the pagination.total property with a value of true.\nThe service is free to honour this request. If it chooses to do so, it returns the total count as the pagination.total property in the response.\nDeletion of a resource\nHTTP Method: DELETE\nURL example: /products/123\nFor deletion of a single resource, embed the identifier of the resource in the URL (for example /products/123).\nError results\nThe general format for returning an error to the client is as follows:\n{\n&quot;message&quot;: &quot;A human-readable message describing the error&quot;,\n&quot;code&quot;: &quot;A code identifying the concrete error&quot;,\n&quot;uuid&quot;: &quot;An identifier (generally the correlation id) to help identify corresponding requests in logs&quot;\n}\nIf the error is caused by a failed validation of the entity, the above format is extended to also include the list of individual validation errors:\n{\n&quot;message&quot;: &quot;A human-readable message describing the error&quot;,\n&quot;code&quot;: &quot;A code identifying the concrete error&quot;,\n&quot;uuid&quot;: &quot;An identifier (generally the correlation id) to help identify corresponding requests in logs&quot;,\n&quot;errors&quot;: {\n&quot;property failing validation&quot;: [\n&quot;First error message on this property&quot;,\n&quot;Second error message on this property&quot;\n],\n// ....\n}\n}\n9.21.8. REST Media Types\nThe payload of a REST service can be in any format as REST by itself does not specify this. The most established ones that the devonfw recommends are XML and JSON. Follow these links for further details and guidance how to use them properly. JAX-RS and CXF properly support these formats (MediaType.APPLICATION_JSON and MediaType.APPLICATION_XML can be specified for @Produces or @Consumes). Try to decide for a single format for all services if possible and NEVER mix different formats in a service.\n9.21.9. REST Testing\nFor testing REST services in general consult the testing guide.\nFor manual testing REST services there are browser plugins:\nFirefox: httprequester (or poster)\nChrome: postman (advanced-rest-client)\n9.21.10. Security\nYour services are the major entry point to your application. Hence security considerations are important here.\nCSRF\nA common security threat is CSRF for REST services. Therefore all REST operations that are performing modifications (PUT, POST, DELETE, etc. - all except GET) have to be secured against CSRF attacks. In devon4j we are using spring-security that already solves CSRF token generation and verification. The integration is part of the application template as well as the sample-application.\nFor testing in development environment the CSRF protection can be disabled using the JVM option -DCsrfDisabled=true when starting the application.\nJSON top-level arrays\nOWASP suggests to prevent returning JSON arrays at the top-level, to prevent attacks (see https://www.owasp.org/index.php/OWASP_AJAX_Security_Guidelines). However, no rationale is given at OWASP. We digged deep and found anatomy-of-a-subtle-json-vulnerability. To sum it up the attack is many years old and does not work in any recent or relevant browser. Hence it is fine to use arrays as top-level result in a JSON REST service (means you can return List&lt;Foo&gt; in a Java JAX-RS service).\n9.22. SOAP\nSOAP is a common protocol for services that is rather complex and heavy. It allows to build inter-operable and well specified services (see WSDL). SOAP is transport neutral what is not only an advantage. We strongly recommend to use HTTPS transport and ignore additional complex standards like WS-Security and use established HTTP-Standards such as RFC2617 (and RFC5280).\n9.22.1. JAX-WS\nFor building web-services with Java we use the JAX-WS standard.\nThere are two approaches:\ncode first\ncontract first\nHere is an example in case you define a code-first service.\nWeb-Service Interface\nWe define a regular interface to define the API of the service and annotate it with JAX-WS annotations:\n@WebService\npublic interface TablemanagmentWebService {\n@WebMethod\n@WebResult(name = &quot;message&quot;)\nTableEto getTable(@WebParam(name = &quot;id&quot;) String id);\n}\nWeb-Service Implementation\nAnd here is a simple implementation of the service:\n@Named\n@WebService(endpointInterface = &quot;com.devonfw.application.mtsj.tablemanagement.service.api.ws.TablemanagmentWebService&quot;)\npublic class TablemanagementWebServiceImpl implements TablemanagmentWebService {\nprivate Tablemanagement tableManagement;\n@Override\npublic TableEto getTable(String id) {\nreturn this.tableManagement.findTable(id);\n}\n9.22.2. SOAP Custom Mapping\nIn order to map custom datatypes or other types that do not follow the Java bean conventions, you need to write adapters for JAXB (see XML).\n9.22.3. SOAP Testing\nFor testing SOAP services in general consult the testing guide.\nFor testing SOAP services manually we strongly recommend SoapUI.\n9.23. Service Client\nThis guide is about consuming (calling) services from other applications (micro-services). For providing services see the Service-Layer Guide. Services can be consumed in the client or the server. As the client is typically not written in Java you should consult the according guide for your client technology. In case you want to call a service within your Java code this guide is the right place to get help.\n9.23.1. Motivation\nVarious solutions already exist for calling services such as RestTemplate from spring or the JAX-RS client API. Further each and every service framework offers its own API as well. These solutions might be suitable for very small and simple projects (with one or two such invocations). However, with the trend of microservices the invocation of a service becomes a very common use-case that occurs all over the place. Have a look at the following features to get an idea why you want to use the solution offered here.\n9.23.2. Requirements\nYou need to add (at least one of) these dependencies to your application:\n&lt;!-- Starter for consuming REST services --&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.devonfw.java.starters&lt;/groupId&gt;\n&lt;artifactId&gt;devon4j-starter-cxf-client-rest&lt;/artifactId&gt;\n&lt;/dependency&gt;\n&lt;!-- Starter for consuming SOAP services --&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;com.devonfw.java.starters&lt;/groupId&gt;\n&lt;artifactId&gt;devon4j-starter-cxf-client-ws&lt;/artifactId&gt;\n&lt;/dependency&gt;\n9.23.3. Features\nWhen invoking a service you need to consider many cross-cutting aspects. You might not think about them in the very first place and you do not want to implement them multiple times redundantly. Therefore you should consider using this approach. The following sub-sections list the covered features and aspects:\nSimple usage\nAssuming you already have a Java interface MyService of the service you want to invoke:\npackage com.company.department.foo.mycomponent.service.api.rest;\n...\n@Path(&quot;/myservice&quot;)\npublic interface MyService extends RestService {\n@POST\n@Path(&quot;/getresult&quot;)\nMyResult getResult(MyArgs myArgs);\n}\nThen all you need to do is this:\n@Named\npublic class UcMyUseCaseImpl extends MyUseCaseBase implements UcMyUseCase {\n@Inject\nprivate ServiceClientFactory serviceClientFactory;\n...\nprivate MyResult callMyServiceMethod(MyArgs myArgs) {\nMyService myService = this.serviceClientFactory.create(MyService.class);\nMyResult myResult = myService.myMethod(myArgs); // synchronous call of service over the wire\nreturn myResult;\n}\nAs you can see the synchronous invocation of a service is very simple. Still it is very flexible and powerful (see following features). The actual call of myMethod will technically call the remote service over the wire (e.g. via HTTP) including marshaling the arguments (e.g. converting myArgs to JSON) and unmarshalling the result (e.g. converting the received JSON to myResult).\nConfiguration\nThis solution allows a very flexible configuration on the following levels:\nGlobal configuration (defaults)\nConfiguration per remote service application (microservice)\nConfiguration per invocation.\nA configuration on a deeper level (e.g. 3) overrides the configuration from a higher level (e.g. 1).\nThe configuration on Level 1 and 2 are configured via application.properties\n(see configuration guide).\nFor Level 1 the prefix service.client.default. is used for properties.\nFurther, for level 2. the prefix service.client.app.&#xAB;application&#xBB;. is used where &#xAB;application&#xBB; is the\ntechnical name of the application providing the service. This name will automatically be derived from\nthe java package of the service interface (e.g. foo in MyService interface before) following our\npackaging conventions.\nIn case these conventions are not met it will fallback to the fully qualified name of the service interface.\nConfiguration on Level 3 has to be provided as Map argument to the method\nServiceClientFactory.create(Class&lt;S&gt; serviceInterface, Map&lt;String, String&gt; config).\nThe keys of this Map will not use prefixes (such as the ones above). For common configuration\nparameters a type-safe builder is offered to create such map via ServiceClientConfigBuilder.\nE.g. for testing you may want to do:\nthis.serviceClientFactory.create(MyService.class,\nnew ServiceClientConfigBuilder().authBasic().userLogin(login).userPassword(password).buildMap());\nTODO add configuration properties for external/mock service from Sneha\nService Discovery\nYou do not want to hardwire service URLs in your code, right? Therefore different strategies might apply\nto discover the URL of the invoked service. This is done internally by an implementation of the interface\nServiceDiscoverer. The default implementation simply reads the base URL from the configuration (see above).\nSo you can simply add this to your application.properties:\nservice.client.app.foo.url=https://foo.company.com:8443/services/rest\nservice.client.app.bar.url=http://bar.company.com:8080/services/rest\nservice.client.default.url=https://api.company.com/services/rest\nAssuming your service interface would have the fully qualified name\ncom.company.department.foo.mycomponent.service.api.rest.MyService then the URL would be resolved to\nhttps://foo.company.com:8443/services/rest as the &#xAB;application&#xBB; is foo.\nAdditionally, the URL might use the following variables that will automatically be resolved:\n${app} to &#xAB;application&#xBB; (useful for default URL)\n${type} to the type of the service. E.g. rest in case of a REST service and ws for a SOAP service.\n${local.server.port} for the port of your current Java servlet container running the JVM. Should only used for testing with spring-boot random port mechanism (technically spring can not resolve this variable but we do it for you here).\nTherefore, the default URL may also be configured as:\nservice.client.default.url=https://api.company.com/${app}/services/${type}\nAs you can use any implementation of ServiceDiscoverer, you can also easily use eureka (or anything else) instead to discover your services.\nHeaders\nA very common demand is to tweak (HTTP) headers in the request to invoke the service. May it be for security (authentication data) or for other cross-cutting concerns (such as the Correlation ID). This is done internally by implementations of the interface ServiceHeaderCustomizer.\nWe already provide several implementations such as:\nServiceHeaderCustomizerBasicAuth for basic authentication (mainly for testing).\nServiceHeaderCustomizerOAuth for OAuth (passes a security token from security context such as a JWT via OAuth).\nServiceHeaderCustomizerCorrelationId passed the Correlation ID to the service request.\nAdditionally, you can add further custom implementations of ServiceHeaderCustomizer for your individual requirements and additional headers.\nTimeouts\nYou can configure timeouts in a very flexible way. First of all you can configure timeouts to establish the connection (timeout.connection) and to wait for the response (timeout.response) separately. These timeouts can be configured on all three levels as described in the configuration section above.\nError Handling\nWhilst invoking a remote service an error may occur. This solution will automatically handle such errors and map them to a higher level ServiceInvocationFailedException. In general we separate two different types of errors:\nNetwork error\nIn such case (host not found, connection refused, time out, etc.) there is not even a response from the server. However, in advance to a low-level exception you will get a wrapped ServiceInvocationFailedException (with code ServiceInvoke) with a readable message containing the service that could not be invoked.\nService error\nIn case the service failed on the server-side the error result will be parsed and thrown as a ServiceInvocationFailedException with the received message and code.\nLogging\nBy default this solution will log all invocations including the URL of the invoked service, success or error status flag and the duration in seconds (with decimal nano precision as available). Therefore you can easily monitor the status and performance of the service invocations.\nAsynchronous Support\nAn important aspect is also asynchronous (and reactive) support. So far we only propose this as an enhancement for a future release with an API like this:\npublic interface ServiceClientAsyncFactory extends ServiceClientFactory {\nAsyncClient&lt;S&gt; createAsync(Class&lt;S&gt; serviceInterface);\n}\npublic interface AsyncClient&lt;S&gt; {\nS getClient();\n&lt;R&gt; Mono&lt;R&gt; call(R result);\n&lt;T&gt; Flux&lt;T&gt; call(Collection&lt;? extends T&gt; result);\n&lt;R&gt; void call(R result, Consumer&lt;R&gt; callback);\n&lt;R&gt; CompletableFuture&lt;R&gt; callFuture(R result);\n}\nThis API would allow typesafe usage like this:\n@Named\npublic class UcMyUseCaseImpl extends MyUseCaseBase implements UcMyUseCase {\n@Inject private ServiceClientFactoryAsync clientFactory;\n@Override @RolesAllowed(...)\npublic void doSomething(Bar bar) {\nAsyncClient&lt;MyExternalServiceApi&gt; client = this.clientFactory.createAsync(MyExternalServiceApi.class);\nMono&lt;Some&gt; result = client.call(client.getService().doSomething(convert(bar)));\n// client.call(client.getService().doSomething(convert(bar)), x -&gt; processSync(x));\nreturn process(result);\n}\n}\nHow can this work? The ServiceClientAsyncFactory implementation would create its own dynamic proxy for the given service interface. That proxy would only track the last call that was invoked internally and always return a dummy result (null for Object types, false for boolean, 0 for primitive numbers). The actual implementation of the call methods can access the internal invocation that has been recorded from the last service call. It will then trigger the actual service call internally according to the desired style (using a Consumer callback, Mono, Flux, Future&#x2026;&#x200B;).\nResilience\nResilience adds a lot of complexity and that typically means that addressing this here would most probably result in not being up-to-date and not meeting all requirements. Therefore we recommend something completely different: the sidecar approach (based on sidecar pattern). This means that you use a generic proxy app that runs as a separate process on the same host, VM, or container of your actual application. Then in your app you are calling the service via the sidecar proxy on localhost (service discovery URL is e.g. http://localhost:8081/${app}/services/${type}) that then acts as proxy to the actual remote service. Now aspects such as resilience with circuit breaking and the actual service discovery can be configured in the sidecar proxy app and independent of your actual application. Therefore, you can even share and reuse configuration and experience with such a sidecar proxy app even across different technologies (Java, .NET/C#, Node.JS, etc.).\nVarious implementations of such sidecar proxy apps are available as free open source software.\nTODO: Decide for the best available solution and suggest here as default.\nNetflix Sidecar - see Spring Cloud Netflix docs\nEnvoy - see Microservices Patterns With Envoy Sidecar Proxy\nPrana - see Prana: A Sidecar for your Netflix PaaS based Applications and Services &#x2190; Not updated as it&#x2019;s not used internally by Netflix\nKeycloak - see Protecting Jaeger UI with a sidecar security proxy\n9.24. Testing\n9.24.1. General best practices\nFor testing please follow our general best practices:\nTests should have a clear goal that should also be documented.\nTests have to be classified into different integration levels.\nTests should follow a clear naming convention.\nAutomated tests need to properly assert the result of the tested operation(s) in a reliable way. E.g. avoid stuff like assertThat(service.getAllEntities()).hasSize(42) or even worse tests that have no assertion at all.\nTests need to be independent of each other. Never write test-cases or tests (in Java @Test methods) that depend on another test to be executed before.\nUse AssertJ to write good readable and maintainable tests that also provide valuable feedback in case a test fails. Do not use legacy JUnit methods like assertEquals anymore!\nFor easy understanding divide your test in three commented sections:\n//given\n//when\n//then\nPlan your tests and test data management properly before implementing.\nInstead of having a too strong focus on test coverage better ensure you have covered your critical core functionality properly and review the code including tests.\nTest code shall NOT be seen as second class code. You shall consider design, architecture and code-style also for your test code but do not over-engineer it.\nTest automation is good but should be considered in relation to cost per use. Creating full coverage via automated system tests can cause a massive amount of test-code that can turn out as a huge maintenance hell. Always consider all aspects including product life-cycle, criticality of use-cases to test, and variability of the aspect to test (e.g. UI, test-data).\nUse continuous integration and establish that the entire team wants to have clean builds and running tests.\nPrefer delegation over inheritance for cross-cutting testing functionality. Good places to put this kind of code can be realized and reused via the JUnit @Rule mechanism.\n9.24.2. Test Automation Technology Stack\nFor test automation we use JUnit. However, we are strictly doing all assertions with AssertJ. For mocking we use mockito.\nIn order to mock remote connections we use wiremock.\nFor testing entire components or sub-systems we recommend to use spring-boot-starter-test as lightweight and fast testing infrastructure that is already shipped with devon4j-test.\nIn case you have to use a full blown JEE application server, we recommend to use arquillian. To get started with arquillian, look here.\n9.24.3. Test Doubles\nWe use test doubles as generic term for mocks, stubs, fakes, dummies, or spys to avoid confusion. Here is a short summary from stubs VS mocks:\nDummy objects specifying no logic at all. May declare data in a POJO style to be used as boiler plate code to parameter lists or even influence the control flow towards the test&#x2019;s needs.\nFake objects actually have working implementations, but usually take some shortcut which makes them not suitable for production (an in memory database is a good example).\nStubs provide canned answers to calls made during the test, usually not responding at all to anything outside what&#x2019;s programmed in for the test. Stubs may also record information about calls, such as an email gateway stub that remembers the messages it &apos;sent&apos;, or maybe only how many messages it &apos;sent&apos;.\nMocks are objects pre-programmed with expectations, which form a specification of the calls they are expected to receive.\nWe try to give some examples, which should make it somehow clearer:\nStubs\nBest Practices for applications:\nA good way to replace small to medium large boundary systems, whose impact (e.g. latency) should be ignored during load and performance tests of the application under development.\nAs stub implementation will rely on state-based verification, there is the threat, that test developers will partially reimplement the state transitions based on the replaced code. This will immediately lead to a black maintenance whole, so better use mocks to assure the certain behavior on interface level.\nDo NOT use stubs as basis of a large amount of test cases as due to state-based verification of stubs, test developers will enrich the stub implementation to become a large monster with its own hunger after maintenance efforts.\nMocks\nBest Practices for applications:\nReplace not-needed dependencies of your system-under-test (SUT) to minimize the application context to start of your component framework.\nReplace dependencies of your SUT to impact the control flow under test without establishing all the context parameters needed to match the control flow.\nRemember: Not everything has to be mocked! Especially on lower levels of tests like isolated module tests you can be betrayed into a mocking delusion, where you end up in a hundred lines of code mocking the whole context and five lines executing the test and verifying the mocks behavior. Always keep in mind the benefit-cost ratio, when implementing tests using mocks.\nWiremock\nIf you need to mock remote connections such as HTTP-Servers, wiremock offers easy to use functionality. For a full description see the homepage or the github repository. Wiremock can be used either as a JUnit Rule, in Java outside of JUnit or as a standalone process. The mocked server can be configured to respond to specific requests in a given way via a fluent Java API, JSON files and JSON over HTTP. An example as an integration to JUnit can look as follows.\nimport static com.github.tomakehurst.wiremock.core.WireMockConfiguration.wireMockConfig;\nimport com.github.tomakehurst.wiremock.junit.WireMockRule;\npublic class WireMockOfferImport{\n@Rule\npublic WireMockRule mockServer = new WireMockRule(wireMockConfig().dynamicPort());\n@Test\npublic void requestDataTest() throws Exception {\nint port = this.mockServer.port();\n...}\nThis creates a server on a randomly chosen free port on the running machine. You can also specify the port to be used if wanted. Other than that there are several options to further configure the server. This includes HTTPs, proxy settings, file locations, logging and extensions.\n@Test\npublic void requestDataTest() throws Exception {\nthis.mockServer.stubFor(get(urlEqualTo(&quot;/new/offers&quot;)).withHeader(&quot;Accept&quot;, equalTo(&quot;application/json&quot;))\n.withHeader(&quot;Authorization&quot;, containing(&quot;Basic&quot;)).willReturn(aResponse().withStatus(200).withFixedDelay(1000)\n.withHeader(&quot;Content-Type&quot;, &quot;application/json&quot;).withBodyFile(&quot;/wireMockTest/jsonBodyFile.json&quot;)));\n}\nThis will stub the URL localhost:port/new/offers to respond with a status 200 message containing a header (Content-Type: application/json) and a body with content given in jsonBodyFile.json if the request matches several conditions.\nIt has to be a GET request to ../new/offers with the two given header properties.\nNote that by default files are located in src/test/resources/__files/. When using only one WireMock server one can omit the this.mockServer in before the stubFor call (static method).\nYou can also add a fixed delay to the response or processing delay with WireMock.addRequestProcessingDelay(time) in order to test for timeouts.\nWireMock can also respond with different corrupted messages to simulate faulty behaviour.\n@Test(expected = ResourceAccessException.class)\npublic void faultTest() {\nthis.mockServer.stubFor(get(urlEqualTo(&quot;/fault&quot;)).willReturn(aResponse()\n.withFault(Fault.MALFORMED_RESPONSE_CHUNK)));\n...}\nA GET request to ../fault returns an OK status header, then garbage, and then closes the connection.\n9.24.4. Integration Levels\nThere are many discussions about the right level of integration for test automation. Sometimes it is better to focus on small, isolated modules of the system - whatever a &quot;module&quot; may be. In other cases it makes more sense to test integrated groups of modules. Because there is no universal answer to this question, devonfw only defines a common terminology for what could be tested. Each project must make its own decision where to put the focus of test automation. There is no worldwide accepted terminology for the integration levels of testing. In general we consider ISTQB. However, with a technical focus on test automation we want to get more precise.\nThe following picture shows a simplified view of an application based on the devonfw reference architecture. We define four integration levels that are explained in detail below.\nThe boxes in the picture contain parenthesized numbers. These numbers depict the lowest integration level, a box belongs to. Higher integration levels also contain all boxes of lower integration levels. When writing tests for a given integration level, related boxes with a lower integration level must be replaced by test doubles or drivers.\nThe main difference between the integration levels is the amount of infrastructure needed to test them. The more infrastructure you need, the more bugs you will find, but the more instable and the slower your tests will be. So each project has to make a trade-off between pros and contras of including much infrastructure in tests and has to select the integration levels that fit best to the project.\nConsider, that more infrastructure does not automatically lead to a better bug-detection. There may be bugs in your software that are masked by bugs in the infrastructure. The best way to find those bugs is to test with very few infrastructure.\nExternal systems do not belong to any of the integration levels defined here. devonfw does not recommend involving real external systems in test automation. This means, they have to be replaced by test doubles in automated tests. An exception may be external systems that are fully under control of the own development team.\nThe following chapters describe the four integration levels.\nLevel 1 Module Test\nThe goal of a isolated module test is to provide fast feedback to the developer. Consequently, isolated module tests must not have any interaction with the client, the database, the file system, the network, etc.\nAn isolated module test is testing a single classes or at least a small set of classes in isolation. If such classes depend on other components or external resources, etc. these shall be replaced with a test double.\npublic class MyClassTest extends ModuleTest {\n@Test\npublic void testMyClass() {\n// given\nMyClass myClass = new MyClass();\n// when\nString value = myClass.doSomething();\n// then\nassertThat(value).isEqualTo(&quot;expected value&quot;);\n}\n}\nFor an advanced example see here.\nLevel 2 Component Test\nA component test aims to test components or component parts as a unit.\nThese tests typically run with a (light-weight) infrastructure such as spring-boot-starter-test and can access resources such as a database (e.g. for DAO tests).\nFurther, no remote communication is intended here. Access to external systems shall be replaced by a test double.\nWith devon4j and spring you can write a component-test as easy as illustrated in the following example:\n@SpringBootTest(classes = { MySpringBootApp.class }, webEnvironment = WebEnvironment.NONE)\npublic class UcFindCountryTest extends ComponentTest {\n@Inject\nprivate UcFindCountry ucFindCountry;\n@Test\npublic void testFindCountry() {\n// given\nString countryCode = &quot;de&quot;;\n// when\nTestUtil.login(&quot;user&quot;, MyAccessControlConfig.FIND_COUNTRY);\nCountryEto country = this.ucFindCountry.findCountry(countryCode);\n// then\nassertThat(country).isNotNull();\nassertThat(country.getCountryCode()).isEqualTo(countryCode);\nassertThat(country.getName()).isEqualTo(&quot;Germany&quot;);\n}\n}\nThis test will start the entire spring-context of your app (MySpringBootApp). Within the test spring will inject according spring-beans into all your fields annotated with @Inject. In the test methods you can use these spring-beans and perform your actual tests. This pattern can be used for testing DAOs/Repositories, Use-Cases, or any other spring-bean with its entire configuration including database and transactions.\nWhen you are testing use-cases your authorization will also be in place. Therefore, you have to simulate a logon in advance what is done via the login method in the above example. The test-infrastructure will automatically do a logout for you after each test method in doTearDown.\nLevel 3 Subsystem Test\nA subsystem test runs against the external interfaces (e.g. HTTP service) of the integrated subsystem. Subsystem tests of the client subsystem are described in the devon4ng testing guide. In devon4j the server (JEE application) is the subsystem under test. The tests act as a client (e.g. service consumer) and the server has to be integrated and started in a container.\nWith devon4j and spring you can write a subsystem-test as easy as illustrated in the following example:\n@SpringBootTest(classes = { MySpringBootApp.class }, webEnvironment = WebEnvironment.RANDOM_PORT)\npublic class CountryRestServiceTest extends SubsystemTest {\n@Inject\nprivate ServiceClientFactory serviceClientFactory;\n@Test\npublic void testFindCountry() {\n// given\nString countryCode = &quot;de&quot;;\n// when\nCountryRestService service = this.serviceClientFactory.create(CountryRestService.class);\nCountryEto country = service.findCountry(countryCode);\n// then\nassertThat(country).isNotNull();\nassertThat(country.getCountryCode()).isEqualTo(countryCode);\nassertThat(country.getName()).isEqualTo(&quot;Germany&quot;);\n}\n}\nEven though not obvious on the first look this test will start your entire application as a server on a free random port (so that it works in CI with parallel builds for different branches) and tests the invocation of a (REST) service including (un)marshalling of data (e.g. as JSON) and transport via HTTP (all in the invocation of the findCountry method).\nDo not confuse a subsystem test with a system integration test. A system integration test validates the interaction of several systems where we do not recommend test automation.\nLevel 4 System Test\nA system test has the goal to test the system as a whole against its official interfaces such as its UI or batches. The system itself runs as a separate process in a way close to a regular deployment. Only external systems are simulated by test doubles.\nThe devonfw only gives advice for automated system test (TODO see allure testing framework). In nearly every project there must be manual system tests, too. This manual system tests are out of scope here.\nClassifying Integration-Levels\ndevon4j defines Category-Interfaces that shall be used as JUnit Categories.\nAlso devon4j provides abstract base classes that you may extend in your test-cases if you like.\ndevon4j further pre-configures the maven build to only run integration levels 1-2 by default (e.g. for fast feedback in continuous integration). It offers the profiles subsystemtest (1-3) and systemtest (1-4). In your nightly build you can simply add -Psystemtest to run all tests.\n9.24.5. Implementation\nThis section introduces how to implement tests on the different levels with the given devonfw infrastructure and the proposed frameworks.\nModule Test\nIn devon4j you can extend the abstract class ModuleTest to basically get access to assertions. In order to test classes embedded in dependencies and external services one needs to provide mocks for that. As the technology stack recommends we use the Mockito framework to offer this functionality. The following example shows how to implement Mockito into a JUnit test.\nimport static org.mockito.Mockito.when;\nimport static org.mockito.Mockito.mock;\n...\npublic class StaffmanagementImplTest extends ModuleTest {\n@Rule\npublic MockitoRule rule = MockitoJUnit.rule();\n@Test\npublic void testFindStaffMember() {\n...}\n}\nNote that the test class does not use the @SpringApplicationConfiguration annotation. In a module test one does not use the whole application.\nThe JUnit rule is the best solution to use in order to get all needed functionality of Mockito. Static imports are a convenient option to enhance readability within Mockito tests.\nYou can define mocks with the @Mock annotation or the mock(*.class) call. To inject the mocked objects into your class under test you can use the @InjectMocks annotation. This automatically uses the setters of StaffmanagementImpl to inject the defined mocks into the class under test (CUT) when there is a setter available. In this case the beanMapper and the staffMemberDao are injected. Of course it is possible to do this manually if you need more control.\n@Mock\nprivate BeanMapper beanMapper;\n@Mock\nprivate StaffMemberEntity staffMemberEntity;\n@Mock\nprivate StaffMemberEto staffMemberEto;\n@Mock\nprivate StaffMemberDao staffMemberDao;\n@InjectMocks\nStaffmanagementImpl staffmanagementImpl = new StaffmanagementImpl();\nThe mocked objects do not provide any functionality at the time being. To define what happens on a method call on a mocked dependency in the CUT one can use when(condition).thenReturn(result). In this case we want to test findStaffMember(Long id) in the StaffmanagementImpl.\npublic StaffMemberEto findStaffMember(Long id) {\nreturn getBeanMapper().map(getStaffMemberDao().find(id), StaffMemberEto.class);\n}\nIn this simple example one has to stub two calls on the CUT as you can see below. For example the method call of the CUT staffMemberDao.find(id) is stubbed for returning a mock object staffMemberEntity that is also defined as mock.\nSubsystem Test\ndevon4j provides a simple test infrastructure to aid with the implementation of subsystem tests. It becomes available by simply subclassing AbstractRestServiceTest.java.\n//given\nlong id = 1L;\nClass&lt;StaffMemberEto&gt; targetClass = StaffMemberEto.class;\nwhen(this.staffMemberDao.find(id)).thenReturn(this.staffMemberEntity);\nwhen(this.beanMapper.map(this.staffMemberEntity, targetClass)).thenReturn(this.staffMemberEto);\n//when\nStaffMemberEto resultEto = this.staffmanagementImpl.findStaffMember(id);\n//then\nassertThat(resultEto).isNotNull();\nassertThat(resultEto).isEqualTo(this.staffMemberEto);\nAfter the test method call one can verify the expected results. Mockito can check whether a mocked method call was indeed called. This can be done using Mockito verify. Note that it does not generate any value if you check for method calls that are needed to reach the asserted result anyway. Call verification can be useful e.g. when you want to assure that statistics are written out without actually testing them.\nTODO\nAs an example let us go to the class Tablemanagement. When testing the method deleteTable() there are several scenarios that can happen and thus should be covered by tests.\nFirst let us see the valid conditions to delete a table:\nOne needs permission to delete a table PermissionConstants.DELETE_TABLE\nThe table to delete needs to exist (the table with the given id has to be in the database) and\nThe table to delete is required to be TableState.FREE\nInvalid conditions are: No credentials, table does not exist or table is not free.\nIf you combine one invalid condition with valid conditions this yields the following test cases. Note that not working actions yield exceptions that can be expected in a test method.\nThe caller of the method does not have the required credentials\n@Test(expected = AccessDeniedException.class)\npublic void testDeleteTableWithoutCredentials() {...}\nThe caller has the required credentials but the table to be deleted is occupied\n@Test(expected = IllegalEntityStateException.class)\npublic void testDeleteTableWithCredentialsButNotDeletable() {...}\nThe caller has the required credentials but the table to be deleted does not exist\n@Test(expected = ObjectNotFoundUserException.class)\npublic void testDeleteTableWithCredentialsNotExisting() {...}\nThe caller has the required credentials and the table to be deleted exists and is free\n@Test\npublic void testDeleteTableWithCredentials() {...}\nThis type of testing is known as equivalence class analysis. Note that this is a general practice and can be applied to every level of tests.\n9.24.6. Deployment Pipeline\nA deployment pipeline is a semi-automated process that gets software-changes from version control into production. It contains several validation steps, e.g. automated tests of all integration levels.\nBecause devon4j should fit to different project types - from agile to waterfall - it does not define a standard deployment pipeline. But we recommend to define such a deployment pipeline explicitly for each project and to find the right place in it for each type of test.\nFor that purpose, it is advisable to have fast running test suite that gives as much confidence as possible without needing too much time and too much infrastructure. This test suite should run in an early stage of your deployment pipeline. Maybe the developer should run it even before he/she checked in the code. Usually lower integration levels are more suitable for this test suite than higher integration levels.\nNote, that the deployment pipeline always should contain manual validation steps, at least manual acceptance testing. There also may be manual validation steps that have to be executed for special changes only, e.g. usability testing. Management and execution processes of those manual validation steps are currently not in the scope of devonfw.\n9.24.7. Test Coverage\nWe are using tools (SonarQube/Jacoco) to measure the coverage of the tests. Please always keep in mind that the only reliable message of a code coverage of X% is that (100-X)% of the code is entirely untested. It does not say anything about the quality of the tests or the software though it often relates to it.\n9.24.8. Test Configuration\nThis section covers test configuration in general without focusing on integration levels as in the first chapter.\nConfigure Test Specific Beans\nSometimes it can become handy to provide other or differently configured bean implementations via CDI than those available in production. For example, when creating beans using @Bean-annotated methods they are usually configured within those methods. WebSecurityBeansConfig shows an example of such methods.\n@Configuration\npublic class WebSecurityBeansConfig {\n//...\n@Bean\npublic AccessControlSchemaProvider accessControlSchemaProvider() {\n// actually no additional configuration is shown here\nreturn new AccessControlSchemaProviderImpl();\n}\n//...\n}\nAccessControlSchemaProvider allows to programmatically access data defined in some XML file, e.g. access-control-schema.xml. Now, one can imagine that it would be helpful if AccessControlSchemaProvider would point to some other file than the default within a test class. That file could provide content that differs from the default.\nThe question is: how can I change resource path of AccessControlSchemaProviderImpl within a test?\nOne very helpful solution is to use static inner classes.\nStatic inner classes can contain @Bean -annotated methods, and by placing them in the classes parameter in @SpringBootTest(classes = { /* place class here*/ }) annotation the beans returned by these methods are placed in the application context during test execution. Combining this feature with inheritance allows to override methods defined in other configuration classes as shown in the following listing where TempWebSecurityConfig extends WebSecurityBeansConfig. This relationship allows to override public AccessControlSchemaProvider accessControlSchemaProvider(). Here we are able to configure the instance of type AccessControlSchemaProviderImpl before returning it (and, of course, we could also have used a completely different implementation of the AccessControlSchemaProvider interface). By overriding the method the implementation of the super class is ignored, hence, only the new implementation is called at runtime. Other methods defined in WebSecurityBeansConfig which are not overridden by the subclass are still dispatched to WebSecurityBeansConfig.\n//... Other testing related annotations\n@SpringBootTest(classes = { TempWebSecurityConfig.class })\npublic class SomeTestClass {\npublic static class TempWebSecurityConfig extends WebSecurityBeansConfig {\n@Override\n@Bean\npublic AccessControlSchemaProvider accessControlSchemaProvider() {\nClassPathResource resource = new ClassPathResource(locationPrefix + &quot;access-control-schema3.xml&quot;);\nAccessControlSchemaProviderImpl accessControlSchemaProvider = new AccessControlSchemaProviderImpl();\naccessControlSchemaProvider.setAccessControlSchema(resource);\nreturn accessControlSchemaProvider;\n}\n}\n}\nThe following chapter of the Spring framework documentation explains issue, but uses a slightly different way to obtain the configuration.\nTest Data\nIt is possible to obtain test data in two different ways depending on your test&#x2019;s integration level.\n9.24.9. Debugging Tests\nThe following two sections describe two debugging approaches for tests. Tests are either run from within the IDE or from the command line using Maven.\nDebugging with the IDE\nDebugging with the IDE is as easy as always. Even if you want to execute a SubsystemTest which needs a Spring context and a server infrastructure to run properly, you just set your breakpoints and click on Debug As &#x2192; JUnit Test. The test infrastructure will take care of initializing the necessary infrastructure - if everything is configured properly.\nDebugging with Maven\nPlease refer to the following two links to find a guide for debugging tests when running them from Maven.\nhttp://maven.apache.org/surefire/maven-surefire-plugin/examples/debugging.html\nhttps://www.eclipse.org/jetty/documentation/9.3.x/debugging-with-eclipse.html\nIn essence, you first have to start execute a test using the command line. Maven will halt just before the test execution and wait for your IDE to connect to the process. When receiving a connection the test will start and then pause at any breakpoint set in advance.\nThe first link states that tests are started through the following command:\nmvn -Dmaven.surefire.debug test\nAlthough this is correct, it will run every test class in your project and - which is time consuming and mostly unnecessary - halt before each of these tests.\nTo counter this problem you can simply execute a single test class through the following command (here we execute the TablemanagementRestServiceTest from the restaurant sample application):\nmvn test -Dmaven.surefire.debug test -Dtest=TablemanagementRestServiceTest\nIt is important to notice that you first have to execute the Maven command in the according submodule, e.g. to execute the TablemanagementRestServiceTest you have first to navigate to the core module&#x2019;s directory.\n9.25. Transfer-Objects\nThe technical data model is defined in form of persistent entities.\nHowever, passing persistent entities via call-by-reference across the entire application will soon cause problems:\nChanges to a persistent entity are directly written back to the persistent store when the transaction is committed. When the entity is send across the application also changes tend to take place in multiple places endangering data sovereignty and leading to inconsistency.\nYou want to send and receive data via services across the network and have to define what section of your data is actually transferred. If you have relations in your technical model you quickly end up loading and transferring way too much data.\nModifications to your technical data model shall not automatically have impact on your external services causing incompatibilities.\nTo prevent such problems transfer-objects are used leading to a call-by-value model and decoupling changes to persistent entities.\nIn the following sections the different types of transfer-objects are explained.\nYou will find all according naming-conventions in the architecture-mapping\nETO\nFor each persistent entity &#xAB;BusinessObject&#xBB;Entity we create or generate a corresponding entity transfer object (ETO) named &#xAB;BusinessObject&#xBB;Eto. It has the same properties except for relations.\nBO\nIn order to centralize the properties (getters and setters with their javadoc) we create a common interface &#xAB;BusinessObject&#xBB; implemented both by the entity and its ETO. This also gives us compile-time safety that\nbean-mapper can properly map all properties between entity and ETO.\nCTO\nIf we need to pass an entity with its relation(s) we create a corresponding composite transfer object (CTO) named &#xAB;BusinessObject&#xBB;&#xAB;Subset&#xBB;Cto that only contains other transfer-objects or collections of them. Here &#xAB;Subset&#xBB; is empty for the canonical CTO that holds the ETO together with all its relations.\nThis is what can be generated automatically with CobiGen.\nHowever, be careful to generate CTOs without thinking and considering design.\nIf there are no relations at all a CTO is pointless and shall be omitted.\nHowever, if there are multiple relations you typically need multiple CTOs for the same &#xAB;BusinessObject&#xBB; that define different subsets of the related data.\nThese will typically be designed and implemented by hand.\nE.g. you may have CustomerWithAddressCto and CustomerWithContractCto. Most CTOs correspond to a specific &#xAB;BusinessObject&#xBB; and therefore contain a &#xAB;BusinessObject&#xBB;Eto. Such CTOs should inherit from MasterCto.\nThis pattern with entities, ETOs and CTOs is illustrated by the following UML diagram from our sample application.\nFigure 26. ETOs and CTOs\n9.25.4. TO\nFinally, there are typically transfer-objects for data that is never persistent. For very generic cases\nthese just carry the suffix To.\n9.25.5. SearchCriteriaTo\nFor searching we create or generate a `&#xAB;BusinessObject&#xBB;SearchCriteriaTo` representing a query to find instances of `&#xAB;BusinessObject&#xBB;`.\n9.25.6. STO\nWe can potentially create separate service transfer objects (STO) (if possible named &#xAB;BusinessObject&#xBB;Sto) to keep the service API stable and independent of the actual data-model.\nHowever, we usually do not need this and want to keep our architecture simple.\nOnly create STOs if you need service versioning and support previous APIs or to provide legacy service technologies that require their own isolated data-model.\nIn such case you also need beanmapping between STOs and ETOs what means extra effort and complexity that should be avoided.\nIn such case you also need beanmapping between STOs and ETOs what means extra effort and complexity that should be avoided.\n9.26. Bean-Mapping\nFor decoupling you sometimes need to create separate objects (beans) for a different view. E.g. for an external service you will use a transfer-object instead of the persistence entity so internal changes to the entity do not implicitly change or break the service.\nTherefore you have the need to map similar objects what creates a copy. This also has the benefit that modifications to the copy have no side-effect on the original source object. However, to implement such mapping code by hand is very tedious and error-prone (if new properties are added to beans but not to mapping code):\npublic UserEto mapUser(UserEntity source) {\nUserEto target = new UserEto();\ntarget.setUsername(source.getUsername());\ntarget.setEmail(source.getEmail());\n...\nreturn target;\n}\nTherefore we are using a BeanMapper for this purpose that makes our lives a lot easier.\n9.26.1. Bean-Mapper Dependency\nTo get access to the BeanMapper we use this dependency in our POM:\n&lt;dependency&gt;\n&lt;groupId&gt;com.devonfw.java&lt;/groupId&gt;\n&lt;artifactId&gt;devon4j-beanmapping&lt;/artifactId&gt;\n&lt;/dependency&gt;\n9.26.2. Bean-Mapper Configuration\nThe BeanMapper implementation is based on an existing open-source bean mapping framework.\nIn case of Dozer the mapping is configured src/main/resources/config/app/common/dozer-mapping.xml.\nSee the my-thai-star dozer-mapping.xml as an example.\nImportant is that you configure all your custom datatypes as &lt;copy-by-reference&gt; tags and have the mapping from PersistenceEntity (ApplicationPersistenceEntity) to AbstractEto configured properly:\n&lt;mapping type=&quot;one-way&quot;&gt;\n&lt;class-a&gt;com.devonfw.module.basic.common.api.entity.PersistenceEntity&lt;/class-a&gt;\n&lt;class-b&gt;com.devonfw.module.basic.common.api.to.AbstractEto&lt;/class-b&gt;\n&lt;field custom-converter=&quot;com.devonfw.module.beanmapping.common.impl.dozer.IdentityConverter&quot;&gt;\n&lt;a&gt;this&lt;/a&gt;\n&lt;b is-accessible=&quot;true&quot;&gt;persistentEntity&lt;/b&gt;\n&lt;/field&gt;\n&lt;/mapping&gt;\n9.26.3. Bean-Mapper Usage\nThen we can get the BeanMapper via dependency-injection what we typically already provide by an abstract base class (e.g. AbstractUc). Now we can solve our problem very easy:\n...\nUserEntity resultEntity = ...;\n...\nreturn getBeanMapper().map(resultEntity, UserEto.class);\nThere is also additional support for mapping entire collections.\n9.27. Datatypes\nA datatype is an object representing a value of a specific type with the following aspects:\nIt has a technical or business specific semantic.\nIts JavaDoc explains the meaning and semantic of the value.\nIt is immutable and therefore stateless (its value assigned at construction time and can not be modified).\nIt is serializable.\nIt properly implements #equals(Object) and #hashCode() (two different instances with the same value are equal and have the same hash).\nIt shall ensure syntactical validation so it is NOT possible to create an instance with an invalid value.\nIt is responsible for formatting its value to a string representation suitable for sinks such as UI, loggers, etc. Also consider cases like a Datatype representing a password where toString() should return something like &quot;**&quot; instead of the actual password to prevent security accidents.\nIt is responsible for parsing the value from other representations such as a string (as needed).\nIt shall provide required logical operations on the value to prevent redundancies. Due to the immutable attribute all manipulative operations have to return a new Datatype instance (see e.g. BigDecimal.add(java.math.BigDecimal)).\nIt should implement Comparable if a natural order is defined.\nBased on the Datatype a presentation layer can decide how to view and how to edit the value. Therefore a structured data model should make use of custom datatypes in order to be expressive.\nCommon generic datatypes are String, Boolean, Number and its subclasses, Currency, etc.\nPlease note that both Date and Calendar are mutable and have very confusing APIs. Therefore, use JSR-310 or jodatime instead.\nEven if a datatype is technically nothing but a String or a Number but logically something special it is worth to define it as a dedicated datatype class already for the purpose of having a central javadoc to explain it. On the other side avoid to introduce technical datatypes like String32 for a String with a maximum length of 32 characters as this is not adding value in the sense of a real Datatype.\nIt is suitable and in most cases also recommended to use the class implementing the datatype as API omitting a dedicated interface.\n&#x2014; mmm project\ndatatype javadoc\nSee mmm datatype javadoc.\n9.27.1. Datatype Packaging\nFor the devonfw we use a common packaging schema.\nThe specifics for datatypes are as following:\nSegment\nValue\nExplanation\n&lt;component&gt;\n*\nHere we use the (business) component defining the datatype or general for generic datatypes.\n&lt;layer&gt;\ncommon\nDatatypes are used across all layers and are not assigned to a dedicated layer.\n&lt;scope&gt;\napi\nDatatypes are always used directly as API even tough they may contain (simple) implementation logic. Most datatypes are simple wrappers for generic Java types (e.g. String) but make these explicit and might add some validation.\n9.27.2. Technical Concerns\nMany technologies like Dozer and QueryDSL&#x2019;s (alias API) are heavily based on reflection. For them to work properly with custom datatypes, the frameworks must be able to instantiate custom datatypes with no-argument constructors. It is therefore recommended to implement a no-argument constructor for each datatype of at least protected visibility.\n9.27.3. Datatypes in Entities\nThe usage of custom datatypes in entities is explained in the persistence layer guide.\n9.27.4. Datatypes in Transfer-Objects\nXML\nFor mapping datatypes with JAXB see XML guide.\nJSON\nFor mapping datatypes from and to JSON see JSON custom mapping.\n9.28. Accessibility\nTODO\nhttp://www.w3.org/TR/WCAG20/\nhttp://www.w3.org/WAI/intro/aria\nhttp://www.einfach-fuer-alle.de/artikel/bitv/\nhttp://www.banu.bund.de\nhttp://www.de.capgemini.com/public-sector/igov\n9.29. Caching\nCaching is a technical approach to improve performance. While it may appear easy on the first sight it is an advanced topic. In general, try to use caching only when required for performance reasons. If you come to the point that you need caching first think about:\nWhat to cache?\nBe sure about what you want to cache. Is it static data? How often will it change? What will happen if the data changes but due to caching you might receive &quot;old&quot; values? Can this be tolerated? For how long? This is not a technical question but a business requirement.\nWhere to cache?\nWill you cache data on client or server? Where exactly?\nHow to cache?\nIs a local cache sufficient or do you need a shared cache?\n9.29.1. Local Cache\n9.29.2. Shared Cache\nDistributed Cache\n9.29.3. Products\nhttp://ehcache.org/\nhttp://hazelcast.org/\nhttp://terracotta.org/\nhttp://memcached.org/\n9.29.4. Caching of Web-Resources\nhttp://www.mobify.com/blog/beginners-guide-to-http-cache-headers/\nhttp://en.wikipedia.org/wiki/Web_cache#Cache_control\nhttp://en.wikipedia.org/wiki/List_of_HTTP_header_fields#Avoiding_caching\n9.30. CORS support\nWhen you are developing Javascript client and server application separately, you have to deal with cross domain issues. We have to request from a origin domain distinct to target domain and browser does not allow this.\nSo , we need to prepare server side to accept request from other domains. We need to cover the following points:\nAccept request from other domains.\nAccept devonfw used headers like X-CSRF-TOKEN or correlationId.\nBe prepared to receive secured request (cookies).\nIt is important to note that if you are using security in your request (sending cookies) you have to set withCredentials flag to true in your client side request and deal with special IE8 characteristics.\n9.30.1. Configuring CORS support\nOn the server side we have defined a new filter in Spring security chain filters to support CORS and we have configured devonfw security chain filter to use it.\nYou only have to change CORSDisabled property value in application-default.properties properties file.\n#CORS support\nsecurity.cors.enabled=false\n9.31. BLOB support\n9.31.1. Introduction\nBLOB stands for Binary Large Object. A BLOB may be an image, an office document, ZIP archive or any other multimedia object. devon4j supports BLOB via its BinaryObject data type. The devonfw Maven archetype generates the following Java files for dealing with BLOBs:\ngeneral.common.api.BinaryObject\nInterface for a BinaryObject\ngeneral.dataaccess.api.BinaryObjectEntity\nInstance of BinaryObject entity, contains the actual BLOB\ngeneral.dataaccess.api.dao.BinaryObjectDao.java\nDAO for BinaryObject entity\ngeneral.dataaccess.base.dao.BinaryObjectDaoImpl\nImplementation of the BinaryObjectDao\ngeneral.logic.api.to.BinaryObjectEto\nETO for BinaryObject\ngeneral.logic.base.UcManageBinaryObject\nUse case for managing BinaryObject. This use case contains methods for finding, getting, deleting and saving a BLOB.\ngeneral.logic.impl.UCManageBinaryObjectImpl\nImplementation of the UcManageBinaryObject\n9.31.2. Implementing BLOB support: an example\nIn the sample application the business component Offermanagement uses BLOBs for product pictures.\nFeel free to use the following approach as starting point for BLOB support in your application.\nLogic Layer\nUse the methods declared in general.logic.base.UcManageBinaryObject in the implementation of your business component.\nLet&#x2019;s take a look at an example from the sample application.\nThe method\nOffermanagementImpl.updateProductPicture(Long productId, Blob blob, BinaryObjectEto binaryObjectEto)\nsaves a new picture for a given product.\nThis is done by calling an appropriate method, declared in the BinaryObject use case.\n@Override\n@RolesAllowed(PermissionConstants.SAVE_PRODUCT_PICTURE)\npublic void updateProductPicture(Long productId, Blob blob, BinaryObjectEto binaryObjectEto) {\n...\nbinaryObjectEto = getUcManageBinaryObject().saveBinaryObject(blob, binaryObjectEto);\n...\n}\nService Layer\nFollowing the devonfw conventions, you must implement a REST service for each business component. There you define, how BLOBs are uploaded/downloaded. According to that, the REST service for the business component Offermanagement is implemented in a class named OffermanagementRestServiceImpl.\nThe coding examples below are taken from the afore mentioned class.\nThe sample application uses the content-type &quot;multipart/mixed&quot; to transfer pictures plus additional header data.\nUpload\n@Consumes(&quot;multipart/mixed&quot;)\n@POST\n@Path(&quot;/product/{id}/picture&quot;)\npublic void updateProductPicture(@PathParam(&quot;id&quot;) long productId,\n@Multipart(value = &quot;binaryObjectEto&quot;, type = MediaType.APPLICATION_JSON) BinaryObjectEto binaryObjectEto,\n@Multipart(value = &quot;blob&quot;, type = MediaType.APPLICATION_OCTET_STREAM) InputStream picture)\nthrows SerialException, SQLException, IOException {\nBlob blob = new SerialBlob(IOUtils.readBytesFromStream(picture));\nthis.offerManagement.updateProductPicture(productId, blob, binaryObjectEto);\n}\nA new Blob object is being created by reading the data (IOUtils.readBytesFromStream(picture)).\nDownload\n@Produces(&quot;multipart/mixed&quot;)\n@GET\n@Path(&quot;/product/{id}/picture&quot;)\npublic MultipartBody getProductPicture(@PathParam(&quot;id&quot;) long productId) throws SQLException, IOException {\nBlob blob = this.offerManagement.findProductPictureBlob(productId);\nbyte[] data = IOUtils.readBytesFromStream(blob.getBinaryStream());\nList&lt;Attachment&gt; atts = new LinkedList&lt;&gt;();\natts.add(new Attachment(&quot;binaryObjectEto&quot;, MediaType.APPLICATION_JSON, this.offerManagement\n.findProductPicture(productId)));\natts.add(new Attachment(&quot;blob&quot;, MediaType.APPLICATION_OCTET_STREAM, new ByteArrayInputStream(data)));\nreturn new MultipartBody(atts, true);\n}\nAs you may have noticed, the data is loaded into the heap before it is added as an Attachement to the MultiPart body.\nCaution!\nUsing a byte array will cause problems, when dealing with large BLOBs.\nWhy is the sample application using a byte array then?\nAs of now, there is no universal solid way of streaming a BLOB directly from a database to the client without reading the BLOB&#x2019;s content to memory, when streaming over a RESTful service based on JDBC and JAX RS.\nFollowing this approach means: whenever a file is uploaded or downloaded as BLOB it is loaded completely to memory before it is written to the database.\n9.31.3. Further Reading\nThe multipart content type\nJAX-RS : Support for Multiparts\nComponent Implementation\nBLOBs and the Data Access Layer\nSecurity Vulnerability Unrestricted File Upload\n9.32. Java Development Kit\nThe Java Development Kit is an implementation of the Java platform. It provides the Java Virtual Machine (JVM) and the Java Runtime Environment (JRE).\n9.32.1. Editions\nThe JDK exists in different editions:\nOpenJDK is a free and open-source edition of the JDK.\nOracleJDK is a commercial edition of the JDK.\nVarious alternative JDK editions either commercial (e.g. IBM&#x2019;s JVM) or open-source.\nAs Java is evolving and also complex maintaining a JVM requires a lot of energy.\nTherefore many alternative JDK editions are unable to cope with this and support latest Java versions and according compatibility.\nUnfortunately OpenJDK only maintains a specific version of Java for a relative short period of time before moving to the next major version.\nIn the end, this technically means that OpenJDK is continuous beta and can not be used in production for reasonable software projects.\nAs OracleJDK changed its licensing model and can not be used for commercial usage even during development, things can get tricky.\nYou may want to use OpenJDK for development and OracleJDK only in production.\nHowever, e.g. OpenJDK 11 never released a version that is stable enough for reasonable development (e.g. javadoc tool is broken and fixes are not available of OpenJDK 11 - fixed in 11.0.3 what is only available as OracleJDK 11 or you need to go to OpenJDK 12+, what has other bugs) so in the end there is no working release of OpenJDK 11.\nThis more or less forces you to use OracleJDK what requires you to buy a subscription so you can use it for commercial development.\nHowever, there is AdoptOpenJDK that provides forked releases of OpenJDK with bug-fixes what might be an option.\nAnyhow, as you want to have your development environment close to production, the productively used JDK (most likely OracleJDK) should be preferred also for development.\n9.32.2. Upgrading\nUntil Java 8 compatibility was one of the key aspects for Java version updates (after the mess on the Swing updates with Java2 many years ago).\nHowever, Java 9 introduced a lot of breaking changes.\nThis documentation wants to share the experience we collected in devonfw when upgrading from Java 8 to newer versions.\nFirst of all we separate runtime changes that you need if you want to build your software with JDK 8 but such that it can also run on newer versions (e.g. JRE 11)\nfrom changes required to also build your software with more recent JDKs (e.g. JDK 11 or 12).\nRuntime Changes\nThis section describes required changes to your software in order to make it run also with versions newer than Java 8.\nClasses removed from JDK\nThe first thing that most users hit when running their software with newer Java versions is a ClassNotFoundException like this:\nCaused by: java.lang.ClassNotFoundException: javax.xml.bind.JAXBException\nAs Java 9 introduced a module system with Jigsaw, the JDK that has been a monolithic mess is now a well-defined set of structured modules.\nSome of the classes that used to come with the JDK moved to modules that where not available by default in Java 9 and have even been removed entirely in later versions of Java.\nTherefore you should simply treat such code just like any other 3rd party component that you can add as a (maven) dependency.\nThe following table gives you the required hints to make your software work even with such classes / modules removed from the JDK (please note that the specified version is just a suggestion that worked, feel free to pick a more recent or more appropriate version):\nTable 15. Dependencies for classes removed from Java 8 since 9+\nClass\nGroupId\nArtifactId\nVersion\njavax.xml.bind.*\njavax.xml.bind\njaxb-api\n2.3.1\ncom.sun.xml.bind.*\norg.glassfish.jaxb\njaxb-runtime\n2.3.1\njava.activation.*\njavax.activation\njavax.activation-api\n1.2.0\njava.transaction.*\njavax.transaction\njavax.transaction-api\n1.2\njava.xml.ws.*\njavax.xml.ws\njaxws-api\n2.3.1\njavax.jws.*\njavax.jws\njavax.jws-api\n1.1\njavax.annotation.*\njavax.annotation\njavax.annotation-api\n1.3.2\n3rd Party Updates\nFurther, internal and inofficial APIs (e.g. sun.misc.Unsafe) have been removed.\nThese are typically not used by your software directly but by low-level 3rd party libraries like asm that need to be updated.\nAlso simple things like the Java version have changed (from 1.8.x to 9.x, 10.x, 11.x, 12.x, etc.).\nSome 3rd party libraries were parsing the Java version in a very naive way making them unable to be used with Java 9+:\nCaused by: java.lang.NullPointerException\nat org.apache.maven.surefire.shade.org.apache.commons.lang3.SystemUtils.isJavaVersionAtLeast (SystemUtils.java:1626)\nTherefore the following table gives an overview of common 3rd party libraries that have been affected by such breaking changes and need to be updated to at least the specified version:\nTable 16. Minimum recommended versions of common 3rd party for Java 9+\nGroupId\nArtifactId\nVersion\nIssue\norg.apache.commons\ncommons-lang3\n3.7\nLANG-1365\ncglib\ncglib\n3.2.9\n102, 93, 133\norg.ow2.asm\nasm\n7.1\n2941\norg.javassist\njavassist\n3.25.0-GA\n194, 228, 246, 171\nResourceBundles\nFor internationalization (i18n) and localization (l10n) ResourceBundle is used for language and country specific texts and configurations as properties (e.g. MyResourceBundle_de.properties). With Java modules there are changes and impacts you need to know to get things working. The most important change is documented in the JavaDoc of ResourceBundle. However, instead of using ResourceBundleProvider and refactoring your entire code causing incompatibilities, you can simply put the resource bundles in a regular JAR on the classpath rather than a named module (or into the lauching app).\nIf you want to implement (new) Java modules with i18n support, you can have a look at mmm-nls.\nBuildtime Changes\nIf you also want to change your build to work with a recent JDK you also need to ensure that test frameworks and maven plugins properly support this.\nFindbugs\nFindbugs does not work with Java 9+ and is actually a dead project.\nThe new findbugs is SpotBugs.\nFor maven the new solution is spotbugs-maven-plugin:\n&lt;plugin&gt;\n&lt;groupId&gt;com.github.spotbugs&lt;/groupId&gt;\n&lt;artifactId&gt;spotbugs-maven-plugin&lt;/artifactId&gt;\n&lt;version&gt;3.1.11&lt;/version&gt;\n&lt;/plugin&gt;\nTest Frameworks\nTable 17. Minimum recommended versions of common 3rd party test frameworks for Java 9+\nGroupId\nArtifactId\nVersion\nIssue\norg.mockito\nmockito-core\n2.23.4\n1419, 1696, 1607, 1594, 1577, 1482\nMaven Plugins\nTable 18. Minimum recommended versions of common maven plugins for Java 9+\nGroupId\nArtifactId\n(min.) Version\nIssue\norg.apache.maven.plugins\nmaven-compiler-plugin\n3.8.1\nx\norg.apache.maven.plugins\nmaven-surefire-plugin\n2.22.2\nSUREFIRE-1439\norg.apache.maven.plugins\nmaven-surefire-report-plugin\n2.22.2\nSUREFIRE-1439\norg.apache.maven.plugins\nmaven-archetype-plugin\n3.1.0\nx\norg.apache.maven.plugins\nmaven-javadoc-plugin\n3.1.0\nx\norg.jacoco\njacoco-maven-plugin\n0.8.3\n663\nMaven Usage\nWith Java modules you can not run Javadoc standalone anymore or you will get this error when running mvn javadoc:javadoc:\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:3.1.1:javadoc (default-cli) on project mmm-base: An error has occurred in Javadoc report generation:\n[ERROR] Exit code: 1 - error: module not found: io.github.mmm.base\n[ERROR]\n[ERROR] Command line was: /projects/mmm/software/java/bin/javadoc @options @packages @argfile\nAs a solution or workaround you need to include the compile goal into your build lifecycle so the module-path is properly configured:\nmvn compile javadoc:javadoc\n9.32.3. Sources and Links\nWe want to give credits and say thanks to the following articles that have been there before and helped us on our way:\nJava 9 Migration Guide: The Seven Most Common Challenges\nIt&#x2019;s time! Migrating to Java 11\nMigrate Maven Projects to Java 11\nJAXB on Java 9, 10, 11 and beyond\nJAXB Artifacts\n9.33. Application Performance Management\nThis guide gives hints how to manage, monitor and analyse performance of Java applications.\n9.33.1. Temporary Analysis\nIf you are facing performance issues and want to do a punctual analysis we recommend you to use glowroot. It is ideal in cases where monitoring in your local development environment is suitable. However, it is also possible to use it in your test environment. It is entirely free and open-source. Still it is very powerful and helps to trace down bottlenecks. To get a first impression of the tool take a look at the demo.\nJEE/WTP\nIn case you are forced to use an JEE application server and want to do a temporary analysis you can double click your server instance from the servers view in Eclipse and click on the link Open launch configuration in order to add the -javaagent JVM option.\n9.33.2. Regular Analysis\nIn case you want to manage application performance regularly we recommend to use JavaMelody that can be integrated into your application. More information on javamelody is available on the JavaMelody Wiki\n9.33.3. Alternatives\nPinPoint\nOpenAPM\nAppDynamics\nZabbix\n&#x2190;&#xA0;Previous:&#xA0;Layers&#xA0;| &#x2191;&#xA0;Up:&#xA0;devon4j&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;devonfw Development&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devon4j.asciidoc_introduction.html","title":"6. Introduction","body":"\n6. Introduction\nThe devonfw provides a solution to building applications which combine best-in-class frameworks and libraries as well as industry proven practices and code conventions.\nIt massively speeds up development, reduces risks and helps you to deliver better results.\nThis document contains the complete compendium of the devon4j, the Java stack of devonfw. From this link you will also find the latest release or nightly snapshot of this documentation.\n6.1. Architecture\nThere are many different views on what is summarized by the term architecture. First we introduce the key principles and architecture principles of the devonfw. Then we go into details of the the architecture of an application.\n6.1.1. Key Principles\nFor the devonfw we follow these fundamental key principles for all decisions about architecture, design, or choosing standards, libraries, and frameworks:\nKISS\nKeep it small and simple\nOpen\nCommitment to open standards and solutions (no required dependencies to commercial or vendor-specific standards or solutions)\nPatterns\nWe concentrate on providing patterns, best-practices and examples rather than writing framework code.\nSolid\nWe pick solutions that are established and have been proven to be solid and robust in real-live (business) projects.\n6.1.2. Architecture Principles\nAdditionally we define the following principles that our architecture is based on:\nComponent Oriented Design\nWe follow a strictly component oriented design to address the following sub-principles:\nSeparation of Concerns\nReusability and avoiding redundant code\nInformation Hiding via component API and its exchangeable implementation treated as secret.\nDesign by Contract for self-contained, descriptive, and stable component APIs.\nLayering as well as separation of business logic from technical code for better maintenance.\nData Sovereignty (and high cohesion with low coupling) says that a component is responsible for its data and changes to this data shall only happen via the component. Otherwise maintenance problems will arise to ensure that data remains consistent. Therefore interfaces of a component that may be used by other components are designed call-by-value and not call-by-reference.\nHomogeneity\nSolve similar problems in similar ways and establish a uniform code-style.\n6.1.3. Application Architecture\nFor the architecture of an application we distinguish the following views:\nThe Business Architecture describes an application from the business perspective. It divides the application into business components and with full abstraction of technical aspects.\nThe Technical Architecture describes an application from the technical implementation perspective. It divides the application into technical layers and defines which technical products and frameworks are used to support these layers.\nThe Infrastructure Architecture describes an application from the operational infrastructure perspective. It defines the nodes used to run the application including clustering, load-balancing and networking. This view is not explored further in this guide.\nBusiness Architecture\nThe business architecture divides the application into business components. A business component has a well-defined responsibility that it encapsulates. All aspects related to that responsibility have to be implemented within that business component. Further the business architecture defines the dependencies between the business components. These dependencies need to be free of cycles. A business component exports his functionality via well-defined interfaces as a self-contained API. A business component may use another business component via its API and compliant with the dependencies defined by the business architecture.\nAs the business domain and logic of an application can be totally different, the devonfw can not define a standardized business architecture. Depending on the business domain it has to be defined from scratch or from a domain reference architecture template. For very small systems it may be suitable to define just a single business component containing all the code.\nTechnical Architecture\nThe technical architecture divides the application into technical layers based on the multilayered architecture. A layer is a unit of code with the same category such as service or presentation logic. A layer is therefore often supported by a technical framework. Each business component can therefore be split into component parts for each layer. However, a business component may not have component parts for every layer (e.g. only a presentation part that utilized logic from other components).\nAn overview of the technical reference architecture of the devonfw is given by figure &quot;Technical Reference Architecture&quot;.\nIt defines the following layers visualized as horizontal boxes:\nclient layer for the front-end (GUI).\nservice layer for the services used to expose functionality of the\nback-end to the client or other consumers.\nbatch layer for exposing functionality in batch-processes (e.g. mass imports).\nlogic layer for the business logic.\ndata-access layer for the data access (esp. persistence).\nAlso you can see the (business) components as vertical boxes (e.g. A and X) and how they are composed out of component parts each one assigned to one of the technical layers.\nFurther, there are technical components for cross-cutting aspects grouped by the gray box on the left. Here is a complete list:\nSecurity\nLogging\nMonitoring\nTransaction-Handling\nException-Handling\nInternationalization\nDependency-Injection\nFigure 23. Technical Reference Architecture\nWe reflect this architecture in our code as described in our coding conventions allowing a traceability of business components, use-cases, layers, etc. into the code and giving\ndevelopers a sound orientation within the project.\nFurther, the architecture diagram shows the allowed dependencies illustrated by the dark green connectors.\nWithin a business component a component part can call the next component part on the layer directly below via a dependency on its API (vertical connectors).\nWhile this is natural and obvious it is generally forbidden to have dependencies upwards the layers\nor to skip a layer by a direct dependency on a component part two or more layers below.\nThe general dependencies allowed between business components are defined by the business architecture.\nIn our reference architecture diagram we assume that the business component X is allowed to depend\non component A. Therefore a use-case within the logic component part of X is allowed to call a\nuse-case from A via a dependency on the component API. The same applies for dialogs on the client layer.\nThis is illustrated by the horizontal connectors. Please note that persistence entities are part of the API of the data-access component part so only the logic component part of the same\nbusiness component may depend on them.\nThe technical architecture has to address non-functional requirements:\nscalability\nis established by keeping state in the client and making the server state-less (except for login session). Via load-balancers new server nodes can be added to improve performance (horizontal scaling).\navailability and reliability\nare addressed by clustering with redundant nodes avoiding any single-point-of failure. If one node fails the system is still available. Further the software has to be robust so there are no dead-locks or other bad effects that can make the system unavailable or not reliable.\nsecurity\nis archived in the devonfw by the right templates and best-practices that avoid vulnerabilities. See security guidelines for further details.\nperformance\nis obtained by choosing the right products and proper configurations. While the actual implementation of the application matters for performance a proper design is important as it is the key to allow performance-optimizations (see e.g. caching).\nTechnology Stack\nThe technology stack of the devonfw is illustrated by the following table.\nTable 2. Technology Stack of devonfw\nTopic\nDetail\nStandard\nSuggested implementation\nruntime\nlanguage &amp; VM\nJava\nOracle JDK\nruntime\nservlet-container\nJEE\ntomcat\ncomponent management\ndependency injection\nJSR330 &amp; JSR250\nspring\nconfiguration\nframework\n-\nspring-boot\npersistence\nOR-mapper\nJPA\nhibernate\nbatch\nframework\nJSR352\nspring-batch\nservice\nSOAP services\nJAX-WS\nCXF\nservice\nREST services\nJAX-RS\nCXF\nlogging\nframework\nslf4j\nlogback\nvalidation\nframework\nbeanvalidation/JSR303\nhibernate-validator\nsecurity\nAuthentication &amp; Authorization\nJAAS\nspring-security\nmonitoring\nframework\nJMX\nspring\nmonitoring\nHTTP Bridge\nHTTP &amp; JSON\njolokia\nAOP\nframework\ndynamic proxies\nspring AOP\n6.2. Components\nFollowing separation-of-concerns we divide an application into components using our package-conventions and architecture-mapping.\nAs described by the architecture each component is divided into these layers:\nclient-layer with the dialogs to view and modify the component&#x2019;s data.\nservice-layer with the services to access the component&#x2019;s data remotely.\nlogic-layer with the component-facade providing the business-logic to manage the component&#x2019;s data.\ndataaccess-layer with the entities defining and the repositories (or DAOs) accessing the component&#x2019;s data.\nPlease note that only CRUD oriented components will have all four layers within the same component.\nSome types of applications may have completely different components for the client.\n6.2.1. General Component\nCross-cutting aspects belong to the implicit component general. It contains technical configurations and very general code that is not business specific. Such code shall not have any dependencies to other components and therefore business related code.\n6.2.2. Business Component\nThe business-architecture defines the business components with their allowed dependencies. A small application (microservice) may just have one component and no dependencies making it simple while the same architecture can scale up to large and complex applications (from bigger microservice up to modulith).\nTailoring an business domain into applications and applications into components is a tricky task that needs the skills of an experienced architect.\nAlso the tailoring should follow the business and not split by technical reasons or only by size.\nSize is only an indicator but not a driver of tailoring.\nWhatever hypes like microservices are telling you, never get mislead in this regard:\nIf your system grows and reaches MAX+1 lines of code, it is not the right motivation to split it into two microservices of ~MAX/2 lines of code - such approaches will waste huge amounts of money and lead to chaos.\n6.2.3. App Component\nOnly in case you need cross-cutting code that aggregates other component you may introduce the component app.\nIt is allowed to depend on all other components but no other component may depend on it.\nWith the modularity and flexibility of spring you typically do not need this.\nHowever, when you need to have a class that registers all services or component-facades using direct code dependencies, you can introduce this component.\n6.2.4. Component Example\nThe following class diagram illustrates an example of the business component Staffmanagement:\nHere you can see the structure and flow from the service-layer (REST service call) via the logic-layer to the dataaccess-layer (and back).\n&#x2191;&#xA0;Up:&#xA0;devon4j&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Coding&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devon4j.asciidoc_layers.html","title":"8. Layers","body":"\n8. Layers\n8.1. Client Layer\nThere are various technical approaches to building GUI clients. The devonfw proposes rich clients that connect to the server via data-oriented services (e.g. using REST with JSON).\nIn general, we have to distinguish among the following types of clients:\nweb clients\nnative desktop clients\n(native) mobile clients\nOur main focus is on web-clients. In our sample application my-thai-star we offer a responsive web-client based on Angular following devon4ng that integrates seamlessly with the back ends of my-thai-star available for Java using devon4j as well as .NET/C# using devon4net. For building angular clients read the separate devon4ng guide.\n8.1.1. JavaScript for Java Developers\nIn order to get started with client development as a Java developer we give you some hints to get started. Also if you are an experienced JavaScript developer and want to learn Java this can be helpful. First, you need to understand that the JavaScript ecosystem is as large as the Java ecosystem and developing a modern web client requires a lot of knowledge. The following table helps you as experienced developer to get an overview of the tools, configuration-files, and other related aspects from the new world to learn. Also it helps you to map concepts between the ecosystems. Please note that we list the tools recommended by devonfw here (and we know that there are alternatives not listed here such as gradle, grunt, bower, etc.).\nTable 5. Aspects in JavaScript and Java ecosystem\nTopic\nAspect\nJavaScript\nJava\nProgramming\nLanguage\nTypeScript (extends JavaScript)\nJava\nRuntime\nVM\nnodejs (or web-browser)\njvm\nDependency-Management\nTool\nyarn (or npm)\nmaven\nConfig\npackage.json\npom.xml\nRepository\nnpm repo\nmaven central (repo search)\nBuild-Management\nTaskrunner\ngulp\nmaven (or more comparable ant)\nConfig\ngulpfile.js (and gulp/*)\npom.xml (or build.xml)\nClean cmd\ngulp clean\nmvn clean\nBuild cmd\nyarn install &amp;&amp; gulp build:dist\nmvn install (see lifecycle)\nTest cmd\ngulp test\nmvn test\nTesting\nTest-Tool\njasmine\njunit\nTest-Framework\nkarma\njunit / surefire\nBrowser Testing\nPhantomJS\nSelenium\nExtensions\nkarma-*, PhantomJs for browser emulation\nAssertJ,*Unit and spring-test, etc.)\nCode Analysis\nCode Coverage\nkarma-coverage (and remap-istanbul for TypeScript)\nJaCoCo/EclEmma\nDevelopment\nIDE\nMS VS Code or IntelliJ\nEclipse or IntelliJ\nFramework\nAngular (etc.)\nSpring (etc.)\n8.2. Service Layer\nThe service layer is responsible for exposing functionality made available by the logical layer to external consumers over a network via technical protocols.\n8.2.1. Types of Services\nWe distinguish between the following types of services:\nExternal Services\nare used for communication between different companies, vendors, or partners.\nInternal Services\nare used for communication between different applications in the same application landscape of the same vendor.\nBack-end Services\nare internal services between Java back-end components typically with different release and deployment cycles (if not Java consider this as external service).\nJS-Client Services\nare internal services provided by the Java back-end for JavaScript clients (GUI).\nJava-Client Services\nare internal services provided by the Java back-end for a native Java client (JavaFx, EclipseRcp, etc.).\nThe choices for technology and protocols will depend on the type of service. The following table gives a guideline for aspects according to the service types.\nTable 6. Aspects according to service-type\nAspect\nExternal Service\nBack-end Service\nJS-Client Service\nJava-Client Service\nVersioning\nrequired\nrequired\nnot required\nnot required\nInteroperability\nmandatory\nnot required\nimplicit\nnot required\nRecommended Protocol\nSOAP or REST\nREST\nREST+JSON\nREST\n8.2.2. Versioning\nFor services consumed by other applications we use versioning to prevent incompatibilities between applications when deploying updates. This is done by the following conventions:\nWe define a version number and prefix it with v (e.g. v1).\nIf we support previous versions we use that version numbers as part of the Java package defining the service API (e.g. com.foo.application.component.service.api.v1)\nWe use the version number as part of the service name in the remote URL (e.g. https://application.foo.com/services/rest/component/v1/resource)\nWhenever breaking changes are made to the API, create a separate version of the service and increment the version (e.g. v1 &#x2192; v2) . The implementations of the different versions of the service contain compatibility code and delegate to the same unversioned use-case of the logic layer whenever possible.\nFor maintenance and simplicity, avoid keeping more than one previous version.\n8.2.3. Interoperability\nFor services that are consumed by clients with different technology, interoperability is required. This is addressed by selecting the right protocol, following protocol-specific best practices and following our considerations especially simplicity.\n8.2.4. Service Considerations\nThe term service is quite generic and therefore easily misunderstood. It is a unit exposing coherent functionality via a well-defined interface over a network. For the design of a service, we consider the following aspects:\nself-contained\nThe entire API of the service shall be self-contained and have no dependencies on other parts of the application (other services, implementations, etc.).\nidempotence\nE.g. creation of the same master-data entity has no effect (no error)\nloosely coupled\nService consumers have minimum knowledge and dependencies on the service provider.\nnormalized\ncomplete, no redundancy, minimal\ncoarse-grained\nService provides rather large operations (save entire entity or set of entities rather than individual attributes)\natomic\nProcess individual entities (for processing large sets of data, use a batch instead of a service)\nsimplicity\navoid polymorphism, RPC methods with unique name per signature and no overloading, avoid attachments (consider separate download service), etc.\n8.2.5. Security\nYour services are the major entry point to your application. Hence security considerations are important here.\nSee REST Security.\n8.3. Logic Layer\nThe logic layer is the heart of the application and contains the main business logic.\nAccording to our business architecture we divide an application into components.\nFor each component the logic layer defines a component-facade.\nAccording to the complexity you can further divide this into individual use-cases.\nIt is very important that you follow the links to understand the concept of component-facade and use-case in order to properly implement your business logic.\n8.3.1. Responsibility\nThe logic layer is responsible for implementation the business logic according to the specified functional demands and requirements.\nIt therefore creates the actual value of the application.\nThe following additional aspects are also in its responsibility:\nvalidation\nauthorization\ntransaction-handling (in addition to service layer).\n8.3.2. Security\nThe logic layer is the heart of the application. It is also responsible for authorization and hence security is important here. Every method exposed in an interface needs to be annotated with an authorization check, stating what role(s) a caller must provide in order to be allowed to make the call. The authorization concept is described here.\nDirect Object References\nA security threat are Insecure Direct Object References. This simply gives you two options:\navoid direct object references at all\nensure that direct object references are secure\nEspecially when using REST, direct object references via technical IDs are common sense. This implies that you have a proper authorization in place. This is especially tricky when your authorization does not only rely on the type of the data and according static permissions but also on the data itself. Vulnerabilities for this threat can easily happen by design flaws and inadvertence. Here is an example from our sample application:\nWe have a generic use-case to manage BLOBs. In the first place it makes sense to write a generic REST service to load and save these BLOBs. However, the permission to read or even update such BLOB depend on the business object hosting the BLOB. Therefore, such a generic REST service would open the door for this OWASP A4 vulnerability. To solve this in a secure way, you need individual services for each hosting business object to manage the linked BLOB and have to check permissions based on the parent business object. In this example the ID of the BLOB would be the direct object reference and the ID of the business object (and a BLOB property indicator) would be the indirect object reference.\n8.4. Component Facade\nFor each component of the application the logic layer defines a component facade.\nThis is an interface defining all business operations of the component.\nIt carries the name of the component (&#xAB;Component&#xBB;) and has an implementation named &#xAB;Component&#xBB;Impl (see implementation).\n8.4.1. API\nThe component facade interface defines the logic API of the component and has to be business oriented.\nThis means that all parameters and return types of all methods from this API have to be business transfer-objects, datatypes (String, Integer, MyCustomerNumber, etc.), or collections of these.\nThe API may also only access objects of other business components listed in the (transitive) dependencies of the business-architecture.\nHere is an example how such an API may look like:\npublic interface Bookingmanagement {\nBookingEto findBooking(Long id);\nBookingCto findBookingCto(Long id);\nPage&lt;BookingEto&gt; findBookingEtos(BookingSearchCriteriaTo criteria);\nvoid approveBooking(BookingEto booking);\n}\n8.4.2. Implementation\nThe implementation of an interface from the logic layer (a component facade or a use-case) carries the name of that interface with the suffix Impl and is annotated with @Named.\nAn implementation typically needs access to the persistent data.\nThis is done by injecting the corresponding repository (or DAO).\nAccording to data-sovereignty, only repositories of the same business component may be accessed directly.\nFor accessing data from other components the implementation has to use the corresponding API of the logic layer (the component facade). Further, it shall not expose persistent entities from the dataaccess layer and has to map them to transfer objects using the bean-mapper.\n@Named\n@Transactional\npublic class BookingmanagementImpl extends AbstractComponentFacade implements Bookingmanagement {\n@Inject\nprivate BookingRepository bookingRepository;\n@Override\npublic BookingEto findBooking(Long id) {\nLOG.debug(&quot;Get Booking with id {} from database.&quot;, id);\nBookingEntity entity = this.bookingRepository.findOne(id);\nreturn getBeanMapper().map(entity, BookingEto.class));\n}\n}\nAs you can see, entities (BookingEntity) are mapped to corresponding ETOs (BookingEto).\nFurther details about this can be found in bean-mapping.\nFor complex applications, the component facade consisting of many different methods.\nFor better maintainability in such case it is recommended to split it into separate use-cases that are then only aggregated by the component facade.\n8.5. UseCase\nA use-case is a small unit of the logic layer responsible for an operation on a particular entity (business object).\nIt is defined by an interface (API) with its according implementation.\nFollowing our architecture-mapping use-cases are named Uc&#xAB;Operation&#xBB;&#xAB;BusinessObject&#xBB;[Impl]. The prefix Uc stands for use-case and allows to easily find and identify them in your IDE. The &#xAB;Operation&#xBB; stands for a verb that is operated on the entity identified by &#xAB;BusinessObject&#xBB;.\nFor CRUD we use the standard operations Find and Manage that can be generated by CobiGen. This also separates read and write operations (e.g. if you want to do CQSR, or to configure read-only transactions for read operations).\n8.5.1. Find\nThe UcFind&#xAB;BusinessObject&#xBB; defines all read operations to retrieve and search the &#xAB;BusinessObject&#xBB;.\nHere is an example:\npublic interface UcFindBooking {\nBookingEto findBooking(Long id);\nBookingCto findBookingCto(Long id);\nPage&lt;BookingEto&gt; findBookingEtos(BookingSearchCriteriaTo criteria);\nPage&lt;BookingCto&gt; findBookingCtos(BookingSearchCriteriaTo criteria);\n}\n8.5.2. Manage\nThe UcManage&#xAB;BusinessObject&#xBB; defines all CRUD write operations (create, update and delete) for the &#xAB;BusinessObject&#xBB;.\nHere is an example:\npublic interface UcManageBooking {\nBookingEto saveBooking(BookingEto booking);\nboolean deleteBooking(Long id);\n}\n8.5.3. Custom\nAny other non CRUD operation Uc&#xAB;Operation&#xBB;&#xAB;BusinessObject&#xBB; uses any other custom verb for &#xAB;Operation&#xBB;.\nTypically such custom use-cases only define a single method.\nHere is an example:\npublic interface UcApproveBooking {\nvoid approveBooking(BookingEto booking);\n}\n8.5.4. Implementation\nFor the implementation of a use-case the same rules apply that are described for the component-facade implementation.\nHowever, when following the use-case approach, your component facade simply changes to:\npublic interface Bookingmanagement extends UcFindBooking, UcManageBooking, UcApproveBooking {\n}\nWhere the implementation only delegates to the use-cases and gets entirely generated by CobiGen:\npublic class BookingmanagementImpl implements {\n@Inject\nprivate UcFindBooking ucFindBooking;\n@Inject\nprivate UcManageBooking ucManageBooking;\n@Inject\nprivate UcApproveBooking ucApproveBooking;\n@Override\npublic BookingEto findBooking(Long id) {\nreturn this.ucFindBooking.findBooking(id);\n}\n@Override\npublic Page&lt;BookingEto&gt; findBookingEtos(BookingSearchCriteriaTo criteria) {\nreturn this.ucFindBooking.findBookingEtos(criteria);\n}\n@Override\npublic BookingEto saveBooking(BookingEto booking) {\nreturn this.ucManageBooking.saveBooking(booking);\n}\n@Override\npublic boolean deleteBooking(Long id) {\nreturn this.ucManageBooking.deleteBooking(booking);\n}\n@Override\npublic void approveBooking(BookingEto booking) {\nthis.ucApproveBooking.approveBooking(booking);\n}\n...\n}\nThis approach is also illustrated by the following UML diagram:\n8.5.5. Internal use case\nSometimes a component with multiple related entities and many use-cases needs to reuse business logic internally.\nOf course this can be exposed as official use-case API but this will imply using transfer-objects (ETOs) instead of entities. In some cases this is undesired e.g. for better performance to prevent unnecessary mapping of entire collections of entities.\nIn the first place you should try to use abstract base implementations providing reusable methods the actual use-case implementations can inherit from.\nIf your business logic is even more complex and you have multiple aspects of business logic to share and reuse but also run into multi-inheritance issues, you may also just create use-cases that have their interface located in the impl scope package right next to the implementation (or you may just skip the interface). In such case you may define methods that directly take or return entity objects.\nTo avoid confusion with regular use-cases we recommend to add the Internal suffix to the type name leading to Uc&#xAB;Operation&#xBB;&#xAB;BusinessObject&#xBB;Internal[Impl].\n8.5.6. Injection issues\nTechnically now you have two implementations of your use-case:\nthe direct implementation of the use-case (Uc*Impl)\nthe component facade implementation (&#xAB;Component&#xBB;Impl)\nWhen injecting a use-case interface this could cause ambiguities.\nThis is addressed as following:\nIn the component facade implementation (&#xAB;Component&#xBB;Impl) spring is smart enough to resolve the ambiguity as it assumes that a spring bean never wants to inject itself (can already be access via this).\nTherefore only the proper use-case implementation remains as candidate and injection works as expected.\nIn all other places simply always inject the component facade interface instead of the use-case.\nIn case you might have the lucky occasion to hit this nice exception:\norg.springframework.beans.factory.BeanCurrentlyInCreationException: Error creating bean with name &apos;uc...Impl&apos;: Bean with name &apos;uc...Impl&apos; has been injected into other beans [...Impl] in its raw version as part of a circular reference, but has eventually been wrapped. This means that said other beans do not use the final version of the bean. This is often the result of over-eager type matching - consider using &apos;getBeanNamesOfType&apos; with the &apos;allowEagerInit&apos; flag turned off, for example.\nTo get rid of such error you need to annotate your according implementation also with @Lazy in addition to @Named.\n8.6. Data-Access Layer\nThe data-access layer is responsible for all outgoing connections to access and process data. This is mainly about accessing data from a persistent data-store but also about invoking external services.\n8.6.1. RDBMS\nThe classical approach is to use a Relational Database Management System (RDMS). In such case we strongly recommend to follow our JPA Guide. In case you are using Oracle you should also consider the Oracle guide.\n8.6.2. NoSQL\nIn case of specific demands and requirements you may want to choose for a Not only SQL database (NoSQL). There are different categories of such products so you should first be aware what fits your requirements best:\nkey/value DB\ndocument DB\ngraph DB\nwide-column DB\nAs there are many such products and the market is evolving very fast, we do not yet give clear recommendations here. If you are doing a devon project and consider NoSQL please contact us for further details.\n8.7. Batch Layer\nWe understand batch processing as bulk-oriented, non-interactive, typically long running execution of tasks. For simplicity we use the term batch or batch job for such tasks in the following documentation.\ndevonfw uses Spring Batch as batch framework.\nThis guide explains how Spring Batch is used in devonfw applications. Please note that it is not yet fully consistent concerning batches with the sample application. You should adhere to this guide by now.\n8.7.1. Batch architecture\nIn this chapter we will describe the overall architecture (especially concerning layering) and how to administer batches.\nLayering\nBatches are implemented in the batch layer. The batch layer is responsible for batch processes, whereas the business logic is implemented in the logic layer. Compared to the service layer you may understand the batch layer just as a different way of accessing the business logic.\nFrom a component point of view each batch is implemented as a subcomponent in the corresponding business component.\nThe business component is defined by the business architecture.\nLet&#x2019;s make an example for that. The sample application implements a batch for exporting bills. This billExport belongs to the salesmanagement business component.\nSo the billExport is implemented in the following package:\n&lt;basepackage&gt;.salesmanagement.batch.impl.billexport.*\nBatches should invoke use cases in the logic layer for doing their work.\nOnly &quot;batch specific&quot; technical aspects should be implemented in the batch layer.\nExample:\nFor a batch, which imports product data from a CSV file, this means that all code for actually reading and parsing the CSV input file is implemented in the batch layer.\nThe batch calls the use case &quot;create product&quot; in the logic layer for actually creating the products for each line read from the CSV input file.\nAccessing data access layer\nIn practice it is not always appropriate to create use cases for every bit of work a batch should do. Instead, the data access layer can be used directly.\nAn example for that is a typical batch for data retention which deletes out-of-time data.\nOften deleting out-dated data is done by invoking a single SQL statement. It is appropriate to implement that SQL in a Repository or DAO method and call this method directly from the batch.\nBut be careful that this pattern is a simplification which could lead to business logic cluttered in different layers which reduces maintainability of your application.\nIt is a typical design decision you have to take when designing your specific batches.\nBatch administration and execution\nStarting and Stopping Batches\nSpring Batch provides a simple command line API for execution and parameterization of batches, the CommandLineJobRunner. It is not yet fully compatible with Spring Boot, however. For those using Spring Boot, devonfw provides the SpringBootBatchCommandLine with similar functionalities.\nBoth execute batches as a &quot;simple&quot; standalone process (instantiating a new JVM and creating a new ApplicationContext).\nStarting a Batch Job\nFor starting a batch job, the following parameters are required:\njobPath(s)\nThe location of the JavaConfig classes (usually annotated with @Configuration or @SpringBootApplication) and/or XML files that will be used to create an ApplicationContext.\nThe CommandLineJobRunner only accepts one class/file, which must contain everything needed to run a job (potentially by referencing other classes/files), the SpringBootBatchCommandLine, however, expects that there are two paths given: one for the general batch setup and one for the XML file containing the batch job to be executed.\nThere is an example of a general batch setup for Spring Boot in the my-thai-star batch module. The main class is SpringBootBatchApp, which also imports the general configuration class introduced in the chapter on the general configuration. Note that SpringBootBatchApp deactivates the evaluation of annotations used for authorization, especially the @RolesAllowed annotation. You should of course make sure that only authorized users can start batches, but once the batch is started there is usually no need to check any authorization.\njobName\nThe name of the job to be run.\nAll arguments after the job name are considered to be job parameters and must be in the format of name=value:\nExample for the CommandLineJobRunner:\njava org.springframework.batch.core.launch.support.CommandLineJobRunner classpath:config/app/batch/beans-billexport.xml billExportJob -outputFile=file:out.csv date(date)=2015/12/20\nExample for the SpringBootBatchCommandLine:\njava com.devonfw.module.batch.common.base.SpringBootBatchCommandLine com.devonfw.application.mtsj.SpringBootBatchApp classpath:config/app/batch/beans-billexport.xml billExportJob -outputFile=file:out.csv date(date)=2015/12/20\nThe date parameter will be explained in the section on parameters.\nNote that when a batch is started with the same parameters as a previous execution of the same batch job, the new execution is considered a restart, see restarts for further details. Parameters starting with a &quot;-&quot; are ignored when deciding whether an execution is a restart or not (so called non identifying parameters).\nWhen trying to restart a batch that was already complete, there will either be an exception (message: &quot;A job instance already exists and is complete for parameters={&#x2026;&#x200B;}. If you want to run this job again, change the parameters.&quot;) or the batch will simply do nothing (might happen when no or only non identifying parameters are set; in this case the console log contains the following message for every step: &quot;Step already complete or not restartable, so no action to execute: &#x2026;&#x200B;&quot;).\nStopping a Job\nThe command line option to stop a running execution is as follows:\njava org.springframework.batch.core.launch.support.CommandLineJobRunner classpath:config/app/batch/beans-billexport.xml &#x2013;stop billExportJob\nor\njava com.devonfw.module.batch.common.base.SpringBootBatchCommandLine com.devonfw.application.mtsj.SpringBootBatchApp classpath:config/app/batch/beans-billexport.xml billExportJob &#x2013;stop\nNote that the job is not shutdown immediately, but might actually take some time to stop.\nScheduling\nIn real world scheduling of batches is not as simple as it first might look like.\nMultiple batches have to be executed in order to achieve complex tasks. If one of those batches fails the further execution has to be stopped and operations should be notified for example.\nInput files or those created by batches have to be copied from one node to another.\nScheduling batch executing could get complex easily (quarterly jobs, run job on first workday of a month, &#x2026;&#x200B;)\nFor devonfw we propose the batches themselves should not mess around with details of batch administration.\nLikewise your application should not do so.\nBatch administration should be externalized to a dedicated batch administration service or scheduler.\nThis service could be a complex product or a simple tool like cron. We propose Rundeck as an open source job scheduler.\nThis gives full control to operations to choose the solution which fits best into existing administration procedures.\n8.7.2. Implementation\nIn this chapter we will describe how to properly setup and implement batches.\nMain Challenges\nAt a first glimpse, implementing batches is much like implementing a backend for client processing.\nThere are, however, some points at which batches have to be implemented totally different. This is especially true if large data volumes are to be processed.\nThe most important points are:\nTransaction handling\nFor processing request made by clients there is usually one transaction for each request. If anything goes wrong, the transaction is rolled back and all changes are reverted.\nA naive approach for batches would be to execute a whole batch in one single transaction so that if anything goes wrong, all changes are reverted and the batch could start from scratch. For processing large amounts of data, this is technically not feasible, because the database system would have to be able to undo every action made within this transaction. And the space for storing the undo information needed for this (the so called &quot;undo tablespace&quot;) is usually quite limited.\nSo there is a need of short running transactions. To help programmers to do so, Spring Batch offers the so called chunk processing which will be explained here.\nRestarting Batches\nIn client processing mode, when an exception occurs, the transaction is rolled back and there is no need to worry about data inconsistencies.\nThis is not true for batches however, due to the fact that you usually can&#x2019;t have just one transaction. When an unexpected error occurs and the batch aborts, the system is in a state where the data is partly processed and partly not and there needs to be some sort of plan on how to continue from there.\nEven if a batch was perfectly reliable, there might be errors that are not under the control of the application, e.g. lost connection to the database, so that there is always a need for being able to restart.\nThe section on restarts describes how to design a batch that is restartable. What&#x2019;s important is that a programmer has to invest some time upfront for a batch to be able restart after aborts.\nException handling in Batches\nThe problem with exception handling is that a single record can cause a whole batch to fail and many records will remain unprocessed. In contrast to this, in client processing mode when processing fails this usually affects only one user.\nTo prevent this situation, Spring Batch allows to skip data when certain exceptions occur. However, the feature should not be misused in a way that you just skip all exceptions independently of their cause.\nSo when implementing a batch, you should think about what exceptional situations might occur and how to deal with that and weather it is okay to skip those exceptions or not. When an unexpected exception occurs, the batch should still fail so that this exception is not ignored but its causes are analyzed.\nAnother way of handling exceptions in batches is retrying: Simply try to process the data once more and hope that everything works well this time. This approach often works for database problems, e.g. timeouts.\nThe section on exception handling explains skipping and retrying in more detail.\nNote that exceptions are another reason why you should not execute a whole batch in one transaction. If anything goes wrong, you could either rollback the transaction and start the batch from scratch or you could manually revert all relevant changes. Both are not very good solutions.\nPerformance issues\nIn client processing mode, optimizing throughput (and response times) is an important topic as well, of course.\nHowever, a performance that is still considered okay for client processing might be problematic for batches as these usually have to process large volumes of data and the time for their execution is usually quite limited (batches are often executed at night when no one is using the application).\nAn example: If processing the data of one person takes a second, this is usually still considered OK for client processing (even though performance could be better). However if a batch has to process the data of 100.000 persons in one night and is not executed with multiple threads, this takes roughly 28 hours, which is by far too much.\nThe section on performance contains some tips on how to deal with performance problems.\nSetup\nDatabase\nSpring Batch needs some meta data tables for monitoring batch executions and for restoring state for restarts. Detailed description about needed tables, sequences and indexes can be found in Spring Batch - Reference Documentation: Appendix B. Meta-Data Schema.\nIt is not recommended to add additional meta data tables, because this easily leads to inconsistencies with what is stored in those tables maintained by Spring Batch.\nYou should rather try to extract all needed information out of the standard tables in case the standard API (especially JobRepository and JobExplorer, see below) does not fit your needs.\nFailure information\nBATCH_JOB_EXECUTION.EXIT_MESSAGE and BATCH_STEP_EXECUTION.EXIT_MESSAGE store a detailed description of how the job exited. In the case of failure, this might include as much of the stack trace as is possible.\nBATCH_STEP_EXECUTION_CONTEXT.SHORT_CONTEXT stores a stringified version of the step&#x2019;s ExecutionContext (see saving and restoring state, the rest is stored in a BLOB if needed).\nThe default length of those columns in the sample schema scripts is 2500.\nIt is good to increase the length of those columns as far as the database allows it to make it easier to find out which exception failed a batch (not every exception causes a failure, see exception handling). Some JDBC drivers cast CLOBs to string automatically. If this is the case, you can use CLOBs instead.\nGeneral Configuration\nFor configuring batches, we recommend not to use annotations (would not work very well for batches) or JavaConfig, but XML, because this makes the whole batch configuration more transparent, as its structure and implementing beans are immediately visible. Moreover the Spring Batch documentation focuses rather on XML based configurations than on JavaConfig.\nFor explanations on how these XML files are build in general, have a look at the spring documentation.\nThere is, however, some general configuration needed for all batches, for which we use JavaConfig, as it is also used for the setup of all other layers. You can find an example of such a configuration in the samples/core project: BeansBatchConfig. In this section, we will explain the most important parts of this class.\nThe jobRepository is used to update the meta data tables.\nThe database type can optionally be set on the jobRepository for correctly handling database specific things using the setDatabaseType method. Possible values are oracle, mysql, postgres etc.\nIf the size of all three columns, which by default have a length limitation of 2500, has been increased as proposed here, the property maxVarCharLength should be adjusted accordingly using the corresponding setter method in order to actually utilize the additional space.\nThe jobExplorer offers methods for reading from the meta data tables in addition to those methods provided by the jobRepository, e.g. getting the last executions of a batch.\nThe jobLauncher is used to actually start batches.\nWe use our own implementation (JobLauncherWithAdditionalRestartCapabilities) here, which can be found in the module modules/batch (devon4j-batch). It enables a special form of restarting a batch (&quot;restart from scratch&quot;, see the section on restarts for further details).\nThe jobRegistry is basically a map, which contains all batch jobs. It is filled by the bean of type JobRegistryBeanPostProcessor automatically.\nA JobParametersIncremeter (bean incrementer) can be used to generate unique parameters, see restarts and parameters for further details. It should be configured manually for each batch job, see example batch below, otherwise exceptions might occur when starting batches.\nExample-Batch\nAs already mentioned, every batch job consists of one or more batch steps, which internally either use chunk processing or tasklet based processing.\nOur bill export batch job consists of the following to steps:\nRead all (not processed) bills from the database, mark them as processed (additional attribute) and write them into a CSV file (to be further processed by other systems). This step is implemented using chunk processing (see chunk processing).\nDelete all bill from the database which are marked as processed. This step is implemented in a tasklet (see tasklet based processing).\nNote that you could also delete the bills directly. However, for being able to demonstrate tasklet based processing, we have created a separate step here.\nAlso note that in real systems you would usually create a backup of data as important as bills, which is not done here.\nThe beans-billexport.xml configures the batch for exporting the bills.\nAs you can see, there is a job element (billExportJob), which contains the two step elements (createCsvFile and deleteBills). Note that for every step you have to explicitly specify which step comes next (using the next attribute), unless it is the last step.\nThe step elements always contains a tasklet element, even if chunk processing is used. The transaction-attributes element is especially used to set timeout of transactions (in seconds). Note that there is usually more than one transaction per step (see below).\nWhat follows is either a chunk element with ItemReader, ItemProcessor, ItemWriter and a commit interval (see chunk processing) or the tasklet element containing a reference to a tasklet.\nIn the example above the ItemReader named unprocessedBillsReader always reads 1000 ids of unprocessed bills (via a DAO) and returns them one after another. The ItemProcessor processedMarker reads the corresponding bills from the database (see chunk processing why we do not read them directly in the ItemReader) and marks them as processed. The ItemWriter csvFileWriter (see below on how this writer is configured) writes them to a CSV file. The path of this file is provided as batch parameter (outputFile).\nThe tasklet billsDeleter deletes all processed bills (10.000 in one transaction).\nThe chunkLoggingListener, which is also used in the example above, can be utilized for all chunk steps to log exceptions together with the items where these exceptions occurred (see listeners for further details on listeners). It&#x2019;s implementation can be found in the module modules/batch. Note that classes used for items have to have an appropriate toString() method in order for this listener to be useful.\nRestarts\nA batch execution is considered a restart, if it was run already (with the same parameters) and there was a (non skippable) failure or the batch has been stopped.\nThere are basically two ways to do a restart:\nUndo all changes and restart from scratch.\nRestore the state of that batch at the time the error occurred and continue processing.\nThe first approach has two major disadvantages:\nOne is that depending on what the batch does, reverting all of its changes can get quite complex. And you easily end up having implemented a batch that is restartable, but not if it fails in the wrong step.\nThe second disadvantage is that if a batch runs for several hours and then it fails it has to start all over again. And as the time for executing batches is usually quite limited, this can be problematic.\nIf reverting all changes is as easy as deleting all files in a given directory or something like that and the expected duration for an execution of the batch is rather short, you might consider the option of always starting at the beginning, otherwise you shouldn&#x2019;t.\nSpring Batch supports implementing the second option. By default, if a batch is restarted with the same parameters as a previous execution of this batch, then this new execution continues processing at the step where the last execution was stopped or failed. If the last execution was already complete, an exception is raised.\nThe step itself has to be implemented in a way so that it can restore its internal state, which is the main drawback of this second option.\nHowever, there are &apos;standard implementations&apos; that are capable of doing so and these can easily be adapted to your needs. They are introduced in the section on chunk processing.\nFor instructing Spring Batch to always restart a batch at the very beginning even though there has been an execution of this batch with the same parameters already, set the restartable attribute of the Job element to false.\nBy default, setting this attribute to false means that the batch is not restartable (i.e. it cannot be started with the same parameters once more). It would raise an error if there was attempt to do so, so that it cannot be restarted where it left off.\nWe use our own JobLauncher (JobLauncherWithAdditionalRestartCapabilities) as described in the section on the general configuration to modify this behavior so that those batches are always restarted from the first step on by adding an extra parameter (instead of raising an exception), so that you do not have to take care of that yourself. So don&#x2019;t think of a batch marked with restartable=&quot;false&quot; as a batch that is not restartable (as most people would probably assume just looking at the attribute) but as a batch that restarts always from the first step on.\nNote that if a batch is restartable by restoring its internal state, it might not work correctly if the batch is started with different parameters after it failed, which usually comes down to the same thing as restating it from scratch. So, the batch has to be restarted and completed successfully before executing the next regular &apos;run&apos;. When scheduling batches, you should make that sure.\nChunk Processing\nChunk processing is item based processing. Items can be bills, persons or whatever needs to be processed. Those items are grouped into chunks of a fixed size and all items within such a chunk are processed in one transaction. There is not one transaction for every single (small) item because there would be too many commits which degrades performance.\nAll items of a chunk are read by an ItemReader (e.g. from a file or from database), processed by an ItemProcessor (e.g. modified or converted) and written out as a whole by an ItemWriter (e.g. to a file or to database).\nThe size of a chunk is also called commit interval. One has to be careful , while choosing a large chunk size: When a skip or retry occurs for a single item (see exception handling), the current transaction has to be rolled back and all items of the chunk have to be reprocessed. This is especially a problem when skips and retries occur more often and results in long runtimes.\nThe most important advantages of chunk processing are:\ngood trade-off between size and number of transactions (configurable via commit size)\ntransaction timeouts that do not have to be adapted for larger amounts of data that needs to be processed (as there is always one transaction for a fixed number of items)\nan exception handling that is more fain-grained than aborting/restarting the whole batch (item based skipping and retrying, see exception handling)\nlogging items where exceptions occurred (which makes failure analysis much more easy)\nNote that you could actually achieve similar results using tasklets as described below. However, you would have to write many lines of additional code whereas you get these advantages out of the box using chunk processing (logging exceptions and items where these exceptions occurred is an extension, see example batch).\nAlso note that items should not be too &quot;big&quot;. For example, one might consider processing all bills within one month as one item. However, doing so you would not have those advantages any more. For instance, you would have larger transactions, as there are usually quite a lot of bills per month or payment method and if an exception occurs, you would not know which bill actually caused the exception. Additionally you would lose control of commit size, since one commit would process many bills hard coded and you cannot choose smaller chunks.\nNevertheless, there are sometimes, situations where you cannot further &quot;divide&quot; items, e.g. when these are needed for one single call to an external system (e.g. for creating a PDF of all bills within a certain month, if PDFs are created by an external system). In this case you should do as much of the processing as possible on the basis of &quot;small&quot; items and then add an extra step to do what cannot be done based on these &quot;small&quot; items.\nItemReader\nA reader has to implement the ItemReader interface, which has the following method:\npublic T read() throws Exception;\nT is a type parameter of the ItemReader interface to be replaced with the type of items to be read.\nThe method returns all items (one at a time) that need to be processed or null if there are no more items.\nIf an exception occurs during read, Spring Batch cannot tell which item caused the exception (as it has not been read yet). That is why a reader should contain as little processing logic as possible, minimizing the potential for failures.\nCaching\nBy default, all items read by an ItemReader are cached by Spring Batch. This is useful because when a skippable exception occurs during processing of a chunk, all items (or at least those, that did not cause the exception) have to be reprocessed. These items are not read twice but taken from the cache then.\nThis is often necessary, because if a reader saves it&#x2019;s current state in member variables (e.g. the current position within a list of items) or uses some sort of cursor, these will be updated already and the next calls of the read method would deliver the next items ready and not those that have to be reprocessed.\nHowever this also means that when the items read by an ItemReader are entities, these might be detached, because these might have been read in a different transaction. In some standard implementations Spring Batch even manually detaches entities in ItemReaders.\nIn case these entities are to be modified it is a good practice that the ItemReader only reads IDs and the ItemProcessor loads the entities for these IDs to avoid the problem.\nReading from Transactional Queues\nIn case the reader reads from a transactional queue (e.g. using JMS), you must not use caching, because then an item might get processed twice: Once from cache and once from queue to where it has been returned after the rollback. To achieve this, set reader-transactional-queue=&quot;true&quot; in the chunk element in the step definition.\nMoreover the equals and hashCode methods of the class used for items have to be appropriately implemented for Spring Batch to be able to identify items that were processed before unsuccessfully (causing a rollback and thereby returning them to the queue). Otherwise the batch might be caught in an infinite loop trying to process the same item over and over again (e.g. when the item is about to be skipped, see exception handling).\nReading from the Database\nWhen selecting data from a database, there is usually some sort of cursor used. One challenge is to make this cursor not participate in the chunk&#x2019;s transaction, because it would be closed after the first chunk.\nWe will show how to use JDBC based cursors for ItemReader implementations in later releases of this documentation.\nFor JPA/JPQL based queries, cursors cannot be used, because JPA does not know of the concept of a cursor. Instead it supports pagination as introduced in the chapter on the data access layer, which can be used for this purpose as well. Note that pagination requires the result set to be sorted in an unambiguous order to work reliably. The order itself is irrelevant as long as it does not change (you can e.g. sort the entities by their primary key).\nAn ItemReader using pagination should inherit from the AbstractPagingItemReader, which already provides most of the needed functionality. It manages the internal state, i.e. the current position, which can be correctly restored after a restart (when using an unambiguous order for the result set).\nClasses inheriting from AbstractPagingItemReader must implement two methods.\nThe method doReadPage() performs the actual read of a page. The result is not returned (return type is void) but used to replace the content of the &apos;results&apos; instance variable (type: List).\nDue to our layering concept and the persistence layer being the only place where access to the database should take place, you should not directly execute a query in this method, but call a DAO, which itself executes the query (using pagination).\nAbstractPagingItemReader provides methods for finding out the current position: use getPage() for the current page and getPageSize() for the (max.) page size. These values should be passed to the DAO as parameters. Note that the AbstractPagingItemReader starts counting pages from zero, whereas the PaginationTo used for pagination (retrieved by calling SearchCriteriaTo.getPagination()) starts counting from one, which is why you always have to increment the page number by one.\nThe second method is doJumpToPage(int), which usually only requires an empty implementation.\nFurthermore, you need to set the property pageSize, which specifies how many items should be read at once. A page size that is as big as the commit interval usually results in the best performance.\nThe approach of using pagination for ItemReader should not be used when items (usually entities) are added or removed or modified by the batch step itself or in parallel with the execution of the batch step so that the order changes, e.g. by other batches or due to operations started by clients (i.e. if the batch is executed in online mode). In this case there might be items processed twice or not processed at all. Be aware that due to hibernate&#x2019;s Hi/Lo-Algorithm newer entities could get lower IDs than existing IDs and you probably will not process all entities if you rely on strict ID monotony!\nA simple solution for such scenarios would be to introduce a new flag &apos;processed&apos; for the entities read if that is an option (as it is also done in the example batch). The query should be rewritten then so that only unprocessed items are read (additionally limiting the result set size to the number of items to be processed in the current chunk, but not more).\nNote that most of the standard implementations provided by Spring Batch do not fit to the layering approach in devonfw applications, as these mostly require direct access to an EntityManager or a JDBC connection for example. You should think twice when using them and not break the layering concept.\nReading from Files\nFor reading simply structured files, e.g. for those in which every line corresponds to an item to be processed by the batch, the FlatFileItemReader can be used. It requires two properties to be set: The first one the LineMapper (property lineMapper), which is used to convert a line (i.e. a String) to an item. It is a very simple interface which will not be discussed in more detail here. The second one is the resource, which is actually the file to be read. When set in the XML, it is sufficient to specify the path with a &quot;file:&quot; in front of it if it is a normal file from the file system.\nIn addition to that, the property linesToSkip (integer) can be set to skip headers for example. For reading more than one line before for creating an item, a RecordSeparatorPolicy can be used, which will not be discussed in more detail here, too. By default, all lines starting with a &apos;#&apos; will be considered to be a comment, which can be changed by changing the comment property (string array). The encoding property can be used to set the encoding. A FlatFileItemReader can restore its state after restarts.\nFor reading XML files, you can use the StaxEventItemReader (StAX is an alternative to DOM and SAX), which will not be discussed in further detail here.\nIn case the standard implementations introduced here do not fit your needs, you will need to implement your own ItemReader. If this ItemReader has some internal state (usually stored in member variables), which needs to be restored in case of restarts, see the section on saving and restoring state for information on how to do this.\nItemProcessor\nA processor must implement the ItemProcessor interface, which has the following method:\npublic O process(I item) throws Exception;\nAs you can see, there are two type parameters involved: one for the type of items received from the ItemReader and one for the type of items passed to the ItemWriter. These can be the same.\nIf an item has been selected by the ItemReader, but there is no need to further process this item (i.e. it should not be passed to the ItemWriter), the ItemProcessor can return null instead of an item.\nStrictly interpreting chunk processing, the ItemProcessor should not modify anything but should only give instructions to the ItemWriter on how to do modifications. For entities however this is not really practical and as it requires no special logic in case of rollbacks/restarts (as all modifications are transactional), it is usually OK to modify them directly.\nIn contrast to this, performing accesses to files or calling external systems should only be done in ItemReader/ItemWriter and the code needed for properly handling failures (restarts for example) should be encapsulated there.\nIt is usually a good practice to make ItemProcessor implementations stateless, as the process method might be called more than once for one item (see the section on ItemReader why). If your ItemProcessor really needs to have some internal state, see saving and restoring state on how to save and restore the state for restarts.\nDo not forget to implement use cases instead of implementing everything directly in the ItemProcessor if the processing logic gets more complex.\nItemWriter\nA writer has to implement the ItemWriter interface, which has the following method:\npublic void write(List&lt;? extends T&gt; items) Exception;\nThis method is called at the end of each chunk with a list of all (processed) items. It is not called once for every item, because it is often more efficient doing &apos;bulk writes&apos;, e.g. when writing to files.\nNote that this method might also be called more than once for one item (see the section on ItemReader&#x2019;s why).\nAt the end of the write method, there should always be a flush.\nWhen writing to files, this should be obvious, because when a chunks completes, it is expected that all changes are already there in case of restarts, which is not true if these changes were only buffered but have not been written out.\nWhen modifying the database, the flush method on the EntityManager should be called, too (via a DAO), because there might be changes not written out yet and therefore constraints were not checked yet. This can be problematic, because Spring Batch considers all exceptions that occur during commit as critical, which is why these exceptions cannot be skipped. You should be careful using deferred constraints for the same reason.\nWriting to Database or Transactional Queues\nAll changes made which are transactional can be conducted directly, there is no special logic needed for restarts, because these changes are applied if and only if the chunk succeeds.\nWriting to Files\nFor writing simply structured files, the FlatFileItemWriter can be used. Similar to the FlatFileItemReader it requires the resource (i.e. the file) and a LineAggregator (property lineAggregator instead of the lineMapper) to be set.\nThere are various properties that can be used of which we will only present the most important ones here. As with the FlatFileItemReader, the encoding property is used to set the encoding. A FlatFileHeaderCallback (property headerCallback) can be used to write a header.\nThe FlatFileItemWriter can restore its state correctly after restarts. In case, the files contain too many lines (written out in chunks that did not complete successfully), these lines are removed before continuing execution.\nFor writing XML files, you can use the StaxEventItemWriter, which will not be discussed in further detail here.\nJust as with ItemReader and ItemProcessor: In case your ItemWriter has some internal state this state is not managed by a standard implementation, see saving and restoring state on how to make your implementation restartable (restart by restoring the internal state).\nSaving and Restoring State\nFor saving and restoring (in case of restarts) state, e.g. saving and restoring values of member variables, the ItemStream interface should be implemented by the ItemReader/ItemProcessor/ItemWriter, which has the following methods:\npublic void open(ExecutionContext executionContext) throws ItemStreamException;\npublic void update(ExecutionContext executionContext) throws ItemStreamException;\npublic void close() throws ItemStreamException;\nThe open method is always called before the actual processing starts for the current step and can be used to restore state when restarting.\nThe ExecutionContext passed in as parameter is basically a map to be used to retrieve values set before the failure. The method containsKey(String) can be used to check if a value for a given key is set. If it is not set, this might be because the current batch execution is no restart or no value has been set before the failure.\nThere are several getter methods for actually retrieving a value for a given key: get(String) for objects (must be serializable), getInt(String), getLong(String), getDouble(String) and getString(String). These values will be the same as after the subsequent call to the update method after the last chunk that completed successfully. Note that if you update the ExecutionContext outside of the update method (e.g. in the read method of an ItemReader), it might contain values set in chunks that did not finish successfully after restarts, which is why you should not do that.\nSo the update method is the right place to update the current state. It is called after each chunk (and before and after each step).\nFor setting values, there are several put methods: put(String, Object), putInt(String, int), putLong(String, long), putDouble(String, double) and putString(String, String). You can choose keys (String) freely as long as these are unique within the current step.\nNote that when a skip occurs, the update method is sometimes but not always called, so you should design your code in a way that it can deal with both situations.\nThe close method is usually not needed.\nDo not misuse the ItemStream interface for purposes other than storing/restoring state. For instance, do not use the update method for flushing, because you will not have the chance to properly handle failure (e.g. skipping). For opening or closing a file handle, you should rather use a StepExecutionListener as introduced in the section on listeners. The state can also be restored in the beforeStep(ExecutionListener) method (instead of the open method).\nNote that when a batch that always starts from scratch (i.e. the restartable attribute has been set to false for the batch job) is restarted, the ExecutionContext will not contain any state from the previous (failed) execution, so there is no use in storing the state in this case and usually no need to, of course, because the batch will start all over again.\nTasklet based Processing\nTasklets are the alternative to chunk processing. In the section on chunk processing we already mentioned the advantages of chunk processing as compared to tasklets. However, if only very few data needs to be processed (within one transaction) or if you need to do some sort of bulk operation (e.g. deleting all records from a database table), where the currently processed item does not matter and it is unlikely that a &apos;fine grained&apos; exception handling will be needed, tasklets might still be considered an option. Note that for the latter use case you should still use more than one transaction, which is possible when using tasklets, too.\nTasklets have to implement the interface with the same name, which has the following method:\npublic RepeatStatus execute(StepContribution contribution, ChunkContext chunkContext) throws Exception;\nThis method might be called several times. Every call is executed inside a new transaction automatically. If processing is not finished yet and the execute method should be called once more, just use RepeatStatus.CONTINUABLE as return value and RepeatStatus.FINISHED otherwise.\nThe StepContribution parameter can be used to set how many items have been processed manually (which is done automatically using chunk processing), there is, however, usually no need to do so.\nThe ChunkContext is similar to the ExecutionContext, but is only used within one chunk. If there is a retry in chunk processing, the same context should be used (with the same state that this context had when the exception occurred).\nNote that tasklets serve as the basis for chunk processing internally. For chunk processing there is a Spring Batch internal tasklet, which has an execute method that is called for every chunk and itself calls ItemReader, ItemProcessor and ItemWriter.\nThat is the reason why a StepContribution and a ChunkContext are passed to tasklets as parameters, even though they are more useful in chunk processing. Moreover this is also the reason why you have to use the tasklet element in the XML even though you want to specify a step that uses chunk processing (see the example batch).\nException Handling\nAs already mentioned, in chunk processing you can configure a step so that items are skipped or retried when certain exceptions occur.\nIf retries are exhausted (by default, there is no retry) and the exception that occurred cannot be skipped (by default, no exception can be skipped), the batch will fail (i.e. stop executing).\nIn tasklet based processing this cannot be done, the only chance is to implement the needed logic yourself.\nSkipping\nBefore skipping items you should think about what to do if a skip occurs. If a skip occurs, the exception will be logged in the server log. However if no one evaluates those logs on a regular basis and informs those who are affected further actions need to take place when implementing the batch.\nImplement the SkipListener interface to be informed when a skip occurs. For example, you could store a notification or send a message to someone. For skips that occurred in ItemReader&#x2019;s there is no information available about the item that was skipped (as it has not been read yet) which is why there should be as little processing logic as possible in an ItemReader. It might also be a reason why you might want to forbid to skip exceptions that might occur in readers.\nDo not try to catch skipped exceptions and write something into the database in a new transaction (e.g. a notification) instead of using a SkipListener, because a skipped item might be processed more than once before actually being skipped (for example, if a skippable exception is thrown during a call of an ItemWriter, Spring Batch does not know which item of the current chunk actually caused the exception and therefore has to retry each item separately in order to know which item actually caused the exception).\nSkippable exception classes can be specified as shown below:\n&lt;batch:chunk ... skip-limit=&quot;10&quot;&gt;\n&lt;batch:skippable-exception-classes&gt;\n&lt;batch:include class=&quot;...&quot;/&gt;\n&lt;batch:include class=&quot;...&quot;/&gt;\n...\n&lt;/batch:skippable-exception-classes&gt;\n&lt;/batch:chunk&gt;\nThe attribute skip-limit, which has to be set in case there is any skippable exception class configured, is used to set how many items should be skipped at most. It is useful to avoid situations where many items are skipped but the batch still completes successfully and no one notices this situation.\nSkippable exception classes are specified by their fully qualified name (e.g. java.lang.Exception), each of such class set in its own include element as shown above. Subclasses of such classes are also skipped.\nTo programmatically decide whether to skip an exception or not, you can set a skip policy as shown below:\n&lt;batch:chunk ... skip-policy=&quot;mySkipPolicy&quot;&gt;\nThe skip policy (here mySkipPolicy) has to be a bean that implements the interface SkipPolicy with the following method:\npublic boolean shouldSkip(java.lang.Throwable t,\nint skipCount)\nthrows SkipLimitExceededException\nTo skip the exception and continue processing, just return true and otherwise false.\nThe parameter skipCount can be used for a skip limit. A SkipLimitExceededException should be thrown if there should be no more skips. Note that this method is sometimes called with a skipCount less than zero to test if an exception is skippable in general.\nWhen a SkipPolicy is set, the attribute skip-limit and element skippable-exception-classes are ignored.\nYou could of course skip every exception (using java.lang.Exception as skippable exception class). This is, however, not a good practice as it might easily result in an error in the code that is ignored as the batch still completes successfully and everything seems to be fine. Instead, you should think about what kind of exceptions might actually occur, what to do if they occur and if it is OK to skip them. If an unexpected exception occurs, it is usually better to fail the batch execution and analyze the cause of the exception before restarting the batch.\nExceptions that can occur in instances of ItemWriter that write something to file should not be skipped unless the ItemWriter can properly deal with that. Otherwise there might be data written out even though the according item is skipped, because operations in the file systems are not transactional.\nAnother situation where skips can be problematic is when calls to external interfaces are being made and these calls change something &quot;on the other side&quot;, as these calls are usually not transactional. So be careful using skips here, too.\nRetrying\nFor some types of exceptions, processing should be retried independently of weather the exception can be skipped or would otherwise fail the batch execution.\nFor example, if there was a database timeout, this might be because there were too many requests at the time the chunk was processed. And it is not unlikely that retrying to successfully complete the chunk would succeed.\nThere are, of course, also exceptions where retrying does not make much sense. E.g. exceptions caused by the business logic should be deterministic and therefore retrying does not make much sense in this case.\nNevertheless, retrying every exception results in longer runtime but should in general be considered OK if you do not know which exceptions might occur or do not have the time to think about it.\nRetryable exception classes can be set similarly to setting skippable exception classes:\n&lt;batch:chunk ... retry-limit=&quot;3&quot;&gt;\n&lt;batch:retryable-exception-classes&gt;\n&lt;batch:include class=&quot;...&quot;/&gt;\n&lt;batch:include class=&quot;...&quot;/&gt;\n...\n&lt;/batch:retryable-exception-classes&gt;\n&lt;/batch:chunk&gt;\nThe retry-limit attribute specifies how many times one individual item can be retried, as long as the exception thrown is &quot;retryable&quot;.\nAs with skippable exception classes, retryable exception classes are set in include elements and their subclasses are retried, too.\nTo programmatically decide, whether to retry an exception or not, you can use a RetryPolicy, which is not covered in more detail here.\nNote that even if no retry is configured, an item might nevertheless be processed more than once. This is because if a skippable exception occurs in a chunk, all items of the chunk that did not cause the exception have to reprocessed, which is done in a separate transaction for every item, as the transaction in which these items were processed in the first place was rolled back. And even if the exception is not skippable, there is no guarantee that Spring Batch will not attempt to reprocess each item separately.\nListeners\nSpring Batch provides various listeners for various events to be notified about.\nFor every listener there is an interface which can either be implemented by an ItemReader, ItemProcessor, ItemWriter or Tasklet or by a separate listener class, which can be registered for a step like this:\n&lt;batch:tasklet&gt;\n&lt;batch:chunk .../&gt;\n&lt;batch:listeners&gt;\n&lt;batch:listener ref=&quot;listener1&quot;/&gt;\n&lt;batch:listener ref=&quot;listener2&quot;/&gt;\n....\n&lt;/batch:listeners&gt;\n&lt;/batch:tasklet&gt;\n&lt;beans:bean id=&quot;listener1&quot; class=&quot;..&quot;/&gt;\n&lt;beans:bean id=&quot;listener2&quot; class=&quot;..&quot;/&gt;\n...\nThe most commonly use listener is probably the StepExecutionListener, which has methods that are called before and after the execution of the step. It can be utilized e.g. for opening and closing files.\nThe following example shows how to use the listener:\npublic class MyListener implements StepExecutionListener {\npublic void beforeStep(StepExecution stepExecution) {\n// take actions before processing of the step starts\n}\npublic ExitStatus afterStep(StepExecution stepExecution) {\ntry {\n// take actions after processing is finished\n} catch (Exception e) {\nstepExecution.addFailureException(e);\nstepExecution.setStatus(BatchStatus.FAILED);\nreturn ExitStatus.FAILED.addExitDescription(e);\n}\nreturn null;\n}\n}\nIn the afterStep(StepExecution) method, you can check the outcome of the batch execution (completed, failed, stopped etc.) checking the ExitStatus, which can be accessed via StepExecution.getExitStatus(). You can even modify the ExitStatus by returning a new ExitStatus, which is something we will not discuss in further detail here. If you do not want to modify the ExitStatus, just return null.\nThrowing an exception in this method has no effect. If you want to fail the whole batch in case an exception occurs, you have to do an exception handling as shown above. This does not apply to the beforeStep method.\nFor other types of listeners (among others the SkipListener mentioned already) see Spring Batch Reference Documentation - 5. Configuring a Step - Intercepting Step Execution.\nNote that exception handling for listeners is often a problem, because exceptions are mostly ignored, which is not always documented very well. If an important part of a batch is implemented in listener methods, you should always test what happens when exceptions occur. Or you might think about not implementing important things in listeners &#x2026;&#x200B;\nIf you want an exception to fail the whole batch, you can always wrap it in a FatalStepExecutionException, which will stop the execution.\nParameters\nThe section on starting and stopping batches already showed how to start a batch with parameters.\nOne way to get access to the values set is using the StepExecutionListener introduced in the section on listeners like this:\npublic void beforeStep(StepExecution stepExecution) {\nString parameterValue = stepExecution.getJobExecution().getJobParameters().\ngetString(&quot;parameterKey&quot;);\n}\nThere are getter methods for strings, doubles, longs and dates. Note that when set via the CommandLineJobRunner or SpringBootBatchCommandLine, all parameters will be of type string unless the type is specified in brackets after the parameter key, e.g. processUntil(date)=2015/12/31. The parameter key here is processUntil.\nAnother way is to inject values. In order for this to work, the bean has to have step scope, which means there is a new object created for every execution of a batch step. It works like this:\n&lt;bean id=&quot;myProcessor&quot; class=&quot;...MyItemProcessor&quot; scope=&quot;step&quot;&gt;\n&lt;property name=&quot;parameter&quot; value=&quot;#{jobParameters[&apos;parameterKey&apos;]}&quot; /&gt;\n&lt;bean&gt;\nThere has to be an appropriate setter method for the parameter of course.\nAs already mentioned in the section on restarts, a batch that successfully completed with a certain set of parameters cannot be started once more with the same parameters as this would be considered a restart, which is not necessary, because the batch was already finished.\nSo using no parameters for a batch would mean that it can be started until it completes successfully once, which usually does not make much sense.\nAs batches are usually not executed more than once a day, we propose introducing a general date parameter (without time) for all batch executions.\nIt is advisable to add the date parameter automatically in the JobLauncher if it has not been set manually, which can be done as shown below:\nprivate static final String DATE_PARAMETER = &quot;date&quot;;\n...\nif (jobParameters.getDate(&quot;DATE_PARAMETER&quot;) == null) {\nDate dateWithoutTime = new Date();\nCalendar cal = Calendar.getInstance();\ncal.setTime(dateWithoutTime);\ncal.set(Calendar.HOUR_OF_DAY, 0);\ncal.set(Calendar.MINUTE, 0);\ncal.set(Calendar.SECOND, 0);\ncal.set(Calendar.MILLISECOND, 0);\ndateWithoutTime = cal.getTime();\njobParameters = new JobParametersBuilder(jobParameters).addDate(\nDATE_PARAMETER, dateWithoutTime).toJobParameters();\n... // using the jobParametersIncrementer as shown above\n}\nKeep in mind that you might need to set the date parameter explicitly for restarts. Also note that automatically setting the date parameter can be problematic if a batch is sometimes started before and sometimes after midnight, which might result in a batch not being executed (as it has already been executed with the same parameters), so at least for productive systems you should always set it explicitly.\nThe date parameters can also be useful for controlling the business logic, e.g. a batch can process all data that was created until the current date (as set in the date parameter), thereby giving a chance to control how much is actually processed.\nIf your batch has to run more than once a day you could easily adapt the concept of timestamps. If you are using an external batch scheduler, they often provide a counter for the execution and you might automatically pass this instead of the date parameter.\nPerformance Tuning\nMost important for performance are of course the algorithms that you write and how fast (and scalable) these are, which is the same as for client processing. Apart from that, the performance of batches is usually closely related to the performance of the database system.\nIf you are retrieving information from the database, you can have one complex query executed in the ItemReader (via a DAO) retrieving all the information needed for the current set of items, or you can execute further queries in the ItemProcessor (or ItemWriter) on a per item basis to retrieve further information.\nThe first approach usually results in far more performance, because there is an overhead for every query being executed and this approach results in less queries being executed. Note that there is a tradeoff between performance and maintainability here. If you put everything into the query executed by an ItemReader, this query can get quite complex.\nUsing cursors instead of pagination as described in the section on ItemReaders can result in a better performance for the same reason: When using a cursor, the query is only executed once, when using pagination, the query is usually executed once per chunk. You could of course manually cache items, however this easily leads to a high memory consumption.\nFurther possibilities for optimizations are query (plan) optimization and adding missing database indexes.\nTesting\nThe Section Testing covers how to unit and integration test in detail. Therefore we focus here on testing batches.\nIn order for the unit test to run a batch job the unit test class must extend the AbstractSpringBatchIntegrationTest class. Annotation used to load the job&#x2019;s ApplicationContext:\n@SpringBootTest(classes = {&#x2026;&#x200B;}): Indicates which JavaConfig classes (attribute classes)\n@ImportResource(&quot;classpath:../sample_BatchContext.xml&quot;) : Indicates XML files that contain the `ApplicationContext. Use @ContextConfiguration(&#x2026;&#x200B;) if Spring Boot is not used.\npublic abstract class AbstractSpringBatchIntegrationTest extends AbstractComponentTest {..}\n@SpringBootTest(classes = { SpringBootBatchApp.class }, webEnvironment = WebEnvironment.RANDOM_PORT)\n@ImportResource(&quot;classpath:config/app/batch/beans-productimport.xml&quot;)\n@EnableAutoConfiguration\npublic class ProductImportJobTest extends AbstractSpringBatchIntegrationTest {..}\nTesting Batch Jobs\nFor testing the complete run of a batch job from beginning to end involves following steps:\nset up a test condition\nexecute the job\nverify the end result.\nThe test method below begins by setting up the database with test data. The test then launches the Job using the launchJob() method. The launchJob() method is provided by the JobLauncherTestUtils class.\nAlso provided by the utils class is launchJob(JobParameters), which allows the test to give particular parameters. The launchJob() method returns the JobExecution object which is useful for asserting particular information about the Job run. In the case below, the test verifies that the Job ended with ExitStatus COMPLETED.\n@SpringBootTest(classes = { SpringBootBatchApp.class }, webEnvironment = WebEnvironment.RANDOM_PORT)\n@ImportResource(&quot;classpath:config/app/batch/beans-productimport.xml&quot;)\n@EnableAutoConfiguration\npublic class ProductImportJobTest extends AbstractSpringBatchIntegrationTest {\n@Inject\nprivate Job productImportJob;\n@Test\npublic void testJob() throws Exception {\n......\n......\nJobExecution jobExecution = getJobLauncherTestUtils(this.productImportJob).launchJob(jobParameters);\nassertThat(jobExecution.getStatus()).isEqualTo(BatchStatus.COMPLETED);\n......\n......\n}\n}\nNote that when using the launchJob() method, the batch execution will never be considered as a restart (i.e. it will always start from scratch). This is achieved by adding a unique (random) parameter.\nThis is not true for the method launchJob(JobParameters) however, which will result in an exception if the test is executed twice or a batch is executed in two different tests with the same parameters.\nWe will add methods for appropriately handling this situation in future releases of devonfw. Until then you can help yourself by using the method getUniqueJobParameters() and then add all required parameters to those parameters returned by the method (as shown in the section on parameters).\nAlso note that even if skips occurred, the BatchStatus is still COMPLETED. That is one reason why you should always check whether the batch did what it was supposed to do or not.\nTesting Individual Steps\nFor complex batch jobs individual steps can be tested. For example to test a createCsvFile, run just that particular Step. This approach allows for more targeted tests by allowing the test to set up data for just that step and to validate its results directly.\nJobExecution jobExecution = getJobLauncherTestUtils(this.billExportJob).launchStep(&#x201C;createCsvFile&#x201D;);\nValidating Output Files\nWhen a batch job writes to the database, it is easy to query the database to verify the output. To facilitate the verification of output files Spring Batch provides the class AssertFile. The method assertFileEquals takes two File objects and asserts, line by line, that the two files have the same content. Therefore, it is possible to create a file with the expected output and to compare it to the actual result:\nprivate static final String EXPECTED_FILE = &quot;classpath:expected.csv&quot;;\nprivate static final String OUTPUT_FILE = &quot; file:./temp/output.csv&quot;;\nAssertFile.assertFileEquals(new FileSystemResource(EXPECTED_FILE), new FileSystemResource(OUTPUT_FILE));\nTesting Restarts\nSimulating an exception at an arbitrary method in the code can be done relatively easy using AspectJ. Afterwards you should restart the batch and check if the outcome is still correct.\nNote that when using the launchJob() method, the batch is always started from the beginning (as already mentioned). Use the launchJob(JobParameters) instead with the same parameters for the initial (failing) execution and for the restart.\nTest your code thoroughly. There should be at least one restart test for every step of the batch job.\n&#x2190;&#xA0;Previous:&#xA0;Coding&#xA0;| &#x2191;&#xA0;Up:&#xA0;devon4j&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Guides&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devon4j.asciidoc_tutorials.html","title":"11. Tutorials","body":"\n11. Tutorials\n11.1. Introduction\nThis is an step by step tutorial for starting an devonfw server application from setting up the environment to packaging for production.\nThe tutorial starts by setting up the programmer environment with the aid of the devon-ide project and verifies everything is correct by running the my-thai-start restaurant sample application of the devonfw project.\nAfterwards a new blank application is created by using the provided archetypes and all generated files are reviewed to explain what devonfw is providing.\nA classical CRUD use case is developed for creating, retrieving updating and deleting an entity. With this entity we introduce cross cutting concerns such as exception handling, validation and securing the access from the web.\nFinally the sample will be ready for deployment to a web server so we will package it on a WAR (or EAR) file.\n11.2. Creating a new application\n11.2.1. Running the archetype\nIn order to create a new application you must use the archetype provided by devon4j which uses the maven archetype functionality.\nTo create a new application, you should have installed devonfw IDE.\nYou can choose between 2 alternatives, create it from command line or, in more visual manner, within eclipse.\nFrom command Line\nTo create a new devon4j application from command line, you can simply run the following command:\ndevon java create com.example.application.sampleapp\nFor low-level creation you can also manually call this command:\nmvn -DarchetypeVersion=3.1.0 -DarchetypeGroupId=com.devonfw.java.templates -DarchetypeArtifactId=devon4j-template-server archetype:generate -DgroupId=com.example.application -DartifactId=sampleapp -Dversion=1.0.0-SNAPSHOT -Dpackage=com.devonfw.application.sampleapp\nFurther providing additional properties (using -D parameter) you can customize the generated app:\nTable 19. Options for app template\nproperty\ncomment\nexample\ndbType\nChoose the type of RDBMS to use (hana, oracle, mssql, postgresql, mariadb, mysql, etc.)\n-DdbTpye=postgresql\nbatch\nOption to add an batch module\n-Dbatch=batch\nearProjectName\nOption to add an EAR module with the given name\n-DearProjectName=ear\nFrom Eclipse\nAfter that, you should follow this Eclipse steps to create your application:\nCreate a new Maven Project.\nChoose the devon4j-template-server archetype, just like the image.\nFill the Group Id, Artifact Id, Version and Package for your project.\nIf you want to add an EAR generation mechanism to your project, you should fill the property earProjectName with the value Artifact Id + &quot;-ear&quot;. For example, &quot;sampleapp-ear&quot;. If you only want WAR generation, you can remove the property earProjectName.\nFinish the Eclipse assistant and you are ready to start your project.\n11.2.2. What is generated\nThe application template (archetype) generates a Maven multi-module project. It has the following modules:\napi: module with the API (REST service interfaces, transferobjects, datatypes, etc.) to be imported by other apps as a maven dependency in order to invoke and consume the offered (micro)services.\ncore: maven module containing the core of the application.\nbatch: optional module for batch(es)\nserver: module that bundles the entire app (core with optional batch) as a WAR file.\near: optional maven module is responsible to packaging the application as a EAR file.\nThe toplevel pom.xml of the generated project has the following features:\nProperties definition: Spring-boot version, Java version, etc.\nModules definition for the modules (described above)\nDependency management: define versions for dependencies of the technology stack that are recommended and work together in a compatible way.\nMaven plugins with desired versions and configuration\nProfiles for test stages\nCore Module\nCore module contains the base classes and the base configuration for the application. We are going to describe each Java file and each XML configuration file that archetype has generated.\nJava\nThose are the different Java files contained in each package:\ngeneral.common\nFile\nDescription\napi.ApplicationEntity.java\nAbstract interface for a MutableGenericEntity of this application.\napi.BinaryObject.java\nInterface for a BinaryObject.\napi.NlsBundleApplicationRoot.java\nNlsBundle for this application.\napi.Usermanagement.java\nInterface to get a user from its login.\napi.UserProfile.java\nInterface for the profile of a logged user.\napi.constants.PermissionConstants.java\nConstants for AccessControlPermission&#xB4;s keys.\napi.datatype.OrderBy.java\nEnum for sort order.\napi.datatype.Role.java\nEnum for roles.\napi.exception.ApplicationBusinessException.java\nAbstract business main exception.\napi.exception.ApplicationException.java\nAbstract main exception.\napi.exception.ApplicationTechnicalException.java\nAbstract technical main exception.\napi.exception.IllegalEntityStateException.java\nManage entities illegal state exceptions.\napi.exception.IllegalPropertyChangeException.java\nManage entities illegal property changes exceptions.\napi.exception.NoActiveUserException.java\nManage exceptions when user require to be logged in.\napi.security.UserData.java\nContainer class for the profile of a user.\napi.to.AbstractCto.java\nAbstract class for Composite Transfer Object.\napi.to.AbstractEto.java\nAbstract class for Entity Transfer Object.\napi.to.AbstractTo.java\nAbstract class for a plain Transfer Object.\napi.to.SearchCriteriaTo.java\nAbstract class for a Transfer Object with the criteria for a search query.\napi.to.UserDetailsClientTo.java\n.\nbase.AbstractBeanMapperSupport.java\nProvides access to the BeanMapper.\nimpl.security.ApplicationAuthenticationProvider.java\nResponsible for the security aspects of authentication.\nimpl.security.\nPrincipalAccessControlProviderImpl.java\nImplementation of PrincipalAccessControlProvider.\ngeneral.dataaccess\nFile\nDescription\napi.ApplicationPersistenceEntity.java\nAbstract Entity for all Entities with an id and a version field.\napi.BinaryObjectEntity.java\nBinaryObject entity.\napi.dao.ApplicationDao.java\nInterface for all DAOs of the application.\napi.dao.ApplicationRevisionedDao.java\nInterface for all revisioned DAOs of the application.\napi.dao.BinaryObjectDao.java\nDAO for BinaryObject entity.\ngeneral.gui.api\nFile\nDescription\nLoginController.java\nController for login page.\ngeneral.logic\nFile\nDescription\napi.UseCase.java\nAnnotation to mark all use-cases.\napi.to.BinaryObjectEto.java\nETO for a BinaryObject.\nbase.AbstractUc.java\nAbstract base class for any use case in the application.\nbase.UcManageBinaryObject.java\nUse case for managing BinaryObject.\nimpl.UcManageBinaryObjectImpl.java\nImplementation of the UcManageBinaryObject interface.\nimpl.UsermanagementDummyImpl.java\nImplementation of Usermanagement.\ngeneral.service.impl.rest\nFile\nDescription\nApplicationAccessDeniedHandler.java\nClass to manage denied access.\nApplicationObjectMapperFactory.java\nMappingFactory class to resolve polymorphic conflicts within the application.\nSecurityRestServiceImpl.java\nClass that represents REST service for security.\nResources\nThose are the different XML files contained in resources folder:\nconfig\nFile\nDescription\napp.common.beans-common.xml\nContains beans definition for application common beans like propertyConfigurer bean.\napp.common.beans-dozer.xml\nBeans relationated with Dozer Mappers.\napp.common.dozer-mapping.xml\nDozer mapping configuration.\napp.dataaccess.beans-dataaccess.xml\nParent from the other data access files.\napp.dataaccess.beans-db-plain.xml\nData source configuration for profile db-plain (testing).\napp.dataaccess.beans-db-server.xml\nData source configuration for profile distinct to db-plain.\napp.dataaccess.beans-jpa.xml\nContains necessary beans to configure JPA.\napp.dataaccess.NamedQueries.xml\napp.gui.dispatcher-servlet.xml\napp.logic.beans-logic.xml\nComponent scan configuration for classes in logic path.\napp.security.access-control-schema.xml\napp.security.beans-security-filters.xml\nSecurity filters definition.\napp.security.beans-security.xml\nApplication security configuration.\napp.service.beans-monitoring.xml\napp.service.beans-service.xml\nImporting configuration files, REST beans definition and configuration.\napp.websocket.websocket-context.xml\nScan component package definition for websockets.\napp.application.default.properties\nDefault application properties values.\napp\nbeans-application\nRoot file configuration. It starts the chain and imports other configuration files.\nenv\napplication\nSpecific application properties values.\ndb\nFile\nDescription\nmigration.V0001__Create_schema.slq\nScript template to create the database schema and tables definition.\nTest\nThose are different Java files to serve as base classes in testing:\ngeneral.common\nFile\nDescription\nAbstractSpringIntegrationTest.java\n.\nAccessControlSchemaXmlValidationTest.java\nTests if the access-control-schema.xml is valid.\nPermissionCheckTest.java\nTest to check if all relevant methods in use case implementations have permission checks.\nServer Module\nThis module is contains two files:\nlogback.xml: This file is in the resources folder and it is the responsible to configure the log.\npom.xml: This file has Maven configuration for packaging the application as a WAR. Also, this file has a profile to package the Javascript client ZIP file into the WAR.\nEAR Module\nThis module only contains a pom.xml file to packaging the application as EAR from the WAR generated.\n11.2.3. Database configuration and creation\nIncluding driver installation if oracle or other db is required.\n11.2.4. Editing the pom.xml\nHow to edit the pom.xml file for the project to add dependencies and modules for the application.\n11.2.5. Known Issues\nCould not resolve archetype com.devonfw.java.templates:devon4j-template-server:.. from any of the configured repositories.\nIn Eclipse:\nOpen Window &gt; Preferences\nOpen Maven &gt; Archetypes\nClick &apos;Add Remote Catalog&apos; and add the following:\nCatalog File: http://repo1.maven.org/maven2/archetype-catalog.xml\nDescription: maven catalog\n&#x2190;&#xA0;Previous:&#xA0;devonfw Development&#xA0;| &#x2191;&#xA0;Up:&#xA0;devon4j&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;For Core-Developers&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devon4net.asciidoc.html","title":"V. devon4net","body":"\nV. devon4net\nArchitecture basics\nCoding conventions\nEnvironment\nUser guide\nPackages\nTemplates\nSamples\n&#x2190;&#xA0;Previous:&#xA0;Cookbook&#xA0;| &#x2191;&#xA0;Up:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Architecture basics&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devon4net.asciidoc_architecture-basics.html","title":"21. Architecture basics","body":"\n21. Architecture basics\n21.1. Introduction\nThe devonfw platform provides a solution to building applications which combine best-in-class frameworks and libraries as well as industry proven practices and code conventions.\nIt massively speeds up development, reduces risks and helps you to deliver better results.\n21.1.1. Overview Onion Design\nThis guide shows the overall proposed architecture in terms of separated layers making use the Onion architecture pattern. Each layers represents a logical group of components and functionality. In this guide you will learn the basics of the proposed architecture based in layers in order to develop software making use of the best practices.\n21.1.2. Layer specification\nIt is important to understand the distinction between layers and tiers. Layers describe the logical groupings of the functionality and components in an application; whereas tiers describe the physical distribution of the functionality and components on separate servers, computers, networks, or remote locations. Although both layers and tiers use the same set of names (presentation, business, services, and data), remember that only tiers imply a physical separation. It is quite common to locate more than one layer on the same physical machine (the same tier). You can think of the term tier as referring to physical distribution patterns such as two-tier, three-tier, and n-tier.\n&#x2014; Layered Application Guidelines\nMSDN Microsoft\nThe proposed architecture makes use of cooperating components called layers. Each layer contains a set of components capable to develop a specific functionality.\nThe next figure represents the different layers:\nFigure 91. High level architecture representation\nThe layers are separated in physical tiers making use of interfaces. This pattern makes possible to be flexible in different kind of projects maximizing performance and deployment strategies (synchronous/asynchronous access, security, component deployment in different environments, microservices&#x2026;&#x200B;). Another important point is to provide automated unit testing or test-driven development (TDD) facilities.\nApplication layer\nThe Application Layer encapsulates the different .Net projects and its resource dependencies and manages the user interaction depending on the project&#x2019;s nature.\nFigure 92. Net application stack\nThe provided application template implements an dotnet API application. Also integrates by default the Swagger client. This provides the possibility to share the contract with external applications (angular, mobile apps, external services&#x2026;&#x200B;).\nBusiness layer\nThe business layer implements the core functionality of the application and encapsulates the component&#x2019;s logic.\nThis layer provides the interface between the data transformation and the application exposition. This allow the data to be optimized and ready for different data consumers.\nThis layer may implement for each main entity the API controller, the entity related service and other classes to support the application logic.\nIn order to implement the service logic, the services class must follow the next specification:\npublic class Service&lt;TContext&gt; : IService where TContext: DbContext\nPE: devon4Net API template shows how to implement the TODOs service as follows:\npublic class TodoService: Service&lt;TodoContext&gt;, ITodoService\nWhere Service is the base service class to be inherited and have full access for the Unit of work, TodoContext is the TODOs database context and ITodoService is the interface of the service, which exposes the public extended methods to be implemented.\nData layer\nThe data layer orchestrates the data obtained between the Domain Layer and the Business Layer. Also transforms the data to be used more efficiently between layers.\nSo, if a service needs the help of another service or repository, the implemented Dependency Injection is the solution to accomplish the task.\nThe main aim of this layer is to implement the repository for each entity. The repository&#x2019;s interface is defined in the Domain layer.\nIn order to implement the repository logic, the repository class must follow the next specification:\nRepository&lt;T&gt; : IRepository&lt;T&gt; where T : class\nPE: devon4Net API template shows how to implement the TODOs repository as follows:\npublic class TodoRepository : Repository&lt;Todos&gt;, ITodoRepository\nWhere Repository is the the base repository class to be inherited and have full access for the basic CRUD operations, Todos is the entity defined in the database context. ITodoRepository is the interface of the repository, which exposes the public extended methods to be implemented.\nPlease remember that &lt;T&gt; is the mapped class which reference the entity from the database context. This abstraction allows to write services implementation with different database contexts\nDomain layer\nThe domain layer provides access to data directly exposed from other systems. The main source is used to be a data base system. The provided template makes use of Entity Framework solution from Microsoft in order to achieve this functionality.\nTo make a good use of this technology, Repository Pattern has been implemented with the help of Unit Of Work pattern. Also, the use of generic types are makes this solution to be the most flexible.\nRegarding to data base source, each entity is mapped as a class. Repository pattern allows to use this mapped classes to access the data base via Entity framework:\npublic class UnitOfWork&lt;TContext&gt; : IUnitOfWork&lt;TContext&gt; where TContext : DbContext\nWhere &lt;T&gt; is the mapped class which reference the entity from the database.\nThe repository and unit of work patterns are create an abstraction layer between the data access layer and the business logic layer of an application.\nDomain Layer has no dependencies with other layers. It contains the Entities, datasources and the Repository Interfaces.\ndevon4Net architecture layer implementaion\nThe next picture shows how the devon4Net API template implements the architectured described in previous points:\nFigure 93. devon4Net architecture implementations\nCross-Cutting concerns\nCross-cutting provides the implementation functionality that spans layers. Each functionality is implemented through components able to work stand alone. This approach provides better reusability and maintainability.\nA common component set of cross cutting components include different types of functionality regarding to authentication, authorization, security, caching, configuration, logging, and communication.\n21.1.3. Communication between Layers: Interfaces\nThe main target of the use of interfaces is to loose coupling between layers and minimize dependencies.\nPublic interfaces allow to hide implementation details of the components within the layers making use of dependency inversion.\nIn order to make this possible, we make use of Dependency Injection Pattern (implementation of dependency inversion) given by default in .Net Core.\nThe provided Data Layer contains the abstract classes to inherit from. All new repository and service classes must inherit from them, also the must implement their own interfaces.\nFigure 94. Architecture representation in deep\n21.1.4. Templates\nState of the art\nThe provided bundle contains the devon4Net API template based on .net core. The template allows to create a microservice solution with minimal configuration.\nAlso, the devon4Net framework can be added to third party templates such as the Amazon API template to use lambdas in serverless envirnments.\nSome features are:\nGlobal configuration automated. devon4Net can be instantiated on any .net core application template with no effort\nSupport for HTTP2\nNumber of minium libraries needed\nModular clean Architecture layer\nRed button functionality (aka killswitch) to stop attending API request with custom error\nAPI error management via middleware\nSupport to only accept request from clients with a specific client certificate on Kestrel server. Special thanks to Bart Roozendaal (Capgemini NL)\nAll components use IOptions pattern to be set up properly\nSwagger generation compatible con open api v3\nModules\nThe devon4Net netstandard libraries have been updated to netstandard 2.1\nJWT:\nSecured token encryption (token cannot be decrypted anymore by external parties). You can choose the encryption algorithm depending on your needs\nSupport for secret key or certificate encryption\nAuthorization available in the autogenerated swagger portal\nCircuit breaker\nAdded support to bypass certificate validation\nSupport to use a certificate for https communications using Microsoft&#x2019;s httpclient factory\nUnit of Work\nRepository classes unified for increasing performance and reduce the consumed memory\nSupport for different database servers: In memory, Cosmos, MySQL + MariaDB, Firebird, PostgreSQL, Oracle, SQLite, Access, MS Local.\nSoftware stack\nTable 22. Technology Stack of devon4Net\nTopic\nDetail\nImplementation\nruntime\nlanguage &amp; VM\n.Net Core Version 3.0\npersistence\nOR-mapper\nEntity Framework Core\nservice\nREST services\nWeb API\nservice - integration to external systems - optional\nSOAP services\nWCF\nlogging\nframework\nSerilog\nvalidation\nframework\nNewtonSoft Json, DataAnnotations\ncomponent management\ndependency injection\nUnity\nsecurity\nAuthentication &amp; Authorization\nJWT .Net Security - Token based, local Authentication Provider\nunit tests\nframework\nxUnit\nCircuit breaker\nframework, allows retry pattern on http calls\nPolly\nTarget platforms\nThanks to the new .Net Core platform from Microsoft, the developed software can be published Windows, Linux, OS X and Android platforms.\nThe compete RID (Runtime Identifier) catalog is this:\nWindows\nPortable\nwin-x86\nwin-x64\nWindows 7 / Windows Server 2008 R2\nwin7-x64\nwin7-x86\nWindows 8 / Windows Server 2012\nwin8-x64\nwin8-x86\nwin8-arm\nWindows 8.1 / Windows Server 2012 R2\nwin81-x64\nwin81-x86\nwin81-arm\nWindows 10 / Windows Server 2016\nwin10-x64\nwin10-x86\nwin10-arm\nwin10-arm64\nLinux\nPortable\nlinux-x64\nCentOS\ncentos-x64\ncentos.7-x64\nDebian\ndebian-x64\ndebian.8-x64\nFedora\nfedora-x64\nfedora.24-x64\nfedora.25-x64 (.NET Core 2.0 or later versions)\nfedora.26-x64 (.NET Core 2.0 or later versions)\nGentoo (.NET Core 2.0 or later versions)\ngentoo-x64\nopenSUSE\nopensuse-x64\nopensuse.42.1-x64\nOracle Linux\nol-x64\nol.7-x64\nol.7.0-x64\nol.7.1-x64\nol.7.2-x64\nRed Hat Enterprise Linux\nrhel-x64\nrhel.6-x64 (.NET Core 2.0 or later versions)\nrhel.7-x64\nrhel.7.1-x64\nrhel.7.2-x64\nrhel.7.3-x64 (.NET Core 2.0 or later versions)\nrhel.7.4-x64 (.NET Core 2.0 or later versions)\nTizen (.NET Core 2.0 or later versions)\ntizen\nUbuntu\nubuntu-x64\nubuntu.14.04-x64\nubuntu.14.10-x64\nubuntu.15.04-x64\nubuntu.15.10-x64\nubuntu.16.04-x64\nubuntu.16.10-x64\nUbuntu derivatives\nlinuxmint.17-x64\nlinuxmint.17.1-x64\nlinuxmint.17.2-x64\nlinuxmint.17.3-x64\nlinuxmint.18-x64\nlinuxmint.18.1-x64 (.NET Core 2.0 or later versions)\nOS X\nosx-x64 (.NET Core 2.0 or later versions)\nosx.10.10-x64\nosx.10.11-x64\nosx.10.12-x64 (.NET Core 1.1 or later versions)\nAndroid\nandroid\nandroid.21\n21.1.5. External links\n.Net Frameworks\nEntity Framework documentation from Microsoft\nSwagger API tooling\nDependency Injection in .NET Core\nJson Web Token\nUnit Testing (xUnit)\nRuntime IDentifier for publishing\n&#x2191;&#xA0;Up:&#xA0;devon4net&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Coding conventions&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devon4net.asciidoc_coding-conventions.html","title":"22. Coding conventions","body":"\n22. Coding conventions\n22.1. Code conventions\n22.1.1. Introduction\nThis document covers .NET Coding Standards and is recommended to be read by team leaders/sw architects and developing teams operating in the Microsoft .NET environment.\n&#x201C;All the code in the system looks as if it was written by a single &#x2013; very competent &#x2013; individual&#x201D; (K. Beck)\n22.1.2. Capitalization Conventions\nTerminology\nCamel Case (camelCase)\nEach word or abbreviation in the middle of the phrase begins with a capital letter, with no intervening spaces or punctuation.\nThe camelCasing convention, used only for parameter names, capitalizes the first character of each word except the first word, as shown in the following examples. As the example also shows, two-letter acronyms that begin a camel-cased identifier are both lowercase.\nuse camelCasing for parameter names.\nPascal Case (PascalCase)\nThe first letter of each concatenated word is capitalized. No other characters are used to separate the words, like hyphens or underscores.\nThe PascalCasing convention, used for all identifiers except parameter names, capitalizes the first character of each word (including acronyms over two letters in length).\nuse PascalCasing for all public member, type, and namespace names consisting of multiple words.\nUnderscore Prefix (_underScore)\nFor underscore ( _ ), the word after _ use camelCase terminology.\n22.1.3. General Naming Conventions\nchoose easily readable identifier names.\nfavor readability over brevity.\n&#x25E6; e.g.: GetLength is a better name than GetInt.\n&#x25E6; Aim for the &#x201C;ubiquitous language&#x201D; (E. Evans): A language distilled from the domain language, which helps the team clarifying domain concepts and communicating with domain experts.\nprefer adding a suffix rather than a prefix to indicate a new version of an existing API.\nuse a numeric suffix to indicate a new version of an existing API, particularly if the existing name of the API is the only name that makes sense (i.e., if it is an industry standard) and if adding any meaningful suffix (or changing the name) is not an appropriate option.\ndo not use underscores, hyphens, or any other non-alphanumeric characters.\ndo not use Hungarian notation.\navoid using identifiers that conflict with keywords of widely used programming languages.\ndo not use abbreviations or contractions as part of identifier names.\ndo not use any acronyms that are not widely accepted, and even if they are, only when necessary.\ndo not use the &quot;Ex&quot; (or a similar) suffix for an identifier to distinguish it from an earlier version of the same API.\ndo not use C# reserved words as names.\ndo not use Hungarian notation. Hungarian notation is the practice of including a prefix in identifiers to encode some metadata about the parameter, such as the data type of the identifier.\n&#x25E6; e.g.: iNumberOfClients, sClientName\n22.1.4. Names of Assemblies and DLLs\nAn assembly is the unit of deployment and identity for managed code programs. Although assemblies can span one or more files, typically an assembly maps one-to-one with a DLL. Therefore, this section describes only DLL naming conventions, which then can be mapped to assembly naming conventions.\nchoose names for your assembly DLLs that suggest large chunks of functionality, such as System.Data.\nAssembly and DLL names don&#x2019;t have to correspond to namespace names, but it is reasonable to follow the namespace name when naming assemblies. A good rule of thumb is to name the DLL based on the common prefix of the assemblies contained in the assembly. For example, an assembly with two namespaces, MyCompany.MyTechnology.FirstFeature and MyCompany.MyTechnology.SecondFeature, could be called MyCompany.MyTechnology.dll.\nconsider naming DLLs according to the following pattern:\n&lt;Company&gt;.&lt;Component&gt;.dll\nwhere &lt;Component&gt; contains one or more dot-separated clauses.\nFor example:\nLitware.Controls.dll.\n22.1.5. General coding style\nSource files: One Namespace per file and one class per file.\nBraces: On new line. Always use braces when optional.\nIndention: Use tabs with size of 4.\nComments: Use // for simple comment or /// for summaries. Do not /* &#x2026; */ and do not flower box.\nUse Use built-in C# native data types vs .NET CTS types (string instead of String)\nAvoid changing default type in Enums.\nUse base or this only in constructors or within an override.\nAlways check for null before invoking events.\nAvoid using Finalize. Use C# Destructors and do not create Finalize() method.\nSuggestion: Use blank lines, to make it much more readable by dividing it into small, easy-to-digest sections:\n&#x25E6; Use a single blank line to separate logical groups of code, such as control structures.\n&#x25E6; Use two blank lines to separate method definitions\nCase\nConvention\nSource File\nPascal case. Match class name and file name\nNamespace\nPascal case\nClass\nPascal case\nInterface\nPascal case\nGenerics\nSingle capital letter (T or K)\nMethods\nPascal case (use a Verb or Verb+Object)\nPublic field\nPascal case\nPrivate field\nCamel case with underscore (_) prefix\nStatic field\nPascal case\nProperty\nPascal case. Try to use get and and set convention {get;set;}\nConstant\nPascal case\nEnum\nPascal case\nVariable (inline)\nCamel case\nParam\nCamel case\n22.1.6. Use of Region guideline\nRegions can be used to collapse code inside Visual Studio .NET. Regions are ideal candidates to hide boiler plate style code that adds little value to the reader on your code. Regions can then be expanded to provide progressive disclosure of the underlying details of the class or method.\nDo Not regionalise entire type definitions that are of an important nature. Types such as enums (which tend to be fairly static in their nature) can be regionalised &#x2013; their permissible values show up in Intellisense anyway.\nDo Not regionalise an entire file. When another developer opens the file, all they will see is a single line in the code editor pane.\nDo regionalise boiler plate type code.\n22.1.7. Use of Comment guideline\nCode is the only completely reliable documentation: write &#x201C;good code&#x201D; first!\nAvoid Unnecessary comments\nChoosing good names for fields, methods, parameters, etc. &#x201C;let the code speak&#x201D; (K. Beck) by itself reducing the need for comments and documentation\nAvoid &#x201C;repeating the code&#x201D; and commenting the obvious\nAvoid commenting &#x201C;tricky code&#x201D;: rewrite it! If there&#x2019;s no time at present to refactor a tricky section, mark it with a TODO and schedule time to take care of it as soon as possible.\nEffective comments\nUse comments to summarize a section of code\nUse comments to clarify sensitive pieces of code\nUse comments to clarify the intent of the code\nBad written or out-of-date comments are more damaging than helpful:\nWrite clear and effective comments\nPay attention to pre-existing comments when modifying code or copying&amp;pasting code\n22.1.8. External links\nNaming guidelines\nGeneral naming conventions\nCapitalization conventions\nAssembly and Name Spaces conventions\n&#x2190;&#xA0;Previous:&#xA0;Architecture basics&#xA0;| &#x2191;&#xA0;Up:&#xA0;devon4net&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Environment&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devon4net.asciidoc_environment.html","title":"23. Environment","body":"\n23. Environment\n23.1. Environment\n23.1.1. Overview\n23.1.2. Required software\nVisual Studio Code\nC# Extension for VS Code\n.Net Core SDK\n23.1.3. Setting up the environment\nDownload and install Visual Studio Code\nDownload and install .Net Core SDK\nIntall the extension Omnisharp in Visual Studio Code\nHello world\nOpen a project:\nOpen Visual Studio Code.\nClick on the Explorer icon on the left menu and then click Open Folder.\nSelect the folder you want your C# project to be in and click Select Folder. For our example, we&#x2019;ll create a folder for our project named &apos;HelloWorld&apos;.\nInitialize a C# project:\nOpen the Integrated Terminal from Visual Studio Code by typing CTRL+` (backtick). Alternatively, you can select View &gt; Integrated Terminal from the main menu.\nIn the terminal window, type dotnet new console.\nThis creates a Program.cs file in your folder with a simple &quot;Hello World&quot; program already written, along with a C# project file named HelloWorld.csproj.\nResolve the build assets:\nFor .NET Core 2.0, this step is optional. The dotnet restore command executes automatically when a new project is created.\nRun the &quot;Hello World&quot; program:\nType dotnet run.\nDebug\nOpen Program.cs by clicking on it. The first time you open a C# file in Visual Studio Code, OmniSharp will load in the editor.\nVisual Studio Code will prompt you to add the missing assets to build and debug your app. Select Yes.\nTo open the Debug view, click on the Debugging icon on the left side menu.\nLocate the green arrow at the top of the pane. Make sure the drop-down next to it has .NET Core Launch (console) selected.\nAdd a breakpoint to your project by clicking on the editor margin (the space on the left of the line numbers in the editor).\nSelect F5 or the green arrow to start debugging. The debugger stops execution of your program when it reaches the breakpoint you set in the previous step.\nWhile debugging you can view your local variables in the top left pane or use the debug console.\nSelect the green arrow at the top to continue debugging, or select the red square at the top to stop.\nFor more information and troubleshooting tips on .NET Core debugging with OmniSharp in Visual Studio Code, see Instructions for setting up the .NET Core debugger.\n23.1.4. External links\n.Net Core\nUsing .NET Core in Visual Studio Code\n.Net Core in Visual Studio Code tutorial\n&#x2190;&#xA0;Previous:&#xA0;Coding conventions&#xA0;| &#x2191;&#xA0;Up:&#xA0;devon4net&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;User guide&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devon4net.asciidoc_packages.html","title":"25. Packages","body":"\n25. Packages\n25.1. Packages\n25.1.1. Packages overview\ndevon4Net is composed by a number of packages that increases the functionality and boosts time development. Each package has it&#x2019;s own configuration to make them work properly. In appsettings.json set up your environment. On appsettings.{environment}.json you can configure each component.\n25.1.2. The packages\nYou can get the devon4Net packages on nuget.org.\nDevon4Net.Application.WebAPI.Configuration\nDescription\nThe devon4Net web API configuration core.\nConfiguration\nInstall package on your solution:\nPM&gt; Install-Package Devon4Net.Application.WebAPI.Configuration\nDefaul configuration values\n&quot;devonfw&quot;: {\n&quot;UseDetailedErrorsKey&quot;: true,\n&quot;UseIIS&quot;: false,\n&quot;UseSwagger&quot;: true,\n&quot;Environment&quot;: &quot;Development&quot;,\n&quot;KillSwitch&quot;: {\n&quot;killSwitchSettingsFile&quot;: &quot;killswitch.appsettings.json&quot;\n},\n&quot;Kestrel&quot;: {\n&quot;UseHttps&quot;: true,\n&quot;HttpProtocol&quot;: &quot;Http2&quot;, //Http1, Http2, Http1AndHttp2, none\n&quot;ApplicationPort&quot;: 8082,\n&quot;KeepAliveTimeout&quot;: 120, //in seconds\n&quot;MaxConcurrentConnections&quot;: 100,\n&quot;MaxConcurrentUpgradedConnections&quot;: 100,\n&quot;MaxRequestBodySize&quot;: 28.6, //In MB. The default maximum request body size is 30,000,000 bytes, which is approximately 28.6 MB\n&quot;Http2MaxStreamsPerConnection&quot;: 100,\n&quot;Http2InitialConnectionWindowSize&quot;: 131072, // From 65,535 and less than 2^31 (2,147,483,648)\n&quot;Http2InitialStreamWindowSize&quot;: 98304, // From 65,535 and less than 2^31 (2,147,483,648)\n&quot;AllowSynchronousIO&quot;: true,\n&quot;SslProtocol&quot;: &quot;Tls12&quot;, //Tls, Tls11,Tls12, Tls13, Ssl2, Ssl3, none. For Https2 Tls12 is needed\n&quot;ServerCertificate&quot;: {\n&quot;Certificate&quot;: &quot;localhost.pfx&quot;,\n&quot;CertificatePassword&quot;: &quot;localhost&quot;\n},\n&quot;ClientCertificate&quot;: {\n&quot;DisableClientCertificateCheck&quot;: true,\n&quot;RequireClientCertificate&quot;: false,\n&quot;CheckCertificateRevocation&quot;: true,\n&quot;ClientCertificates&quot;: {\n&quot;Whitelist&quot;: [\n&quot;3A87A49460E8FE0E2A198E63D408DC58435BC501&quot;\n],\n&quot;DisableClientCertificateCheck&quot;: false\n}\n}\n},\n&quot;IIS&quot;: {\n&quot;ForwardClientCertificate&quot;: true,\n&quot;AutomaticAuthentication&quot;: true,\n&quot;AuthenticationDisplayName&quot; : &quot;&quot;\n}\n}\nDevon4Net.Infrastructure.CircuitBreaker\nDescription\nThe Devon4Net.Infrastructure.CircuitBreaker componnet implements the retry pattern for HTTP/HTTPS calls.\nConfiguration\nInstall package on your solution:\nPM&gt; Install-Package Devon4Net.Infrastructure.CircuitBreaker\nDefaul configuration values\n&quot;CircuitBreaker&quot;: {\n&quot;CheckCertificate&quot;: true,\n&quot;Endpoints&quot;: [\n{\n&quot;Name&quot;: &quot;SampleService&quot;,\n&quot;BaseAddress&quot;: &quot;https://localhost:5001/&quot;,\n&quot;Headers&quot;: {\n},\n&quot;WaitAndRetrySeconds&quot;: [\n0.0001,\n0.0005,\n0.001\n],\n&quot;DurationOfBreak&quot;: 0.0005,\n&quot;UseCertificate&quot;: true,\n&quot;Certificate&quot;: &quot;localhost.pfx&quot;,\n&quot;CertificatePassword&quot;: &quot;localhost&quot;,\n&quot;SslProtocol&quot;: &quot;3072&quot; //TLS12\n}\n]\n}\nProperty\nDescription\nCheckCertificate\nTrue if HTTPS is requiered. This is usefull when developing an API Gateway needs a secured HTTP, disabling this on development we can use communications with a valid server certificate\nEndpoints\nArray with predefined sites to connect with\nName\nThe name key to identificate the destination URL\nHeaders\nNot ready yet\nWaitAndRetrySeconds\nArray wich determinates the number of retruies and the lapse period between each retry. The value is in milliseconds.\nCertificate\nCeritificate client to use to perform the HTTP call\nSslProtocol\nThe secure protocol to use on the call\nProtocols\nProtocol\nKey\nDescription\nSSl3\n48\nSpecifies the Secure Socket Layer (SSL) 3.0 security protocol. SSL 3.0 has been superseded by the Transport Layer Security (TLS) protocol and is provided for backward compatibility only.\nTLS\n192\nSpecifies the Transport Layer Security (TLS) 1.0 security protocol. The TLS 1.0 protocol is defined in IETF RFC 2246.\nTLS11\n768\nSpecifies the Transport Layer Security (TLS) 1.1 security protocol. The TLS 1.1 protocol is defined in IETF RFC 4346. On Windows systems, this value is supported starting with Windows 7.\nTLS12\n3072\nSpecifies the Transport Layer Security (TLS) 1.2 security protocol. The TLS 1.2 protocol is defined in IETF RFC 5246. On Windows systems, this value is supported starting with Windows 7.\nTLS13\n12288\nSpecifies the TLS 1.3 security protocol. The TLS protocol is defined in IETF RFC 8446.\nUsage\nAdd via Dependency Injection the circuit breaker instance. PE:\npublic class FooService : Service&lt;TodosContext&gt;, ILoginService\n{\npublic FooService(IUnitOfWork&lt;AUTContext&gt; uoW, ICircuitBreakerHttpClient circuitBreakerClient,\nILogger&lt;LoginService&gt; logger) : base(uoW)\n{\n...\n}\n}\nAt this point you can use the circuit breaker functionality in your code.\nTo perform a POST call you should use your circuit breaker instance as follows:\nawait circuitBreakerClient.PostAsync&lt;YourOutputClass&gt;(NameOftheService, EndPoint, InputData, MediaType.ApplicationJson).ConfigureAwait(false);\nWhere:\nProperty\nDescription\nYourOutputClass\nThe type of the class that you are expecting to retrieve from the POST call\nNameOftheService\nThe key name of the endpoint provided in the appsettings.json file at Endpoints[] node\nEndPoint\nPart of the url to use with the base address. PE: /validate\nInputData\nYour instance of the class with values that you want to use in the POST call\nMediaType.ApplicationJson\nThe media type flag for the POST call\ndevon4Net.Domain.UnitOfWork\nDescription\nUnit of work implementation for devon4net solution. This unit of work provides the different methods to access the data layer with an atomic context. Sync and Async repository operations are provided. Customized Eager Loading method also provided for custom entity properties.\nThis component will move on next releases to Infrastructure instead of being part of Domain components\nConfiguration\nInstall package on your solution:\nPM&gt; Install-Package devon4Net.Domain.UnitOfWork\nAdding the database connection information:\nAdd the databse connection on the SetupDatabase method at Startup.cs\nprivate void SetupDatabase(IServiceCollection services)\n{\nservices.SetupDatabase&lt;TodoContext&gt;(Configuration, &quot;Default&quot;, WebAPI.Configuration.Enums.DatabaseType.InMemory);\n}\nWhere:\nParam\nDescription\nTodoContext\nIs the database context definition\nDefault\nIs the connection string defined at ConnectionString node at the appsettings configuration file\nWebAPI.Configuration.Enums.DatabaseType.InMemory\nIs the database driver selection. In this case InMemory data base is chosen\nThe supported databases are:\nSqlServer\nSqlite\nInMemory\nCosmos\nPostgreSQL\nMySql\nMariaDb\nFireBird\nOracle\nMSAccess\nNotes\nNow you can use the unit of work via dependency injection on your classes:\nFigure 98. Use of Unit of work via dependency injection\nAs you can see in the image, you can use Unit Of Work class with your defined ModelContext classes.\nPredicate expression builder\nUse this expression builder to generate lambda expressions dynamically.\nvar predicate = PredicateBuilder.True&lt;T&gt;();\nWhere T is a class. At this moment, you can build your expression and apply it to obtain your results in a efficient way and not retrieving data each time you apply an expression.\nExample from My Thai Star .Net Core implementation:\npublic async Task&lt;PaginationResult&lt;Dish&gt;&gt; GetpagedDishListFromFilter(int currentpage, int pageSize, bool isFav, decimal maxPrice, int minLikes, string searchBy, IList&lt;long&gt; categoryIdList, long userId)\n{\nvar includeList = new List&lt;string&gt;{&quot;DishCategory&quot;,&quot;DishCategory.IdCategoryNavigation&quot;, &quot;DishIngredient&quot;,&quot;DishIngredient.IdIngredientNavigation&quot;,&quot;IdImageNavigation&quot;};\n//Here we create our predicate builder\nvar dishPredicate = PredicateBuilder.True&lt;Dish&gt;();\n//Now we start applying the different criteria:\nif (!string.IsNullOrEmpty(searchBy))\n{\nvar criteria = searchBy.ToLower();\ndishPredicate = dishPredicate.And(d =&gt; d.Name.ToLower().Contains(criteria) || d.Description.ToLower().Contains(criteria));\n}\nif (maxPrice &gt; 0) dishPredicate = dishPredicate.And(d=&gt;d.Price&lt;=maxPrice);\nif (categoryIdList.Any())\n{\ndishPredicate = dishPredicate.And(r =&gt; r.DishCategory.Any(a =&gt; categoryIdList.Contains(a.IdCategory)));\n}\nif (isFav &amp;&amp; userId &gt;= 0)\n{\nvar favourites = await UoW.Repository&lt;UserFavourite&gt;().GetAllAsync(w=&gt;w.IdUser == userId);\nvar dishes = favourites.Select(s =&gt; s.IdDish);\ndishPredicate = dishPredicate.And(r=&gt; dishes.Contains(r.Id));\n}\n// Now we can use the predicate to retrieve data from database with just one call\nreturn await UoW.Repository&lt;Dish&gt;().GetAllIncludePagedAsync(currentpage, pageSize, includeList, dishPredicate);\n}\ndevon4Net.Infrastructure.Extensions\nDescription\nMiscellaneous extension library which contains :\n- Predicate expression builder\n- DateTime formatter\n- HttpClient\n- HttpContext (Middleware support)\nConfiguration\nInstall package on your solution:\nPM&gt; Install-Package devon4Net.Infrastructure.Extensions\nHttpContext\nTryAddHeader method is used on devon4Net.Infrastructure.Middleware component to add automatically response header options such authorization.\ndevon4Net.Infrastructure.JWT\nDescription\nJSON Web Token (JWT) is an open standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. This information can be verified and trusted because it is digitally signed. JWTs can be signed using a secret (with the HMAC algorithm) or a public/private key pair using RSA or ECDSA.\n&#x2014; What is JSON Web Token?\nhttps://jwt.io/introduction/\ndevon4Net component to manage JWT standard to provide security to .Net API applications.\nConfiguration\nInstall package on your solution:\nPM&gt; devon4Net.Infrastructure.JWT\nDefaul configuration values\n&quot;JWT&quot;: {\n&quot;Audience&quot;: &quot;devon4Net&quot;,\n&quot;Issuer&quot;: &quot;devon4Net&quot;,\n&quot;TokenExpirationTime&quot;: 60,\n&quot;ValidateIssuerSigningKey&quot;: true,\n&quot;ValidateLifetime&quot;: true,\n&quot;ClockSkew&quot;: 5,\n&quot;Security&quot;: {\n&quot;SecretKeyLengthAlgorithm&quot;: &quot;A256KW&quot;,\n&quot;SecretKeyEncryptionAlgorithm&quot;: &quot;A128CBC-HS256&quot;,\n&quot;SecretKey&quot;: &quot;tMFjgSOahNU5Cm4WV7ncY6EeARwqpb1x&quot;,\n&quot;Certificate&quot;: &quot;localhost.pfx&quot;,\n&quot;CertificatePassword&quot;: &quot;localhost&quot;,\n&quot;CertificateEncryptionAlgorithm&quot;: &quot;SHA512&quot;\n}\n}\nClockSkew indicates the token expiration time in minutes\nCertificate you can specify the name of your certificate (if it is on the same path) or the full path of the certificate. If the certificate does not exists an exception will be raised.\nSecretKeyLengthAlgorithm, SecretKeyEncryptionAlgorithm and CertificateEncryptionAlgorithm supported algorithms are:\nAlgorithm\nDescription\nAes128Encryption\n&quot;http://www.w3.org/2001/04/xmlenc#aes128-cbc&quot;\nAes192Encryption\n&quot;http://www.w3.org/2001/04/xmlenc#aes192-cbc&quot;\nAes256Encryption\n&quot;http://www.w3.org/2001/04/xmlenc#aes256-cbc&quot;\nDesEncryption\n&quot;http://www.w3.org/2001/04/xmlenc#des-cbc&quot;\nAes128KeyWrap\n&quot;http://www.w3.org/2001/04/xmlenc#kw-aes128&quot;\nAes192KeyWrap\n&quot;http://www.w3.org/2001/04/xmlenc#kw-aes192&quot;\nAes256KeyWrap\n&quot;http://www.w3.org/2001/04/xmlenc#kw-aes256&quot;\nRsaV15KeyWrap\n&quot;http://www.w3.org/2001/04/xmlenc#rsa-1_5&quot;\nRipemd160Digest\n&quot;http://www.w3.org/2001/04/xmlenc#ripemd160&quot;\nRsaOaepKeyWrap\n&quot;http://www.w3.org/2001/04/xmlenc#rsa-oaep&quot;\nAes128KW\n&quot;A128KW&quot;\nAes256KW\n&quot;A256KW&quot;\nRsaPKCS1\n&quot;RSA1_5&quot;\nRsaOAEP\n&quot;RSA-OAEP&quot;\nExclusiveC14n\n&quot;http://www.w3.org/2001/10/xml-exc-c14n#&quot;\nExclusiveC14nWithComments\n&quot;http://www.w3.org/2001/10/xml-exc-c14n#WithComments&quot;\nEnvelopedSignature\n&quot;http://www.w3.org/2000/09/xmldsig#enveloped-signature&quot;\nSha256Digest\n&quot;http://www.w3.org/2001/04/xmlenc#sha256&quot;\nSha384Digest\n&quot;http://www.w3.org/2001/04/xmldsig-more#sha384&quot;\nSha512Digest\n&quot;http://www.w3.org/2001/04/xmlenc#sha512&quot;\nSha256\n&quot;SHA256&quot;\nSha384\n&quot;SHA384&quot;\nSha512\n&quot;SHA512&quot;\nEcdsaSha256Signature\n&quot;http://www.w3.org/2001/04/xmldsig-more#ecdsa-sha256&quot;\nEcdsaSha384Signature\n&quot;http://www.w3.org/2001/04/xmldsig-more#ecdsa-sha384&quot;\nEcdsaSha512Signature\n&quot;http://www.w3.org/2001/04/xmldsig-more#ecdsa-sha512&quot;\nHmacSha256Signature\n&quot;http://www.w3.org/2001/04/xmldsig-more#hmac-sha256&quot;\nHmacSha384Signature\n&quot;http://www.w3.org/2001/04/xmldsig-more#hmac-sha384&quot;\nHmacSha512Signature\n&quot;http://www.w3.org/2001/04/xmldsig-more#hmac-sha512&quot;\nRsaSha256Signature\n&quot;http://www.w3.org/2001/04/xmldsig-more#rsa-sha256&quot;\nRsaSha384Signature\n&quot;http://www.w3.org/2001/04/xmldsig-more#rsa-sha384&quot;\nRsaSha512Signature\n&quot;http://www.w3.org/2001/04/xmldsig-more#rsa-sha512&quot;\nRsaSsaPssSha256Signature\n&quot;http://www.w3.org/2007/05/xmldsig-more#sha256-rsa-MGF1&quot;\nRsaSsaPssSha384Signature\n&quot;http://www.w3.org/2007/05/xmldsig-more#sha384-rsa-MGF1&quot;\nRsaSsaPssSha512Signature\n&quot;http://www.w3.org/2007/05/xmldsig-more#sha512-rsa-MGF1&quot;\nEcdsaSha256\n&quot;ES256&quot;\nEcdsaSha384\n&quot;ES384&quot;\nEcdsaSha512\n&quot;ES512&quot;\nHmacSha256\n&quot;HS256&quot;\nHmacSha384\n&quot;HS384&quot;\nHmacSha512\n&quot;HS512&quot;\nNone\n&quot;none&quot;\nRsaSha256\n&quot;RS256&quot;\nRsaSha384\n&quot;RS384&quot;\nRsaSha512\n&quot;RS512&quot;\nRsaSsaPssSha256\n&quot;PS256&quot;\nRsaSsaPssSha384\n&quot;PS384&quot;\nRsaSsaPssSha512\n&quot;PS512&quot;\nAes128CbcHmacSha256\n&quot;A128CBC-HS256&quot;\nAes192CbcHmacSha384\n&quot;A192CBC-HS384&quot;\nAes256CbcHmacSha512\n&quot;A256CBC-HS512&quot;\nPlease check Microsoft documentation to get the lastest updates on supported encryption algorithms\nAdd this line of code (only if you use this component stand alone):\nservices.AddBusinessCommonJwtPolicy();\nOn\nStartup.cs\nor on:\ndevon4Net.Application.Configuration.Startup/JwtApplicationConfiguration/ConfigureJwtPolicy method.\nInside the AddBusinessCommonJwtPolicy method you can add your JWT Policy like in My Thai Star application sample:\nservices.ConfigureJwtAddPolicy(&quot;MTSWaiterPolicy&quot;, &quot;role&quot;, &quot;waiter&quot;);\nNotes\nThe certificate will be used to generate the key to encrypt the json web token.\ndevon4Net.Infrastructure.Middleware\nDescription\ndevon4Net support for middleware classes.\nIn ASP.NET Core, middleware classes can handle an HTTP request or response. Middleware can either:\nHandle an incoming HTTP request by generating an HTTP response.\nProcess an incoming HTTP request, modify it, and pass it on to another piece of middleware.\nProcess an outgoing HTTP response, modify it, and pass it on to either another piece of middleware, or the ASP.NET Core web server.\ndevon4Net supports the following automatic response headers:\nAccessControlExposeHeader\nStrictTransportSecurityHeader\nXFrameOptionsHeader\nXssProtectionHeader\nXContentTypeOptionsHeader\nContentSecurityPolicyHeader\nPermittedCrossDomainPoliciesHeader\nReferrerPolicyHeader:toc: macro\nConfiguration\nInstall package on your solution:\nPM&gt; Install-Package devon4Net.Infrastructure.Middleware\nYou can configure your Middleware configuration on appsettings.{environment}.json:\n&quot;Middleware&quot;: {\n&quot;Headers&quot;: {\n&quot;AccessControlExposeHeader&quot;: &quot;Authorization&quot;,\n&quot;StrictTransportSecurityHeader&quot;: &quot;&quot;,\n&quot;XFrameOptionsHeader&quot;: &quot;DENY&quot;,\n&quot;XssProtectionHeader&quot;: &quot;1;mode=block&quot;,\n&quot;XContentTypeOptionsHeader&quot;: &quot;nosniff&quot;,\n&quot;ContentSecurityPolicyHeader&quot;: &quot;&quot;,\n&quot;PermittedCrossDomainPoliciesHeader&quot;: &quot;&quot;,\n&quot;ReferrerPolicyHeader&quot;: &quot;&quot;\n}\n}\nOn the above sample, the server application will add to response header the AccessControlExposeHeader, XFrameOptionsHeader, XssProtectionHeader and XContentTypeOptionsHeader headers.\nIf the header response type does not have a value, it will not be added to the response headers.\ndevon4Net.Infrastructure.Swagger\nDescription\ndevon4net Swagger abstraction to provide full externalized easy configuration.\nSwagger offers the easiest to use tools to take full advantage of all the capabilities of the OpenAPI Specification (OAS).\nConfiguration\nInstall package on your solution:\nPM&gt; devon4Net.Infrastructure.Swagger\nYou can configure your Swagger configuration on appsettings.{environment}.json:\n&quot;Swagger&quot;: {\n&quot;Version&quot;: &quot;v1&quot;,\n&quot;Title&quot;: &quot;devon4net API&quot;,\n&quot;Description&quot;: &quot;devon4net API Contract&quot;,\n&quot;Terms&quot;: &quot;https://www.devonfw.com/terms-of-use/&quot;,\n&quot;Contact&quot;: {\n&quot;Name&quot;: &quot;devonfw&quot;,\n&quot;Email&quot;: &quot;icsddevonfwsupport.apps2@capgemini.com&quot;,\n&quot;Url&quot;: &quot;https://www.devonfw.com&quot;\n},\n&quot;License&quot;: {\n&quot;Name&quot;: &quot;devonfw - Terms of Use&quot;,\n&quot;Url&quot;: &quot;https://www.devonfw.com/terms-of-use/&quot;\n},\n&quot;Endpoint&quot;: {\n&quot;Name&quot;: &quot;V1 Docs&quot;,\n&quot;Url&quot;: &quot;/swagger/v1/swagger.json&quot;,\n&quot;UrlUi&quot;: &quot;swagger&quot;,\n&quot;RouteTemplate&quot;: &quot;swagger/v1/{documentName}/swagger.json&quot;\n}\n}\nAdd this line of code (only if you use this component stand alone):\nservices.ConfigureSwaggerService();\nOn\nStartup.cs\nAlso add this line of code (only if you use this component stand alone):\napp.ConfigureSwaggerApplication();\nOn\nStartup.cs/Configure(IApplicationBuilder app, IHostingEnvironment env)\nEnsure your API actions and non-route parameters are decorated with explicit &quot;Http&quot; and &quot;From&quot; bindings.\nNotes\nTo access to swagger UI launch your API project and type in your html browser the url http://localhost:yourPort/swagger.\nIn order to generate the documentation annotate your actions with summary, remarks and response tags:\n/// &lt;summary&gt;\n/// Method to make a reservation with potential guests. The method returns the reservation token with the format: {(CB_|GB_)}{now.Year}{now.Month:00}{now.Day:00}{_}{MD5({Host/Guest-email}{now.Year}{now.Month:00}{now.Day:00}{now.Hour:00}{now.Minute:00}{now.Second:00})}\n/// &lt;/summary&gt;\n/// &lt;param name=&quot;bookingDto&quot;&gt;&lt;/param&gt;\n/// &lt;response code=&quot;201&quot;&gt;Ok.&lt;/response&gt;\n/// &lt;response code=&quot;400&quot;&gt;Bad request. Parser data error.&lt;/response&gt;\n/// &lt;response code=&quot;401&quot;&gt;Unauthorized. Authentication fail.&lt;/response&gt;\n/// &lt;response code=&quot;403&quot;&gt;Forbidden. Authorization error.&lt;/response&gt;\n/// &lt;response code=&quot;500&quot;&gt;Internal Server Error. The search process ended with error.&lt;/response&gt;\n[HttpPost]\n[HttpOptions]\n[Route(&quot;/mythaistar/services/rest/bookingmanagement/v1/booking&quot;)]\n[AllowAnonymous]\n[EnableCors(&quot;CorsPolicy&quot;)]\npublic async Task&lt;IActionResult&gt; BookingBooking([FromBody]BookingDto bookingDto)\n{\ntry\n{\n...\nEnsure that your project has the generate XML documentation file check active on build menu:\nFigure 99. Swagger documentation\nEnsure that your XML files has the attribute copy always to true:\nFigure 100. Swagger documentation\ndevon4Net.Infrastructure.Test\nDescription\ndevon4Net Base classes to create unit tests and integration tests with Moq and xUnit.\nConfiguration\nLoad the template:\n&gt; dotnet new -i devon4Net.Test.Template\n&gt; dotnet new devon4NetTest\nNotes\nAt this point you can find this classes:\nBaseManagementTest\nDatabaseManagementTest&lt;T&gt; (Where T is a devon4NetBaseContext class)\nFor unit testing, inherit a class from BaseManagementTest.\nFor integration tests, inherit a class from DatabaseManagementTest.\nThe recommended databases in integration test are in memory database or SQlite database.\nPlease check My thai Star test project.\n25.1.3. Deperecated packages\ndevon4Net.Domain.Context\nDescription\ndevon4Net.Domain.Context contains the extended class devon4NetBaseContext in order to make easier the process of having a model context configured against different database engines. This configuration allows an easier testing configuration against local and in memory databases.\nConfiguration\nInstall package on your solution:\nPM&gt; Install-Package devon4Net.Domain.Context\nAdd to appsettings.{environment}.json file your database connections:\n&quot;ConnectionStrings&quot;:\n{\n&quot;DefaultConnection&quot;:\n&quot;Server=localhost;Database=MyThaiStar;User Id=sa;Password=sa;MultipleActiveResultSets=True;&quot;,\n&quot;AuthConnection&quot;:\n&quot;Server=(localdb)\\\\mssqllocaldb;Database=aspnet-DualAuthCore-5E206A0B-D4DA-4E71-92D3-87FD6B120C5E;Trusted_Connection=True;MultipleActiveResultSets=true&quot;,\n&quot;SqliteConnection&quot;: &quot;Data Source=c:\\\\tmp\\\\membership.db;&quot;\n}\nOn Startup.cs :\nvoid ConfigureServices(IServiceCollection services)\nAdd your database connections defined on previous point:\nservices.ConfigureDataBase(\nnew Dictionary&lt;string, string&gt; {\n{ConfigurationConst.DefaultConnection, Configuration.GetConnectionString(ConfigurationConst.DefaultConnection) }});\nOn devon4Net.Application.Configuration.Startup/DataBaseConfiguration/ConfigureDataBase configure your connections.\ndevon4Net.Infrastructure.ApplicationUser\nDescription\ndevon4Net Application user classes to implement basic Microsoft&#x2019;s basic authentication in order to be used on authentication methodologies such Jason Web Token (JWT).\nConfiguration\nInstall package on your solution:\nPM&gt; devon4Net.Infrastructure.ApplicationUser\nAdd the database connection string for user management on appsettings.{environment}.json:\n&quot;ConnectionStrings&quot;:\n{\n&quot;AuthConnection&quot;:\n&quot;Server=(localdb)\\\\mssqllocaldb;Database=aspnet-DualAuthCore-5E206A0B-D4DA-4E71-92D3-87FD6B120C5E;Trusted_Connection=True;MultipleActiveResultSets=true&quot;\n}\nAdd the following line of code\nservices.AddApplicationUserDependencyInjection();\nOn\nStartup.cs/ConfigureServices(IServiceCollection services)\nor on:\ndevon4Net.Application.Configuration.Startup/DependencyInjectionConfiguration/ConfigureDependencyInjectionService method.\nAdd the data seeder on Configure method on start.cs class:\npublic void Configure(IApplicationBuilder app, IHostingEnvironment env, DataSeeder seeder)\n{\n...\napp.UseAuthentication();\nseeder.SeedAsync().Wait();\n...\n}\nNotes\nYou can use the following methods to set up the database configuration:\npublic static void AddApplicationUserDbContextInMemoryService(this IServiceCollection services)\npublic static void AddApplicationUserDbContextSQliteService(this IServiceCollection services, string connectionString)\npublic static void AddApplicationUserDbContextSQlServerService(this IServiceCollection services, string connectionString)\nThe method AddApplicationUserDbContextInMemoryService uses the AuthContext connection string name to set up the database.\nThis component is used with the components devon4Net.Infrastructure.JWT and devon4Net.Infrastructure.JWT.MVC.\ndevon4Net.Infrastructure.Communication\nDescription\nBasic client classes to invoke GET/POST methods asynchronously. This component has the minimal classes to send basic data. For more complex operations please use ASP4Net.Infrastructure.Extensions.\nConfiguration\nInstall package on your solution:\nPM&gt; devon4Net.Infrastructure.Communication\nCreate an instance of RestManagementService class.\nUse next methods to use GET/POST basic options:\npublic Task&lt;string&gt; CallGetMethod(string url);\npublic Task&lt;Stream&gt; CallGetMethodAsStream(string url);\npublic Task&lt;string&gt; CallPostMethod&lt;T&gt;(string url, T dataToSend);\npublic Task&lt;string&gt; CallPutMethod&lt;T&gt;(string url, T dataToSend);\nNotes\nExample:\nprivate async Task RestManagementServiceSample(EmailDto dataToSend)\n{\nvar url = Configuration[&quot;EmailServiceUrl&quot;];\nvar restManagementService = new RestManagementService();\nawait restManagementService.CallPostMethod(url, dataToSend);\n}\ndevon4Net.Infrastructure.JWT.MVC\nDescription\ndevon4Net Extended controller to interact with JWT features\nConfiguration\nExtend your _ Microsoft.AspNetCore.Mvc.Controller_ class with devon4NetJWTController class:\npublic class LoginController : devon4NetJWTController\n{\nprivate readonly ILoginService _loginService;\npublic LoginController(ILoginService loginService, SignInManager&lt;ApplicationUser&gt; signInManager, UserManager&lt;ApplicationUser&gt; userManager, ILogger&lt;LoginController&gt; logger, IMapper mapper) : base(logger,mapper)\n{\n_loginService = loginService;\n}\n....\nNotes\nIn order to generate a JWT, you should implement the JWT generation on user login. For example, in My Thai Star is created as follows:\npublic async Task&lt;IActionResult&gt; Login([FromBody]LoginDto loginDto)\n{\ntry\n{\nif (loginDto == null) return Ok();\nvar logged = await _loginService.LoginAsync(loginDto.UserName, loginDto.Password);\nif (logged)\n{\nvar user = await _loginService.GetUserByUserNameAsync(loginDto.UserName);\nvar encodedJwt = new JwtClientToken().CreateClientToken(_loginService.GetUserClaimsAsync(user));\nResponse.Headers.Add(&quot;Access-Control-Expose-Headers&quot;, &quot;Authorization&quot;);\nResponse.Headers.Add(&quot;Authorization&quot;, $&quot;{JwtBearerDefaults.AuthenticationScheme} {encodedJwt}&quot;);\nreturn Ok(encodedJwt);\n}\nelse\n{\nResponse.Headers.Clear();\nreturn StatusCode((int)HttpStatusCode.Unauthorized, &quot;Login Error&quot;);\n}\n}\ncatch (Exception ex)\n{\nreturn StatusCode((int)HttpStatusCode.InternalServerError, $&quot;{ex.Message} : {ex.InnerException}&quot;);\n}\n}\nIn My Thai Star the JWT will contain the user information such id, roles&#x2026;&#x200B;\nOnce you extend your controller with devon4NetJWTController you will have available these methods to simplify user management:\npublic interface Idevon4NetJWTController\n{\n// Gets the current user\nJwtSecurityToken GetCurrentUser();\n// Gets an specific assigned claim of current user\nClaim GetUserClaim(string claimName, JwtSecurityToken jwtUser = null);\n// Gets all the assigned claims of current user\nIEnumerable&lt;Claim&gt; GetUserClaims(JwtSecurityToken jwtUser = null);\n}\ndevon4Net.Infrastructure.MVC\nDescription\nCommon classes to extend controller functionality on API. Also provides support for paged results in devon4Net applications and automapper injected class.\nConfiguration\nInstall package on your solution:\nPM&gt; devon4Net.Infrastructure.MVC\nNotes\nThe generic class ResultObjectDto&lt;T&gt; provides a typed result object with pagination.\nThe extended class provides the following methods:\nResultObjectDto&lt;T&gt; GenerateResultDto&lt;T&gt;(int? page, int? size, int? total);\nResultObjectDto&lt;T&gt; GenerateResultDto&lt;T&gt;(List&lt;T&gt; result, int? page = null, int? size = null);\nGenerateResultDto provides typed ResultObjectDto object or a list of typed ResultObjectDto object. The aim of this methods is to provide a clean management for result objects and not repeating code through the different controller classes.\nThe following sample from My Thai Star shows how to use it:\npublic async Task&lt;IActionResult&gt; Search([FromBody] FilterDtoSearchObject filterDto)\n{\nif (filterDto == null) filterDto = new FilterDtoSearchObject();\ntry\n{\nvar dishList = await _dishService.GetDishListFromFilter(false, filterDto.GetMaxPrice(), filterDto.GetMinLikes(), filterDto.GetSearchBy(),filterDto.GetCategories(), -1);\nreturn new OkObjectResult(GenerateResultDto(dishList).ToJson());\n}\ncatch (Exception ex)\n{\nreturn StatusCode((int)HttpStatusCode.InternalServerError, $&quot;{ex.Message} : {ex.InnerException}&quot;);\n}\n}\ndevon4Net.Infrastructure.AOP\nDescription\nSimple AOP Exception handler for .Net Controller classes integrated with Serilog.\nConfiguration\nInstall package on your solution:\nPM&gt; Install-Package devon4Net.Domain.AOP\nAdd this line of code on ConfigureServices method on Startup.cs\nservices.AddAopAttributeService();\nNotes\nNow automatically your exposed API methods exposed on controller classes will be tracked on the methods:\nOnActionExecuting\nOnActionExecuted\nOnResultExecuting\nOnResultExecuted\nIf an exception occurs, a message will be displayed on log with the stack trace.\ndevon4Net.Infrastructure.Cors\nDescription\nEnables CORS configuration for devon4Net application. Multiple domains can be configured from configuration. Mandatory to web clients (p.e. Angular) to prevent making AJAX requests to another domain.\nCross-Origin Resource Sharing (CORS) is a mechanism that uses additional HTTP headers to tell a browser to let a web application running at one origin (domain) have permission to access selected resources from a server at a different origin. A web application makes a cross-origin HTTP request when it requests a resource that has a different origin (domain, protocol, and port) than its own origin.\nPlease refer to this link to get more information about CORS and .Net core.\nConfiguration\nInstall package on your solution:\nPM&gt; devon4Net.Infrastructure.Cors\nYou can configure your Cors configuration on appsettings.{environment}.json:\nCorsPolicy: indicates the name of the policy. You can use this name to add security headers on your API exposed methods.\nOrigins: The allowed domains\nHeaders: The allowed headers such accept,content-type,origin,x-custom-header\nIf you specify the cors configuration as empty array, a default cors-policy will be used with all origins enabled:\n&quot;Cors&quot;: []\nOn the other hand, you can specify different Cors policies in your solution as follows:\n&quot;Cors&quot;: []\n[\n{\n&quot;CorsPolicy&quot;: &quot;CorsPolicy1&quot;,\n&quot;Origins&quot;: &quot;http:example.com,http:www.contoso.com&quot;,\n&quot;Headers&quot;: &quot;accept,content-type,origin,x-custom-header&quot;,\n&quot;Methods&quot;: &quot;GET,POST,HEAD&quot;,\n&quot;AllowCredentials&quot;: true\n},\n{\n&quot;CorsPolicy&quot;: &quot;CorsPolicy2&quot;,\n&quot;Origins&quot;: &quot;http:example.com,http:www.contoso.com&quot;,\n&quot;Headers&quot;: &quot;accept,content-type,origin,x-custom-header&quot;,\n&quot;Methods&quot;: &quot;GET,POST,HEAD&quot;,\n&quot;AllowCredentials&quot;: true\n}\n]\nNotes\nTo use CORS in your API methods, use the next notation:\n[EnableCors(&quot;YourCorsPolicy&quot;)]\npublic IActionResult Index() {\nreturn View();\n}\nif you want to disable the CORS check use the following annotation:\n[DisableCors]\npublic IActionResult Index() {\nreturn View();\n}\n25.2. Required software\nVisual Studio Code\nC# Extension for VS Code\n.Net Core SDK\nCORS in .Net Core\n&#x2190;&#xA0;Previous:&#xA0;User guide&#xA0;| &#x2191;&#xA0;Up:&#xA0;devon4net&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Templates&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devon4net.asciidoc_samples.html","title":"27. Samples","body":"\n27. Samples\n27.1. Samples\n27.1.1. My Thai Star Restaurant\nMy Thai Star (.Net Core Server + Angular client)\nYour browser does not support the video tag.\nAngular requirements\nNode\nAngular CLI\nYarn\nAngular client\nInstall Node.js LTS version\nInstall Angular CLI from command line:\nnpm install -g @angular/cli\nInstall Yarn\nGo to Angular client from command line\nExecute : yarn install\nLaunch the app from command line: ng serve and check http://localhost:4200\nYou are ready\n.Net Core server\nBasic architecture details\nFollowing the OASP conventions the .Net Core 2.0 My Thai Star backend is going to be developed dividing the application in Components and using a n-layer architecture.\nComponents\nThe application is going to be divided in different components to encapsulate the different domains of the application functionalities.\nAs main components we will find:\n_BookingService: Manages the bookings part of the application. With this component the users (anonymous/logged in) can create new bookings or cancel an existing booking. The users with waiter role can see all scheduled bookings.\nOrderService: This component handles the process to order dishes (related to bookings). A user (as a host or as a guest) can create orders (that contain dishes) or cancel an existing one. The users with waiter role can see all ordered orders.\nDishService: This component groups the logic related to the menu (dishes) view. Its main feature is to provide the client with the data of the available dishes but also can be used by other components (Ordermanagement) as a data provider in some processes.\nUserService: Takes care of the User Profile management, allowing to create and update the data profiles.\nAs common components (that don&#x2019;t exactly represent an application&#x2019;s area but provide functionalities that can be used by the main components):\nMailservice: with this service we will provide the functionality for sending email notifications. This is a shared service between different app components such as bookingmanagement or ordercomponent.\nOther components:\nSecurity (will manage the access to the private part of the application using a jwt implementation).\nTwitter integration: planned as a Microservice will provide the twitter integration needed for some specific functionalities of the application.\nLayers\nIntroduction\nThe .Net Core backend for My Thai Star application is going to be based on:\nOASP4NET as the .Net Core framework\nVSCode as the Development environment\nTOBAGO as code generation tool\nApplication layer\nThis layer will expose the REST api to exchange information with the client applications.\nThe application will expose the services on port 8081 and it can be launched as a self host console application (microservice approach) and as a Web Api application hosted on IIS/IIS Express.\nBusiness layer\nThis layer will define the controllers which will be used on the application layer to expose the different services. Also, will define the swagger contract making use of summary comments and framework attributes.\nThis layer also includes the object response classes in order to interact with external clients.\nService layer\nThe layer in charge of hosting the business logic of the application. Also orchestrates the object conversion between object response and entity objects defined in Data layer.\nData layer\nThe layer to communicate with the data base.\nData layer makes use of Entity Framework.\nThe Database context is defined on DataAccessLayer assembly (ModelContext).\nThis layer makes use of the Repository pattern and Unit of work in order to encapsulate the complexity. Making use of this combined patterns we ensure an organized and easy work model.\nAs in the previous layers, the data access layer will have both interface and implementation tiers. However, in this case, the implementation will be slightly different due to the use of generics.\nCross-Cutting concerns\nthe layer to make use of transversal components such JWT and mailing.\nJwt basics\nA user will provide a username / password combination to our auth server.\nThe auth server will try to identify the user and, if the credentials match, will issue a token.\nThe user will send the token as the Authorization header to access resources on server protected by JWT Authentication.\nJwt implementation details\nThe Json Web Token pattern will be implemented based on the jwt on .net core framework that is provided by default in the Oasp4Net projects.\nAuthentication\nBased on Microsoft approach, we will implement a class to define the security entry point and filters. Also, as My Thai Star is a mainly public application, we will define here the resources that won&#x2019;t be secured.\nOn Oasp4Net.Infrastructure.JWT assembly is defined a subset of Microsoft&#x2019;s authorization schema Database. It is started up the first time the application launches.\nYOu can read more about _Authorization on:\nAuthorization in ASP.NET Core\nClaim based authorization\nDependency injection\nAs it is explained in the Microsoft documentation we are going to implement the dependency injection pattern basing our solution on .Net Core.\nSeparation of API and implementation: Inside each layer we will separate the elements in different tiers: interface and implementation. The interface tier will store the interface with the methods definition and inside the implementation we will store the class that implements the interface.\nLayer communication method\nThe connection between layers, to access to the functionalities of each one, will be solved using the dependency injection.\nConnection BookingService - Logic\npublic class BookingService : EntityService&lt;Booking&gt;, IBookingService\n{\nprivate readonly IBookingRepository _bookingRepository;\nprivate readonly IRepository&lt;Order&gt; _orderRepository;\nprivate readonly IRepository&lt;InvitedGuest&gt; _invitedGuestRepository;\nprivate readonly IOrderLineRepository _orderLineRepository;\nprivate readonly IUnitOfWork _unitOfWork;\npublic BookingService(IUnitOfWork unitOfWork,\nIBookingRepository repository,\nIRepository&lt;Order&gt; orderRepository,\nIRepository&lt;InvitedGuest&gt; invitedGuestRepository,\nIOrderLineRepository orderLineRepository) : base(unitOfWork, repository)\n{\n_unitOfWork = unitOfWork;\n_bookingRepository = repository;\n_orderRepository = orderRepository;\n_invitedGuestRepository = invitedGuestRepository;\n_orderLineRepository = orderLineRepository;\n}\n}\nTo give service to the defined User Stories we will need to implement the following services:\nprovide all available dishes.\nsave a booking.\nsave an order.\nprovide a list of bookings (only for waiters) and allow filtering.\nprovide a list of orders (only for waiters) and allow filtering.\nlogin service (see the Security section).\nprovide the current user data (see the Security section)\nFollowing the [naming conventions] proposed for Oasp4Net applications we will define the following end points for the listed services.\n(POST) /mythaistar/services/rest/dishmanagement/v1/dish/search.\n(POST) /mythaistar/services/rest/bookingmanagement/v1/booking.\n(POST) /mythaistar/services/rest/ordermanagement/v1/order.\n(POST) /mythaistar/services/rest/bookingmanagement/v1/booking/search.\n(POST) /mythaistar/services/rest/ordermanagement/v1/order/search.\n(POST) /mythaistar/services/rest/ordermanagement/v1/order/filter (to filter with fields that does not belong to the Order entity).\n(POST) /mythaistar/login.\n(GET) /mythaistar/services/rest/security/v1/currentuser/.\nYou can find all the details for the services implementation in the Swagger definition included in the My Thai Star project on Github.\nApi Exposed\nThe Oasp4Net.Business.Controller assembly in the business layer of a component will store the definition of the service by a interface. In this definition of the service we will set-up the endpoints of the service, the type of data expected and returned, the HTTP method for each endpoint of the service and other configurations if needed.\n/// &lt;summary&gt;\n/// Method to make a reservation with potential guests. The method returns the reservation token with the format: {(CB_|GB_)}{now.Year}{now.Month:00}{now.Day:00}{_}{MD5({Host/Guest-email}{now.Year}{now.Month:00}{now.Day:00}{now.Hour:00}{now.Minute:00}{now.Second:00})}\n/// &lt;/summary&gt;\n/// &lt;param name=&quot;bookingView&quot;&gt;&lt;/param&gt;\n/// &lt;response code=&quot;201&quot;&gt;Ok.&lt;/response&gt;\n/// &lt;response code=&quot;400&quot;&gt;Bad request. Parser data error.&lt;/response&gt;\n/// &lt;response code=&quot;401&quot;&gt;Unauthorized. Authentication fail.&lt;/response&gt;\n/// &lt;response code=&quot;403&quot;&gt;Forbidden. Authorization error.&lt;/response&gt;\n/// &lt;response code=&quot;500&quot;&gt;Internal Server Error. The search process ended with error.&lt;/response&gt;\n[HttpPost]\n[HttpOptions]\n[Route(&quot;/mythaistar/services/rest/bookingmanagement/v1/booking&quot;)]\n[AllowAnonymous]\n[EnableCors(&quot;CorsPolicy&quot;)]\npublic IActionResult BookingBooking([FromBody]BookingView bookingView)\n{\n...\nUsing the summary annotations and attributes will tell to swagger the contract via the XML doc generated on compiling time. This doc will be stored in XmlDocumentation folder.\nThe Api methods will be exposed on the application layer.\n27.1.2. Google Mail API Consumer\nGoogle Mail API Consumer\nApplication\nMyThaiStarEmailService.exe\nConfig file\nMyThaiStarEmailService.exe.Config\nDefault port\n8080\nOverview\nExecute MyThaiStarEmailService.exe.\nThe first time google will ask you for credentials\n(just one time) in your default browser:\nAccount: mythaistarrestaurant@gmail.com\nPassword: mythaistarrestaurant2501\nVisit the url: http://localhost:8080/swagger\nYour server is ready!\nFigure 101. GMail Server Swagger contract page\nJSON Example\nThis is the JSON example to test with swagger client. Please read the swagger documentation.\n{\n&quot;EmailFrom&quot;:&quot;mythaistarrestaurant@gmail.com&quot;,\n&quot;EmailAndTokenTo&quot;:{\n&quot;MD5Token1&quot;:&quot; Email_Here!@gmail.com&quot;,\n&quot;MD5Token2&quot;:&quot; Email_Here!@gmail.com&quot;\n},\n&quot;EmailType&quot;:0,\n&quot;DetailMenu&quot;:[\n&quot;Thai Spicy Basil Fried Rice x2&quot;,\n&quot;Thai green chicken curry x2&quot;\n],\n&quot;BookingDate&quot;:&quot;2017-05-31T12:53:39.7864723+02:00&quot;,\n&quot;Assistants&quot;:2,\n&quot;BookingToken&quot;:&quot;MD5Booking&quot;,\n&quot;Price&quot;:20.0,\n&quot;ButtonActionList&quot;:{\n&quot;http://accept.url&quot;:&quot;Accept&quot;,\n&quot;http://cancel.url&quot;:&quot;Cancel&quot;\n},\n&quot;Host&quot;:{\n&quot; Email_Here!@gmail.com&quot;:&quot;Jos&#xE9; Manuel&quot;\n}\n}\nConfigure the service port\nIf you want to change the default port, please edit the config file and\nchange the next entry in appSettings node:\n&lt;appSettings&gt;\n&lt;add key=&quot;LocalListenPort&quot; value=&quot;8080&quot; /&gt;\n&lt;/appSettings&gt;\nExternal links\nGoogle API Account Configuration\nAbout Scopes\n27.1.3. Downloads\nMy Thai Star (.Net Core Server + Angular client)\nGoogle Mail API Consumer\n&#x2190;&#xA0;Previous:&#xA0;Templates&#xA0;| &#x2191;&#xA0;Up:&#xA0;devon4net&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;devon4node&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devon4net.asciidoc_templates.html","title":"26. Templates","body":"\n26. Templates\n26.1. Templates\n26.1.1. Overview\nThe .Net Core and .Net Framework given templates allows to start coding an application with the following functionality ready to use:\nPlease refer to User guide in order to start developing.S\n26.1.2. .Net Core 3.0\nThe .Net Core 3.0 template allows you to start developing an n-layer server application to provide the latest features. The template can be used in Visual Studio Code and Visual Studio 2019.\nThe application result can be deployed as a console application, microservice or web page.\nTo start developing with devon4Net template, please follow this instructions:\nUsing devon4Net template\nOption 1\nOpen your favourite terminal (Win/Linux/iOS)\nGo to future project&#x2019;s path\nType dotnet new --install Devon4Net.WebAPI.Template\nType dotnet new Devon4NetAPI\nGo to project&#x2019;s path\nYou are ready to start developing with devon4Net\nOption 1\nCreate a new dotnet API project from scracht\nAdd the nuget package reference to your project\nType dotnet new --install Devon4Net.WebAPI.Template\n26.1.3. .Net Core 2.1.x\nThe .Net Core 2.1.x template allows you to start developing an n-layer server application to provide the latest features. The template can be used in Visual Studio Code and Visual Studio 2017.\nThe application result can be deployed as a console application, microservice or web page.\nTo start developing with devon4Net template, please follow this instructions:\nUsing devon4Net template\nOpen your favourite terminal (Win/Linux/iOS)\nGo to future project&#x2019;s path\nType dotnet new --install Devon4Net.WebAPI.Template::1.0.8\nType dotnet new Devon4NetAPI\nGo to project&#x2019;s path\nYou are ready to start developing with devon4Net\nFor the latest updates on references packages, please get the sorces from Github\n26.1.4. Links\n.Net templates\n&#x2190;&#xA0;Previous:&#xA0;Packages&#xA0;| &#x2191;&#xA0;Up:&#xA0;devon4net&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Samples&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devon4net.asciidoc_user-guide.html","title":"24. User guide","body":"\n24. User guide\n24.1. devon4net Guide\n24.1.1. Introduction\nWelcome to devon4net framework user guide. In this document you will find the information regarding how to start and deploy your project using the guidelines proposed in our solution.\nAll the guidelines shown and used in this document are a set of rules and conventions proposed and supported by Microsoft and the industry.\n24.1.2. The package\nDevon4Net package solution contains:\nFile / Folder\nContent\nDocumentation\nUser documentation in HTML format\nModules\nContains the source code of the different devon4net modules\nSamples\nDifferent samples implemented in .NET and .NET Core. Also includes My Thai Star Devon flagship restaurant application\nTemplates\nMain .net Core template to start developing from scratch\nLicense\nLicense agreement\nREADME.md\nGithub main page\nTERMS_OF_USE.asciidoc\nThe devon4net terms of use\nLICENSE\nThe devon license\nOther files\nSuch the code of conduct and contributing guide\nApplication templates\nThe application templates given in the bundle are ready to use.\nAt the moment .net Core template is supported. The template is ready to be used as a simple console Kestrel application or being deployed in a web server like IIS.\nSamples\nMy Thai Star\nYou can find My Thai Star .NET port application at Github.\nAs devon4net has been migrated to the latest version of .net core, the template is not finished yet.\n24.1.3. Cookbook\nData management\nTo use EF Core, install the package for the database provider(s) you want to target. This walk-through uses SQL Server.\nFor a list of available providers see Database Providers\nGo to Tools &gt; NuGet Package Manager &gt; Package Manager Console\nRun Install-Package Microsoft.EntityFrameworkCore.SqlServer\nWe will be using some Entity Framework Tools to create a model from the database. So we will install the tools package as well:\nRun Install-Package Microsoft.EntityFrameworkCore.Tools\nWe will be using some ASP.NET Core Scaffolding tools to create controllers and views later on. So we will install this design package as well:\nRun Install-Package Microsoft.VisualStudio.Web.CodeGeneration.Design\nEF Code first\nIn order to design your database model from scratch, we encourage to follow the Microsoft guidelines described here.\nEF Database first\nGo to Tools &gt; NuGet Package Manager &gt; Package Manager Console\nRun the following command to create a model from the existing database:\nScaffold-DbContext &quot;Your connection string to existing database&quot; Microsoft.EntityFrameworkCore.SqlServer -OutputDir Models\nThe command will create the database context and the mapped entities as well inside of Models folder.\nRegister your context with dependency injection\nServices are registered with dependency injection during application startup.\nIn order to register your database context (or multiple database context as well) you can add the following line at ConfigureDbService method at startup.cs:\nservices.AddDbContext&lt;YourModelContext&gt;(\noptions =&gt; options.UseSqlServer(Configuration.GetConnectionString(&quot;Connection name at appsettings.json&quot;)));\nYou should put your Model and Entities in the template&#x2019;s devon4net.Domain.Entities project.\nRepositories and Services\nServices and Repositories are an important part of devon4net proposal. To make them work properly, first of all must be declared and injected at Startup.cs at DI Region.\nServices are declared in devon4net.Business.Common and injected in Controller classes when needed. Use services to build your application logic.\nFigure 95. Screenshot of devon4net.Business.Common project in depth\nFor example, My Thai Star Booking controller constructor looks like this:\npublic BookingController(IBookingService bookingService, IMapper mapper)\n{\nBookingService = bookingService;\nMapper = mapper;\n}\nCurrently devon4net has a Unit of Work class in order to perform CRUD operations to database making use of your designed model context.\nRepositories are declared at devon4net.Domain.UnitOfWork project and make use of Unit of Work class.\nThe common methods to perform CRUD operations (where &lt;T&gt; is an entity from your model) are:\nSync methods:\nIList&lt;T&gt; GetAll(Expression&lt;Func&lt;T, bool&gt;&gt; predicate = null);\nT Get(Expression&lt;Func&lt;T, bool&gt;&gt; predicate = null);\nIList&lt;T&gt; GetAllInclude(IList&lt;string&gt; include, Expression&lt;Func&lt;T, bool&gt;&gt; predicate = null);\nT Create(T entity);\nvoid Delete(T entity);\nvoid DeleteById(object id);\nvoid Delete(Expression&lt;Func&lt;T, bool&gt;&gt; where);\nvoid Edit(T entity);\nAsync methods:\nTask&lt;IList&lt;T&gt;&gt; GetAllAsync(Expression&lt;Func&lt;T, bool&gt;&gt; predicate = null);\nTask&lt;T&gt; GetAsync(Expression&lt;Func&lt;T, bool&gt;&gt; predicate = null);\nTask&lt;IList&lt;T&gt;&gt; GetAllIncludeAsync(IList&lt;string&gt; include, Expression&lt;Func&lt;T, bool&gt;&gt; predicate = null);\nIf you perform a Commit operation and an error happens, changes will be rolled back.\nSwagger integration\nThe given templates allow you to specify the API contract through Swagger integration and the controller classes are the responsible of exposing methods making use of comments in the source code.\nThe next example shows how to comment the method with summaries in order to define the contract. Add (Triple Slash) XML Documentation To Swagger:\n/// &lt;summary&gt;\n/// Method to get reservations\n/// &lt;/summary&gt;\n/// &lt;response code=&quot;201&quot;&gt;Ok.&lt;/response&gt;\n/// &lt;response code=&quot;400&quot;&gt;Bad request. Parser data error.&lt;/response&gt;\n/// &lt;response code=&quot;401&quot;&gt;Unauthorized. Authentication fail.&lt;/response&gt;\n/// &lt;response code=&quot;403&quot;&gt;Forbidden. Authorization error.&lt;/response&gt;\n/// &lt;response code=&quot;500&quot;&gt;Internal Server Error. The search process ended with error.&lt;/response&gt;\n[HttpPost]\n[Route(&quot;/mythaistar/services/rest/bookingmanagement/v1/booking/search&quot;)]\n//[Authorize(Policy = &quot;MTSWaiterPolicy&quot;)]\n[AllowAnonymous]\n[EnableCors(&quot;CorsPolicy&quot;)]\npublic async Task&lt;IActionResult&gt; BookingSearch([FromBody]BookingSearchDto bookingSearchDto)\n{\nIn order to be effective and make use of the comments to build the API contract, the project which contains the controller classes must generate the XML document file. To achieve this, the XML documentation file must be checked in project settings tab:\nFigure 96. Project settings tab\nWe propose to generate the file under the XmlDocumentation folder. For example in devon4net.Domain.Entities project in My Thai Star .NET implementation the output folder is:\nXmlDocumentation\\devon4net.Business.Common.xml\nThe file devon4net.Business.Common.xml won&#x2019;t appear until you build the project. Once the file is generated, please modify its properties as a resource and set it to be Copy always .\nFigure 97. Swagger XML document file properties\nOnce you have this, the swagger user interface will show the method properties defined in your controller comments.\nMaking use of this technique controller are not encapsulated to the application project. Also, you can develop your controller classes in different projects obtain code reusability.\nSwagger comment:\nComment\nFunctionality\n&lt;summary&gt;\nWill map to the operation&#x2019;s summary\n&lt;remarks&gt;\nWill map to the operation&#x2019;s description (shown as &quot;Implementation Notes&quot; in the UI)\n&lt;response code=&quot;###&quot;&gt;\nSpecifies the different response of the target method\n&lt;param&gt;\nWill define the parameter(s) of the target method\nPlease check Microsoft&#x2019;s site regarding to summary notations.\nLogging module\nAn important part of life software is the need of using log and traces. devon4net has a log module pre-configured to achieve this important point.\nBy default Microsoft provides a logging module on .NET Core applications. This module is open and can it can be extended. devon4net uses the serilog implementation. This implementation provides a huge quantity information about events and traces.\nLog file\ndevon4net can write the log information to a simple text file. You can configure the file name and folder at appsettings.json file (LogFile attribute) at devon4net.Application.WebApi project.\nDatabase log\ndevon4net can write the log information to a SQLite database. You can configure the file name and folder at appsettings.json file (LogDatabase attribute) at devon4net.Application.WebApi project.\nWith this method you can launch queries in order to search the information you are looking for.\nSeq log\ndevon4net can write the log information to a serilog server. You can configure the serilog URL at appsettings.json file (SeqLogServerUrl attribute) at devon4net.Application.WebApi project.\nWith this method you can make queries via HTTP.\nBy default you can find the log information at Logs folder.\nJWT module\nJSON Web Tokens are an open, industry standard RFC 7519 method for representing claims securely between two parties allowing you to decode, verify and generate JWT.\nYou should use JWT for:\nAuthentication : allowing the user to access routes, services, and resources that are permitted with that token.\nInformation Exchange: JSON Web Tokens are a good way of securely transmitting information between parties. Additionally, as the signature is calculated using the header and the payload, you can also verify that the content.\nThe JWT module is configured at Startup.cs inside devon4net.Application.WebApi project from .NET Core template. In this class you can configure the different authentication policy and JWT properties.\nOnce the user has been authenticated, the client perform the call to the backend with the attribute Bearer plus the token generated at server side.\nOn My Thai Star sample there are two predefined users: user0 and Waiter. Once they log in the application, the client (Angular/Xamarin) will manage the server call with the json web token. With this method we can manage the server authentication and authorization.\nYou can find more information about JWT at jwt.io\nAOP module\nAOP (Aspect Oriented Programming) tracks al information when a method is call. AOP also tracks the input and output data when a method is call.\nBy default devon4net has AOP module pre-configured and activated for controllers at Startup.cs file at devon4net.Application.WebApi:\noptions.Filters.Add(new Infrastructure.AOP.AopControllerAttribute(Log.Logger));\noptions.Filters.Add(new Infrastructure.AOP.AopExceptionFilter(Log.Logger));\nThis configuration allows all Controller classes to be tracked. If you don&#x2019;t need to track the info comment the lines written before.\nDocker support\ndevon4net Core projects are ready to be integrated with docker.\nMy Thai Star application sample is ready to be use with linux docker containers. The Readme file explains how to launch and setup the sample application.\nangular : Angular client to support backend. Just binaries.\ndatabase : Database scripts and .bak file\nmailservice: Microservice implementation to send notifications.\nnetcore: Server side using .net core 2.0.x.\nxamarin: Xamarin client based on Excalibur framework from The Netherlands using XForms.\nDocker configuration and docker-compose files are provided.\n24.1.4. Testing with XUnit\nxUnit.net is a free, open source, community-focused unit testing tool for the .NET Framework. Written by the original inventor of NUnit v2, xUnit.net is the latest technology for unit testing C#, F#, VB.NET and other .NET languages. xUnit.net works with ReSharper, CodeRush, TestDriven.NET and Xamarin. It is part of the .NET Foundation, and operates under their code of conduct. It is licensed under Apache 2 (an OSI approved license).\n&#x2014; About xUnit.net\nhttps://xunit.github.io/#documentation\nFacts are tests which are always true. They test invariant conditions.\nTheories are tests which are only true for a particular set of data.\nThe first test\nusing Xunit;\nnamespace MyFirstUnitTests\n{\npublic class Class1\n{\n[Fact]\npublic void PassingTest()\n{\nAssert.Equal(4, Add(2, 2));\n}\n[Fact]\npublic void FailingTest()\n{\nAssert.Equal(5, Add(2, 2));\n}\nint Add(int x, int y)\n{\nreturn x + y;\n}\n}\n}\nThe first test with theory\nTheory attribute is used to create tests with input params:\n[Theory]\n[InlineData(3)]\n[InlineData(5)]\n[InlineData(6)]\npublic void MyFirstTheory(int value)\n{\nAssert.True(IsOdd(value));\n}\nbool IsOdd(int value)\n{\nreturn value % 2 == 1;\n}\nCheat Sheet\nOperation\nExample\nTest\n[Fact]\npublic void Test()\n{\n}\nSetup\npublic class TestFixture {\npublic TestFixture()\n{\n&#x2026;&#x200B;\n}\n}\nTeardown\npublic class TestFixture : IDisposable\n{\npublic void Dispose() {\n&#x2026;&#x200B;\n}\n}\nConsole runner return codes\nCode\nMeaning\n0\nThe tests ran successfully.\n1\nOne or more of the tests failed.\n2\nThe help page was shown, either because it was requested, or because the user did not provide any command line arguments.\n3\nThere was a problem with one of the command line options passed to the runner.\n4\nThere was a problem loading one or more of the test assemblies (for example, if a 64-bit only assembly is run with the 32-bit test runner).\n24.1.5. Publishing\nNginx\nIn order to deploy your application to a Nginx server on Linux platform you can follow the instructions from Microsoft here.\nIIS\nIn this point is shown the configuration options that must implement the .Net Core application.\nSupported operating systems:\nWindows 7 and newer\nWindows Server 2008 R2 and newer*\nWebListener server will not work in a reverse-proxy configuration with IIS. You must use the Kestrel server.\nIIS configuration\nEnable the Web Server (IIS) role and establish role services.\nWindows desktop operating systems\nNavigate to Control Panel &gt; Programs &gt; Programs and Features &gt; Turn Windows features on or off (left side of the screen). Open the group for Internet Information Services and Web Management Tools. Check the box for IIS Management Console. Check the box for World Wide Web Services. Accept the default features for World Wide Web Services or customize the IIS features to suit your needs.\n*Conceptually, the IIS configuration described in this document also applies to hosting ASP.NET Core applications on Nano Server IIS, but refer to ASP.NET Core with IIS on Nano Server for specific instructions.\nWindows Server operating systems\nFor server operating systems, use the Add Roles and Features wizard via the Manage menu or the link in Server Manager. On the Server Roles step, check the box for Web Server (IIS).\nOn the Role services step, select the IIS role services you desire or accept the default role services provided.\nProceed through the Confirmation step to install the web server role and services. A server/IIS restart is not required after installing the Web Server (IIS) role.\nInstall the .NET Core Windows Server Hosting bundle\nInstall the .NET Core Windows Server Hosting bundle on the hosting system. The bundle will install the .NET Core Runtime, .NET Core Library, and the ASP.NET Core Module. The module creates the reverse-proxy between IIS and the Kestrel server. Note: If the system doesn&#x2019;t have an Internet connection, obtain and install the Microsoft Visual C++ 2015 Redistributable before installing the .NET Core Windows Server Hosting bundle.\nRestart the system or execute net stop was /y followed by net start w3svc from a command prompt to pick up a change to the system PATH.\nIf you use an IIS Shared Configuration, see ASP.NET Core Module with IIS Shared Configuration.\nTo configure IISIntegration service options, include a service configuration for IISOptions in ConfigureServices:\nservices.Configure&lt;IISOptions&gt;(options =&gt;\n{\n...\n});\nOption\nDefault\nSetting\nAutomaticAuthentication\ntrue\nIf true, the authentication middleware sets the HttpContext.User and responds to generic challenges. If false, the authentication middleware only provides an identity (HttpContext.User) and responds to challenges when explicitly requested by the AuthenticationScheme. Windows Authentication must be enabled in IIS for AutomaticAuthentication to function.\nAuthenticationDisplayName\nnull\nSets the display name shown to users on login pages.\nForwardClientCertificate\ntrue\nIf true and the MS-ASPNETCORE-CLIENTCERT request header is present, the HttpContext.Connection.ClientCertificate is populated.\nweb.config\nThe web.config file configures the ASP.NET Core Module and provides other IIS configuration. Creating, transforming, and publishing web.config is handled by Microsoft.NET.Sdk.Web, which is included when you set your project&#x2019;s SDK at the top of your .csproj file, &lt;Project Sdk=&quot;Microsoft.NET.Sdk.Web&quot;&gt;. To prevent the MSBuild target from transforming your web.config file, add the &lt;IsTransformWebConfigDisabled&gt; property to your project file with a setting of true:\n&lt;PropertyGroup&gt;\n&lt;IsTransformWebConfigDisabled&gt;true&lt;/IsTransformWebConfigDisabled&gt;\n&lt;/PropertyGroup&gt;\nAzure\nIn order to deploy your application to Azure platform you can follow the instructions from Microsoft:\nSet up the development environment\nInstall the latest&#xA0;Azure SDK for Visual Studio. The SDK installs Visual Studio if you don&#x2019;t already have it.\nVerify your&#xA0;Azure account. You can&#xA0;open a free Azure account&#xA0;or&#xA0;Activate Visual Studio subscriber benefits.\nCreate a web app\nIn the Visual Studio Start Page, select&#xA0;File &gt; New &gt; Project&#x2026;&#x200B;\nComplete the&#xA0;New Project&#xA0;dialog:\nIn the left pane, select&#xA0;.NET Core.\nIn the center pane, select&#xA0;ASP.NET Core Web Application.\nSelect&#xA0;OK.\nIn the&#xA0;New ASP.NET Core Web Application&#xA0;dialog:\nSelect&#xA0;Web Application.\nSelect&#xA0;Change Authentication.\nThe&#xA0;Change Authentication&#xA0;dialog appears.\nSelect&#xA0;Individual User Accounts.\nSelect&#xA0;OK&#xA0;to return to the&#xA0;New ASP.NET Core Web Application, then select&#xA0;OK&#xA0;again.\nVisual Studio creates the solution.\nRun the app locally\nChoose&#xA0;Debug&#xA0;then&#xA0;Start Without Debugging&#xA0;to run the app locally.\nClick the&#xA0;About&#xA0;and&#xA0;Contact&#xA0;links to verify the web application works.\nSelect&#xA0;Register&#xA0;and register a new user. You can use a fictitious email address. When you submit, the page displays the following error:\n&quot;Internal Server Error: A database operation failed while processing the request. SQL exception: Cannot open the database. Applying existing migrations for Application DB context may resolve this issue.&quot;\nSelect&#xA0;Apply Migrations&#xA0;and, once the page updates, refresh the page.\nThe app displays the email used to register the new user and a&#xA0;Log out&#xA0;link.\nDeploy the app to Azure\nClose the web page, return to Visual Studio, and select&#xA0;Stop Debugging&#xA0;from the&#xA0;Debug&#xA0;menu.\nRight-click on the project in Solution Explorer and select&#xA0;Publish&#x2026;&#x200B;.\nIn the&#xA0;Publish&#xA0;dialog, select&#xA0;Microsoft Azure App Service&#xA0;and click&#xA0;Publish.\nName the app a unique name.\nSelect a subscription.\nSelect&#xA0;New&#x2026;&#x200B;&#xA0;for the resource group and enter a name for the new resource group.\nSelect&#xA0;New&#x2026;&#x200B;&#xA0;for the app service plan and select a location near you. You can keep the name that is generated by default.\nSelect the&#xA0;Services&#xA0;tab to create a new database.\nSelect the green&#xA0;+&#xA0;icon to create a new SQL Database\nSelect&#xA0;New&#x2026;&#x200B;&#xA0;on the&#xA0;Configure SQL Database&#xA0;dialog to create a new database.\nThe&#xA0;Configure SQL Server&#xA0;dialog appears.\nEnter an administrator user name and password, and then select&#xA0;OK. Don&#x2019;t forget the user name and password you create in this step. You can keep the default&#xA0;Server Name.\nEnter names for the database and connection string.\nNote\n&quot;admin&quot; is not allowed as the administrator user name.\nSelect&#xA0;OK.\nVisual Studio returns to the&#xA0;Create App Service&#xA0;dialog.\nSelect&#xA0;Create&#xA0;on the&#xA0;Create App Service&#xA0;dialog.\nClick the&#xA0;Settings&#xA0;link in the&#xA0;Publish&#xA0;dialog.\nOn the&#xA0;Settings&#xA0;page of the&#xA0;Publish&#xA0;dialog:\nExpand&#xA0;Databases&#xA0;and check&#xA0;Use this connection string at runtime.\nExpand&#xA0;Entity Framework Migrations&#xA0;and check&#xA0;Apply this migration on publish.\nSelect&#xA0;Save. Visual Studio returns to the&#xA0;Publish&#xA0;dialog.\nClick&#xA0;Publish. Visual Studio will publish your app to Azure and launch the cloud app in your browser.\nTest your app in Azure\nTest the&#xA0;About&#xA0;and&#xA0;Contact&#xA0;links\nRegister a new user\nUpdate the app\nEdit the&#xA0;Pages/About.cshtml&#xA0;Razor page and change its contents. For example, you can modify the paragraph to say &quot;Hello ASP.NET Core!&quot;:\nhtml&lt;button class=&quot;action copy&quot; data-bi-name=&quot;copy&quot;&gt;Copy&lt;/button&gt;\n@page\n@model AboutModel\n@{\nViewData[&quot;Title&quot;] = &quot;About&quot;;\n}\n&lt;h2&gt;@ViewData[&quot;Title&quot;]&lt;/h2&gt;\n&lt;h3&gt;@Model.Message&lt;/h3&gt;\n&lt;p&gt;Hello ASP.NET Core!&lt;/p&gt;\nRight-click on the project and select&#xA0;Publish&#x2026;&#x200B;&#xA0;again.\nAfter the app is published, verify the changes you made are available on Azure.\nClean up\nWhen you have finished testing the app, go to the&#xA0;Azure portal&#xA0;and delete the app.\nSelect&#xA0;Resource groups, then select the resource group you created.\nIn the&#xA0;Resource groups&#xA0;page, select&#xA0;Delete.\nEnter the name of the resource group and select&#xA0;Delete. Your app and all other resources created in this tutorial are now deleted from Azure.\n24.1.6. External links\nPublishing .Net Core on IIS\nIIS Shared configuration\nPublishing to Nginx\nPublishing to Docker\nConnection strings\nEF basics\nEntity framework advanced design\nSwagger annotations\nSummary notation\nJWT Official Site\nSerilog\n&#x2190;&#xA0;Previous:&#xA0;Environment&#xA0;| &#x2191;&#xA0;Up:&#xA0;devon4net&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Packages&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devon4ng.asciidoc.html","title":"IV. devon4ng","body":"\nIV. devon4ng\nArchitecture\nLayers\nGuides\nAngular\nIonic\nLayouts\nNgRx\nCookbook\n&#x2190;&#xA0;Previous:&#xA0;For Core-Developers&#xA0;| &#x2191;&#xA0;Up:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Architecture&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devon4ng.asciidoc_angular.html","title":"16. Angular","body":"\n16. Angular\n16.1. Accessibility\nMultiple studies suggest that around 15-20% of the population are living with a disability of some kind. In comparison, that number is higher than any single browser demographic currently, other than Chrome2. Not considering those users when developing an application means excluding a large number of people from being able to use it comfortable or at all.\nSome people are unable to use the mouse, view a screen, see low contrast text, Hear dialogue or music and some people having difficulty to understanding the complex language.This kind of people needed the support like Keyboard support, screen reader support, high contrast text, captions and transcripts and Plain language support. This disability may change the from permanent to the situation.\n16.1.1. Key Concerns of Accessible Web Applications\nSemantic Markup - Allows the application to be understood on a more general level rather than just details of whats being rendered\nKeyboard Accessibility - Applications must still be usable when using only a keyboard\nVisual Assistance - color contrast, focus of elements and text representations of audio and events\nSemantic Markup\nIf you&#x2019;re creating custom element directives, Web Components or HTML in general, use native elements wherever possible to utilize built-in events and properties. Alternatively, use ARIA to communicate semantic meaning.\nHTML tags have attributes that providers extra context on what&#x2019;s being displayed on the browser. For example, the img tag&#x2019;s alt attribute lets the reader know what is being shown using a short description.However, native tags don&#x2019;t cover all cases. This is where ARIA fits in. ARIA attributes can provide context on what roles specific elements have in the application or on how elements within the document relate to each other.\nA modal component can be given the role of dialog or alertdialog to let the browser know that that component is acting as a modal. The modal component template can use the ARIA attributes aria-labelledby and aria-described to describe to readers what the title and purpose of the modal is.\n@Component({\nselector: &apos;ngc2-app&apos;,\ntemplate: `\n&lt;ngc2-notification-button\nmessage=&quot;Hello!&quot;\nlabel=&quot;Greeting&quot;\nrole=&quot;button&quot;&gt;\n&lt;/ngc2-notification-button&gt;\n&lt;ngc2-modal\n[title]=&quot;modal.title&quot;\n[description]=&quot;modal.description&quot;\n[visible]=&quot;modal.visible&quot;\n(close)=&quot;modal.close()&quot;&gt;\n&lt;/ngc2-modal&gt;\n`\n})\nexport class AppComponent {\nconstructor(private modal: ModalService) { }\n}\nnotification-button.component.ts\n@Component({\nselector: &apos;ngc2-modal&apos;,\ntemplate: `\n&lt;div\nrole=&quot;dialog&quot;\naria-labelledby=&quot;modal-title&quot;\naria-describedby=&quot;modal-description&quot;&gt;\n&lt;div id=&quot;modal-title&quot;&gt;{{title}}&lt;/div&gt;\n&lt;p id=&quot;modal-description&quot;&gt;{{description}}&lt;/p&gt;\n&lt;button (click)=&quot;close.emit()&quot;&gt;OK&lt;/button&gt;\n&lt;/div&gt;\n`\n})\nexport class ModalComponent {\n...\n}\nKeyboard Accessibility\nKeyboard accessibility is the ability of your application to be interacted with using just a keyboard. The more streamlined the site can be used this way, the more keyboard accessible it is. Keyboard accessibility is one of the largest aspects of web accessibility since it targets:\nthose with motor disabilities who can&#x2019;t use a mouse\nusers who rely on screen readers and other assistive technology, which require keyboard navigation\nthose who prefer not to use a mouse\nFocus\nKeyboard interaction is driven by something called focus. In web applications, only one element on a document has focus at a time, and keypresses will activate whatever function is bound to that element.\nFocus element border can be styled with CSS using the outline property, but it should not be removed. Elements can also be styled using the :focus psuedo-selector.\nTabbing\nThe most common way of moving focus along the page is through the tab key. Elements will be traversed in the order they appear in the document outline - so that order must be carefully considered during development.\nThere is way change the default behaviour or tab order. This can be done through the tabindex attribute. The tabindex can be given the values:\n* less than zero - to let readers know that an element should be focusable but not keyboard accessible\n* 0 - to let readers know that that element should be accessible by keyboard\n* greater than zero - to let readers know the order in which the focusable element should be reached using the keyboard. Order is calculated from lowest to highest.\nTransitions\nThe majority of transitions that happen in an Angular application will not involve a page reload. This means that developers will need to carefully manage what happens to focus in these cases.\nFor example:\n@Component({\nselector: &apos;ngc2-modal&apos;,\ntemplate: `\n&lt;div\nrole=&quot;dialog&quot;\naria-labelledby=&quot;modal-title&quot;\naria-describedby=&quot;modal-description&quot;&gt;\n&lt;div id=&quot;modal-title&quot;&gt;{{title}}&lt;/div&gt;\n&lt;p id=&quot;modal-description&quot;&gt;{{description}}&lt;/p&gt;\n&lt;button (click)=&quot;close.emit()&quot;&gt;OK&lt;/button&gt;\n&lt;/div&gt;\n`,\n})\nexport class ModalComponent {\nconstructor(private modal: ModalService, private element: ElementRef) { }\nngOnInit() {\nthis.modal.visible$.subscribe(visible =&gt; {\nif(visible) {\nsetTimeout(() =&gt; {\nthis.element.nativeElement.querySelector(&apos;button&apos;).focus();\n}, 0);\n}\n})\n}\n}\n16.1.2. Visual Assistance\nOne large category of disability is visual impairment. This includes not just the blind, but those who are color blind or partially sighted, and require some additional consideration.\nColor Contrast\nWhen choosing colors for text or elements on a website, the contrast between them needs to be considered. For WCAG 2.0 AA, this means that the contrast ratio for text or visual representations of text needs to be at least 4.5:1. There are tools online to measure the contrast ratio such as this color contrast checker from WebAIM or be checked with using automation tests.\nVisual Information\nColor can help a user&#x2019;s understanding of information, but it should never be the only way to convey information to a user. For example, a user with red/green color-blindness may have trouble discerning at a glance if an alert is informing them of success or failure.\nAudiovisual Media\nAudiovisual elements in the application such as video, sound effects or audio (ie. podcasts) need related textual representations such as transcripts, captions or descriptions. They also should never auto-play and playback controls should be provided to the user.\n16.1.3. Accessibility with Angular Material\nThe a11y package provides a number of tools to improve accessibility. Import\nimport { A11yModule } from &apos;@angular/cdk/a11y&apos;;\nListKeyManager\nListKeyManager manages the active option in a list of items based on keyboard interaction. Intended to be used with components that correspond to a role=&quot;menu&quot; or role=&quot;listbox&quot; pattern . Any component that uses a ListKeyManager will generally do three things:\nCreate a @ViewChildren query for the options being managed.\nInitialize the ListKeyManager, passing in the options.\nForward keyboard events from the managed component to the ListKeyManager.\nEach option should implement the ListKeyManagerOption interface:\ninterface ListKeyManagerOption {\ndisabled?: boolean;\ngetLabel?(): string;\n}\nTypes of ListKeyManager\nThere are two varieties of ListKeyManager, FocusKeyManager and ActiveDescendantKeyManager.\nFocusKeyManager\nUsed when options will directly receive browser focus. Each item managed must implement the FocusableOption interface:\ninterface FocusableOption extends ListKeyManagerOption {\nfocus(): void;\n}\nActiveDescendantKeyManager\nUsed when options will be marked as active via aria-activedescendant. Each item managed must implement the Highlightable interface:\ninterface Highlightable extends ListKeyManagerOption {\nsetActiveStyles(): void;\nsetInactiveStyles(): void;\n}\nEach item must also have an ID bound to the listbox&#x2019;s or menu&#x2019;s aria-activedescendant.\nFocusTrap\nThe cdkTrapFocus directive traps Tab key focus within an element. This is intended to be used to create accessible experience for components like modal dialogs, where focus must be constrained. This directive is declared in A11yModule.\nThis directive will not prevent focus from moving out of the trapped region due to mouse interaction.\nFor example:\n&lt;div class=&quot;my-inner-dialog-content&quot; cdkTrapFocus&gt;\n&lt;!-- Tab and Shift + Tab will not leave this element. --&gt;\n&lt;/div&gt;\nRegions\nRegions can be declared explicitly with an initial focus element by using the cdkFocusRegionStart, cdkFocusRegionEnd and cdkFocusInitial DOM attributes. When using the tab key, focus will move through this region and wrap around on either end.\nFor example:\n&lt;a mat-list-item routerLink cdkFocusRegionStart&gt;Focus region start&lt;/a&gt;\n&lt;a mat-list-item routerLink&gt;Link&lt;/a&gt;\n&lt;a mat-list-item routerLink cdkFocusInitial&gt;Initially focused&lt;/a&gt;\n&lt;a mat-list-item routerLink cdkFocusRegionEnd&gt;Focus region end&lt;/a&gt;\nInteractivityChecker\nInteractivityChecker is used to check the interactivity of an element, capturing disabled, visible, tabbable, and focusable states for accessibility purposes.\nLiveAnnouncer\nLiveAnnouncer is used to announce messages for screen-reader users using an aria-live region.\nFor example:\n@Component({...})\nexport class MyComponent {\nconstructor(liveAnnouncer: LiveAnnouncer) {\nliveAnnouncer.announce(&quot;Hey Google&quot;);\n}\n}\nAPI reference for Angular CDK a11y\nAPI reference for Angular CDK a11y\n16.2. Angular Elements\n16.2.1. What are Angular Elements?\nAngular elements are Angular components packaged as custom elements, a web standard for defining new HTML elements in a framework-agnostic way.\nCustom elements are a Web Platform feature currently supported by Chrome, Firefox, Opera, and Safari, and available in other browsers through Polyfills. A custom element extends HTML by allowing you to define a tag whose content is created and controlled by JavaScript code. The browser maintains a CustomElementRegistry of defined custom elements (also called Web Components), which maps an instantiable JavaScript class to an HTML tag.\n16.2.2. Why use Angular Elements?\nAngular Elements allows Angular to work with different frameworks by using input and output elements. This allows Angular to work with many different frameworks if needed. This is an ideal situation if a slow transformation of an application to Angular is needed or some Angular needs to be added in other web applications(For example. ASP.net, JSP etc )\n16.2.3. Negative points about Elements\nAngular Elements is really powerful but since, the transition between views between views is going to be handled by another framework or html/javascript, using Angular Router is not possible. the view transitions have to be handled manually. This fact also eliminates the possibility of just porting an application completely.\n16.2.4. How to use Angular Elements?\nIn a generalized way, a simple Angular component could be transformed to an Angular Element with this steps:\nInstalling Angular Elements\nThe first step is going to be install the library using our prefered packet manager:\nNPM\nnpm install @angular/elements\nYARN\nyarn add @angular/elements\nPreparing the components in the modules\nInside the app.module.ts, in addition to the normal declaration of the components inside declarations, the modules inside imports and the services inside providers, the components need to added in entryComponents. If there are components that have their own module, the same logic is going to be applied for them, only adding in the app.module.ts the components that dont have their own module. Here is an example of this:\n....\n@NgModule({\ndeclarations: [\nDishFormComponent,\nDishviewComponent\n],\nimports: [\nCoreModule, // Module containing Angular Materials\nFormsModule\n],\nentryComponents: [\nDishFormComponent,\nDishviewComponent\n],\nproviders: [DishShareService]\n})\n....\nAfter that is done, the constructor of the module is going to be modified to use injector and boostrap the application defining the components. This is going to allow the Angular Element to get the injections and to define a component tag that will be used later:\n....\n})\nexport class AppModule {\nconstructor(private injector: Injector) {\n}\nngDoBootstrap() {\nconst el = createCustomElement(DishFormComponent, {injector: this.injector});\ncustomElements.define(&apos;dish-form&apos;, el);\nconst elView = createCustomElement(DishviewComponent, {injector: this.injector});\ncustomElements.define(&apos;dish-view&apos;, elView);\n}\n}\n....\nA component example\nIn order to be able to use a component, @Input() and @Output() variables are used. These variables are going to be the ones that will allow the Angular Element to communicate with the framework/javascript:\nComponent html\n&lt;mat-card&gt;\n&lt;mat-grid-list cols=&quot;1&quot; rowHeight=&quot;100px&quot; rowWidth=&quot;50%&quot;&gt;\n&lt;mat-grid-tile colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;\n&lt;span&gt;{{ platename }}&lt;/span&gt;\n&lt;/mat-grid-tile&gt;\n&lt;form (ngSubmit)=&quot;onSubmit(dishForm)&quot; #dishForm=&quot;ngForm&quot;&gt;\n&lt;mat-grid-tile colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;\n&lt;mat-form-field&gt;\n&lt;input matInput placeholder=&quot;Name&quot; name=&quot;name&quot; [(ngModel)]=&quot;dish.name&quot;&gt;\n&lt;/mat-form-field&gt;\n&lt;/mat-grid-tile&gt;\n&lt;mat-grid-tile colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;\n&lt;mat-form-field&gt;\n&lt;textarea matInput placeholder=&quot;Description&quot; name=&quot;description&quot; [(ngModel)]=&quot;dish.description&quot;&gt;&lt;/textarea&gt;\n&lt;/mat-form-field&gt;\n&lt;/mat-grid-tile&gt;\n&lt;mat-grid-tile colspan=&quot;1&quot; rowspan=&quot;1&quot;&gt;\n&lt;button mat-raised-button color=&quot;primary&quot; type=&quot;submit&quot;&gt;Submit&lt;/button&gt;\n&lt;/mat-grid-tile&gt;\n&lt;/form&gt;\n&lt;/mat-grid-list&gt;\n&lt;/mat-card&gt;\nComponent ts\n@Component({\ntemplateUrl: &apos;./dish-form.component.html&apos;,\nstyleUrls: [&apos;./dish-form.component.scss&apos;]\n})\nexport class DishFormComponent implements OnInit {\n@Input() platename;\n@Input() platedescription;\n@Output()\nsubmitDishEvent = new EventEmitter();\nsubmitted = false;\ndish = {name: &apos;&apos;, description: &apos;&apos;};\nconstructor(public dishShareService: DishShareService) { }\nngOnInit() {\nthis.dish.name = this.platename;\nthis.dish.description = this.platedescription;\n}\nonSubmit(dishForm: NgForm): void {\nconsole.log(&apos;SUBMIT&apos;);\nconsole.log(dishForm.value);\nthis.dishShareService.createDish(dishForm.value.name, dishForm.value.description);\nthis.submitDishEvent.emit(&apos;dishSubmited&apos;);\n}\n}\nIn this file there are definitions of multiple variables that will be used as input and output. Since the input variables are going to be used directly by html, only lowercase and underscore strategies can be used for them. On the onSubmit(dishForm: NgForm) a service is used to pass this variables to another component. Finally, as a last thing, the selector inside @Component has been removed since a tag that will be used dynamically was already defined in the last step.\nSolving the error\nIn order to be able to use this Angular Element a Polyfills/Browser support related error needs to solved. This error can be solved in two ways:\nChanging the target\nOne solution is to change the target in tsconfig.json to es2015. This might not be doable for every application since maybe a specific target is required.\nInstalling Polyfaces\nAnother solution is to use AutoPollyfill. In order to do so, the library is going to be installed with a packet manager:\nYarn\nyarn add @webcomponents/webcomponentsjs\nNpm\nnpm install @webcomponents/webcomponentsjs\nAfter the packet manager has finished, inside the src folder a new file polyfills.ts is found. To solve the error, importing the corresponding adapter (custom-elements-es5-adapter.js) is necessary:\n....\n/***************************************************************************************************\n* APPLICATION IMPORTS\n*/\nimport &apos;@webcomponents/webcomponentsjs/custom-elements-es5-adapter.js&apos;;\n....\nIf you want to learn more about polyfills in angular you can do it here\nBuilding the Angular Element\nFirst, before building the Angular Element, every element inside that app component except the module need to be removed. After that, a bash script is created in the root folder,. This script will allow to put every necessary file into a js.\nng build &quot;projectName&quot; --prod --output-hashing=none &amp;&amp; cat dist/&quot;projectName&quot;/runtime.js dist/&quot;projectName&quot;/polyfills.js dist/&quot;projectName&quot;/scripts.js dist/&quot;projectName&quot;/main.js &gt; ./dist/&quot;projectName&quot;/&quot;nameWantedAngularElement&quot;.js\nAfter executing the bash script, it will generate inside the path dist/&quot;projectName&quot; a js file named &quot;nameWantedAngularElement&quot;.js and a css file.\nBuilding with ngx-build-plus (Recommended)\nThe library ngx-build-plus allows to add different options when building. In addition, it solves some errors that will occur when trying to use multiple angular elements in an application. In order to use it, yarn or npm can be used:\nYarn\nyarn add ngx-build-plus\nNpm\nnpm install ngx-build-plus\nIf you want to add it to a specific sub project in your projects folder, use the --project:\n.... ngx-build-plus --project &quot;project-name&quot;\nUsing this library and the following command, an isolated Angular Element which won&#x2019;t have conflict with others can be generated. This Angular Element will not have a polyfill so, the project where we use them will need to include a poliyfill with the Angular Element requirements.\nng build &quot;projectName&quot; --output-hashing none --single-bundle true --prod --bundle-styles false\nThis command will generate three things:\nThe main js bundle\nThe script js\nThe css\nThese files will be used later instead of the single js generated in the last step.\nExtra parameters\nHere are some extra useful parameters that ngx-build-plus provides:\n--keep-polyfills: This paremeter is going to allow us to keep the polyfills. This needs to be used with caution, avoiding using multiple different polyfills that could cause an error is necessary.\n--extraWebpackConfig webpack.extra.js: This parameter allows us to create a javascript file inside our Angular Elements project with the name of different libraries. Using webpack these libraries will not be included in the Angular Element. This is useful to lower the size of our Angular Element by removing libraries shared. Example:\nconst webpack = require(&apos;webpack&apos;);\nmodule.exports = {\n&quot;externals&quot;: {\n&quot;rxjs&quot;: &quot;rxjs&quot;,\n&quot;@angular/core&quot;: &quot;ng.core&quot;,\n&quot;@angular/common&quot;: &quot;ng.common&quot;,\n&quot;@angular/common/http&quot;: &quot;ng.common.http&quot;,\n&quot;@angular/platform-browser&quot;: &quot;ng.platformBrowser&quot;,\n&quot;@angular/platform-browser-dynamic&quot;: &quot;ng.platformBrowserDynamic&quot;,\n&quot;@angular/compiler&quot;: &quot;ng.compiler&quot;,\n&quot;@angular/elements&quot;: &quot;ng.elements&quot;,\n&quot;@angular/router&quot;: &quot;ng.router&quot;,\n&quot;@angular/forms&quot;: &quot;ng.forms&quot;\n}\n}\nNote\nIf some libraries are excluded from the `Angular Element` you will need to add the bundled umd files of those libraries manually.\nUsing the Angular Element\nThe Angular Element that got generated in the last step can be used in almost every framework. In this case, the Angular Element is going to be used in html:\nListing 14. Sample index.html version without ngx-build-plus\n&lt;html&gt;\n&lt;head&gt;\n&lt;link rel=&quot;stylesheet&quot; href=&quot;styles.css&quot;&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;div id=&quot;container&quot;&gt;\n&lt;/div&gt;\n&lt;!--Use of the element non dynamically--&gt;\n&lt;!--&lt;plate-form platename=&quot;test&quot; platedescription=&quot;test&quot;&gt;&lt;/plate-form&gt;--&gt;\n&lt;script src=&quot;./devon4ngAngularElements.js&quot;&gt; &lt;/script&gt;\n&lt;script&gt;\nvar elContainer = document.getElementById(&apos;container&apos;);\nvar el= document.createElement(&apos;dish-form&apos;);\nel.setAttribute(&apos;platename&apos;,&apos;test&apos;);\nel.setAttribute(&apos;platedescription&apos;,&apos;test&apos;);\nel.addEventListener(&apos;submitDishEvent&apos;,(ev)=&gt;{\nvar elView= document.createElement(&apos;dish-view&apos;);\nelContainer.innerHTML = &apos;&apos;;\nelContainer.appendChild(elView);\n});\nelContainer.appendChild(el);\n&lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nListing 15. Sample index.html version with ngx-build-plus\n&lt;html&gt;\n&lt;head&gt;\n&lt;link rel=&quot;stylesheet&quot; href=&quot;styles.css&quot;&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;div id=&quot;container&quot;&gt;\n&lt;/div&gt;\n&lt;!--Use of the element non dynamically--&gt;\n&lt;!--&lt;plate-form platename=&quot;test&quot; platedescription=&quot;test&quot;&gt;&lt;/plate-form&gt;--&gt;\n&lt;script src=&quot;./polyfills.js&quot;&gt; &lt;/script&gt; &lt;!-- Created using --keep-polyfills options --&gt;\n&lt;script src=&quot;./scripts.js&quot;&gt; &lt;/script&gt;\n&lt;script src=&quot;./main.js&quot;&gt; &lt;/script&gt;\n&lt;script&gt;\nvar elContainer = document.getElementById(&apos;container&apos;);\nvar el= document.createElement(&apos;dish-form&apos;);\nel.setAttribute(&apos;platename&apos;,&apos;test&apos;);\nel.setAttribute(&apos;platedescription&apos;,&apos;test&apos;);\nel.addEventListener(&apos;submitDishEvent&apos;,(ev)=&gt;{\nvar elView= document.createElement(&apos;dish-view&apos;);\nelContainer.innerHTML = &apos;&apos;;\nelContainer.appendChild(elView);\n});\nelContainer.appendChild(el);\n&lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nIn this html, the css generated in the last step is going to be imported inside the &lt;head&gt; and then, the javascript element is going to be imported at the end of the body. After that is done, There is two uses of Angular Elements in the html, one directly whith use of the @input() variables as parameters commented in the html:\n....\n&lt;!--Use of the element non dynamically--&gt;\n&lt;!--&lt;plate-form platename=&quot;test&quot; platedescription=&quot;test&quot;&gt;&lt;/plate-form&gt;--&gt;\n....\nand one dynamically inside the script:\n....\n&lt;script&gt;\nvar elContainer = document.getElementById(&apos;container&apos;);\nvar el= document.createElement(&apos;dish-form&apos;);\nel.setAttribute(&apos;platename&apos;,&apos;test&apos;);\nel.setAttribute(&apos;platedescription&apos;,&apos;test&apos;);\nel.addEventListener(&apos;submitDishEvent&apos;,(ev)=&gt;{\nvar elView= document.createElement(&apos;dish-view&apos;);\nelContainer.innerHTML = &apos;&apos;;\nelContainer.appendChild(elView);\n});\nelContainer.appendChild(el);\n&lt;/script&gt;\n....\nThis javascript is an example of how to create dynamically an Angular Element inserting attributed to fill our @Input() variables and listen to the @Output() that was defined earlier. This is done with:\nel.addEventListener(&apos;submitDishEvent&apos;,(ev)=&gt;{\nvar elView= document.createElement(&apos;dish-view&apos;);\nelContainer.innerHTML = &apos;&apos;;\nelContainer.appendChild(elView);\n});\nThis allows javascript to hook with the @Output() event emitter that was defined. When this event gets called, another component that was defined gets inserted dynamically.\n16.2.5. Angular Element within another Angular project\nIn order to use an Angular Element within another Angular project the following steps need to be followed:\nCopy bundled script and css to resources\nFirst copy the generated .js and .css inside assets in the corresponding folder.\nAdd bundled script to angular.json\nInside angular.json both of the files that were copied in the last step are going to be included. This will be done both, in test and in build. Including it on the test, will allow to perform unitary tests.\n{\n....\n&quot;architect&quot;: {\n....\n&quot;build&quot;: {\n....\n&quot;styles&quot;: [\n....\n&quot;src/assets/css/devon4ngAngularElements.css&quot;\n....\n]\n....\n&quot;scripts&quot;: [\n&quot;src/assets/js/devon4ngAngularElements.js&quot;\n]\n....\n}\n....\n&quot;test&quot;: {\n....\n&quot;styles&quot;: [\n....\n&quot;src/assets/css/devon4ngAngularElements.css&quot;\n....\n]\n....\n&quot;scripts&quot;: [\n&quot;src/assets/js/devon4ngAngularElements.js&quot;\n]\n....\n}\n}\n}\nBy declaring the files in the angular.json angular will take care of including them in a proper way.\nUsing Angular Element\nThere are two ways that Angular Element can be used:\nCreate component dynamicly\nIn order to add the component in a dynamic way, first adding a container is necessary:\napp.component.html\n....\n&lt;div id=&quot;container&quot;&gt;\n&lt;/div&gt;\n....\nWith this container created, inside the app.component.ts a method is going to be created. This method is going to find the container, create the dynamic element and append it into the container.\napp.component.ts\nexport class AppComponent implements OnInit {\n....\nngOnInit(): void {\nthis.createComponent();\n}\n....\ncreateComponent(): void {\nconst container = document.getElementById(&apos;container&apos;);\nconst component = document.createElement(&apos;dish-form&apos;);\ncontainer.appendChild(component);\n}\n....\nUsing it directly\nIn order to use it directly on the templates, in the app.module.ts the CUSTOM_ELEMENTS_SCHEMA needs to be added:\n....\nimport { NgModule, CUSTOM_ELEMENTS_SCHEMA } from &apos;@angular/core&apos;;\n....\n@NgModule({\n....\nschemas: [ CUSTOM_ELEMENTS_SCHEMA ],\nThis is going to allow the use of the Angular Element in the templates directly:\napp.component.html\n....\n&lt;div id=&quot;container&quot;&gt;\n&lt;dish-form&gt;&lt;/dish-form&gt;\n&lt;/div&gt;\n16.3. Angular Lazy loading\nWhen the development of an application starts, it just contains a small set of features so the app usually loads fast. However, as new features are added, the overall application size grows up and its loading speed decreases, is in this context where Lazy loading finds its place.\nLazy loading is a dessign pattern that defers initialization of objects until it is needed so, for example, Users that just access to a website&#x2019;s home page do not need to have other areas loaded.\nAngular handles lazy loading through the routing module which redirect to requested pages. Those pages can be loaded at start or on demand.\n16.3.1. An example with Angular\nTo explain how lazy loading is implemented using angular, a basic sample app is going to be developed. This app will consist in a window named &quot;level 1&quot; that contains two buttons that redirects to other windows in a &quot;second level&quot;. It is a simple example, but useful to understand the relation between angular modules and lazy loading.\nFigure 35. Levels app structure.\nThis graphic shows that modules acts as gates to access components &quot;inside&quot; them.\nBecause the objective of this guide is related mainly with logic, the html structure and scss styles are less relevant, but the complete code can be found as a sample here.\nImplementation\nFirst write in a console ng new level-app --routing, to generate a new project called level-app including an app-routing.module.ts file (--routing flag).\nIn the file app.component.html delete all the content except the router-outlet tag.\nListing 16. File app.component.html\n&lt;router-outlet&gt;&lt;/router-outlet&gt;\nThe next steps consists on creating features modules.\nrun ng generate module first --routing to generate a module named first.\nrun ng generate module first/second-left --routing to generate a module named second-left under first.\nrun ng generate module first/second-right --routing to generate a module second-right under first.\nrun ng generate component first/first to generate a component named first inside the module first.\nrun ng generate component first/second-left/content to generate a component content inside the module second-left.\nrun ng generate component first/second-right/content to generate a component content inside the module second-right.\nTo move between components we have to configure the routes used:\nIn app-routing.module.ts add a path &apos;first&apos; to FirstComponent and a redirection from &apos;&apos; to &apos;first&apos;.\nListing 17. File app-routing.module.ts.\n...\nimport { FirstComponent } from &apos;./first/first/first.component&apos;;\nconst routes: Routes = [\n{\npath: &apos;first&apos;,\ncomponent: FirstComponent\n},\n{\npath: &apos;&apos;,\nredirectTo: &apos;first&apos;,\npathMatch: &apos;full&apos;,\n},\n];\n@NgModule({\nimports: [RouterModule.forRoot(routes)],\nexports: [RouterModule],\n})\nexport class AppRoutingModule {}\nIn app.module.ts import the module which includes FirstComponent.\nListing 18. File app.module.ts\n....\nimport { FirstModule } from &apos;./first/first.module&apos;;\n@NgModule({\n...\nimports: [\n....\nFirstModule\n],\n...\n})\nexport class AppModule { }\nIn first-routing.module.ts add routes that direct to the content of SecondRightModule and SecondLeftModule. The content of both modules have the same name so, in order to avoid conflicts the name of the components are going to be changed using as ( original-name as new-name).\nListing 19. File first-routing.module.ts\n...\nimport { ContentComponent as ContentLeft} from &apos;./second-left/content/content.component&apos;;\nimport { ContentComponent as ContentRight} from &apos;./second-right/content/content.component&apos;;\nimport { FirstComponent } from &apos;./first/first.component&apos;;\nconst routes: Routes = [\n{\npath: &apos;&apos;,\ncomponent: FirstComponent\n},\n{\npath: &apos;first/second-left&apos;,\ncomponent: ContentLeft\n},\n{\npath: &apos;first/second-right&apos;,\ncomponent: ContentRight\n}\n];\n@NgModule({\nimports: [RouterModule.forChild(routes)],\nexports: [RouterModule]\n})\nexport class FirstRoutingModule { }\nIn first.module.ts import SecondLeftModule and SecondRightModule.\nListing 20. File first.module.ts\n...\nimport { SecondLeftModule } from &apos;./second-left/second-left.module&apos;;\nimport { SecondRightModule } from &apos;./second-right/second-right.module&apos;;\n@NgModule({\n...\nimports: [\n...\nSecondLeftModule,\nSecondRightModule,\n]\n})\nexport class FirstModule { }\nUsing the current configuration, we have a project that loads all the modules in a eager way. Run ng serve to see what happens.\nFirst, during the compilation we can see that just a main file is built.\nFigure 36. Compile eager.\nIf we go to http//localhost:4200/first and open developer options (F12 on Chrome), it is found that a document named &quot;first&quot; is loaded.\nFigure 37. First level eager.\nIf we click on [Go to right module] a second level module opens, but there is no &apos;second-right&apos; document.\nFigure 38. Second level right eager.\nBut, typing the url directly will load &apos;second-right&apos; but no &apos;first&apos;, even if we click on [Go back]\nFigure 39. Second level right eager direct url.\nModifying an angular application to load its modules lazily is easy, you have to change the routing configuration of the desired module (for example FirstModule).\nListing 21. File app-routing.module.ts.\nconst routes: Routes = [\n{\npath: &apos;first&apos;,\nloadChildren: () =&gt; import(&apos;./first/first.module&apos;).then(m =&gt; m.FirstModule),\n},\n{\npath: &apos;&apos;,\nredirectTo: &apos;first&apos;,\npathMatch: &apos;full&apos;,\n},\n];\n@NgModule({\nimports: [RouterModule.forRoot(routes)],\nexports: [RouterModule],\n})\nexport class AppRoutingModule {}\nNotice that instead of loading a component, you dynamically import it in a loadChildren attribute because modules acts as gates to access components &quot;inside&quot; them. Updating the app to load lazily has four consecuences:\nNo component attribute.\nNo import of FirstComponent.\nFirstModule import has to be removed from the imports array at app.module.ts.\nChange of context.\nIf we check first-routing.module.ts again, the can see that the path for ContentLeft and ContentRight is set to &apos;first/second-left&apos; and &apos;first/second-right&apos; respectively, so writing &apos;http//localhost:4200/first/second-left&apos; will redirect us to ContentLeft. However, after loading a module with loadChildren setting the path to &apos;second-left&apos; and &apos;second-right&apos; is enough because it adquires the context set by AppRoutingModule.\nListing 22. File first-routing.module.ts\nconst routes: Routes = [\n{\npath: &apos;&apos;,\ncomponent: FirstComponent\n},\n{\npath: &apos;second-left&apos;,\ncomponent: ContentLeft\n},\n{\npath: &apos;second-right&apos;,\ncomponent: ContentRight\n}\n];\nIf we go to &apos;first&apos; then FirstModule is situated in &apos;/first&apos; but also its children ContentLeft and ContentRight, so it is not necessary to write in their path &apos;first/second-left&apos; and &apos;first/second-right&apos;, because that will situate the components on &apos;first/first/second-left&apos; and &apos;first/first/second-right&apos;.\nFigure 40. First level lazy wrong path.\nWhen we compile an app with lazy loaded modules, files containing them will be generated\nFigure 41. First level lazy compilation.\nAnd if we go to developer tools &#x2192; network, we can find those modules loaded (if they are needed).\nFigure 42. First level lazy.\nTo load the component ContentComponent of SecondLeftModule lazily, we have to load SecondLeftModule as a children of FirstModule:\nChange component to loadChildren and reference SecondLeftModule.\nListing 23. File first-routing.module.ts.\nconst routes: Routes = [\n{\npath: &apos;&apos;,\ncomponent: FirstComponent\n},\n{\npath: &apos;second-left&apos;,\nloadChildren: () =&gt; import(&apos;./second-left/second-left.module&apos;).then(m =&gt; m.SecondLeftModule),\n},\n{\npath: &apos;second-right&apos;,\ncomponent: ContentRight\n}\n];\nRemove SecondLeftModule at first.component.ts\nRoute the components inside SecondLeftModule. Without this step nothing would be displayed.\nListing 24. File second-left-routing.module.ts.\n...\nimport { ContentComponent } from &apos;./content/content.component&apos;;\nconst routes: Routes = [\n{\npath: &apos;&apos;,\ncomponent: ContentComponent\n}\n];\n@NgModule({\nimports: [RouterModule.forChild(routes)],\nexports: [RouterModule]\n})\nexport class SecondLeftRoutingModule { }\nrun ng serve to generate files containing the lazy modules.\nFigure 43. Second level lazy loading compilation.\nClicking on [Go to left module] triggers the load of SecondLeftModule.\nFigure 44. Second level lazy loading network.\n16.3.2. Conclusion\nLazy loading is a pattern useful when new features are added, these features are usually identified as modules which can be loaded only if needed as shown in this document, reducing the time spent loading an application.\n16.4. Angular Library\nAngular CLI provides us with methods that allow the creation of a library. After that, using either packet manager (npm or yarn) the library can be build and packed which will allow later to install/publish it.\n16.4.1. Whats a library?\nFrom wikipedia: a library is a collection of non-volatile resources used by computer programs, often for software development. These may include configuration data, documentation, help data, message templates, pre-written code and subroutines, classes, values or type specifications.\n16.4.2. How to build a library\nIn this section, a library is going to be build step by step.\nCreating an empty application\nFirst, using Angular CLI we are going to generate a empty application which will be later filled with the generated library. In order to do so, Angular CLI allows us to add to ng new &quot;application-name&quot; an option (--create-application). This option is going to tell Angular CLI not to create the initial app project. This is convenient since a library is going to be generated in later steps. Using this command ng new &quot;application-name&quot; --create-application=false an empty project with the name wanted is created.\nng new &quot;application-name&quot; --create-application=false\nGenerating a library\nAfter generating an empty application, a library is going to be generated. Inside the folder of the project, the Angular CLI command ng generate library &quot;library-name&quot; is going to generate the library as a project (projects/&quot;library-name&quot;). As an addition, the option --prefix=&quot;library-prefix-wanted&quot; allows us to switch the default prefix that Angular generated with (lib). Using the option to change the prefix the command will look like this ng generate library &quot;library-name&quot; --prefix=&quot;library-prefix-wanted&quot;.\nng generate library &quot;library-name&quot; --prefix=&quot;library-prefix-wanted&quot;\nGenerating/Modifying in our library\nIn the last step we generated a library. This generates automaticly a module,service and component inside (projects/&quot;library-name&quot;) that we can modify adding new methods, components etc that we want to use in other projects. We can generate other elements, using the usual Angular CLI generate commands adding the option --project=&quot;library-name&quot; is going to allow to generate elements within our project . An example of this is: ng generate service &quot;name&quot; --project=&quot;library-name&quot;.\nng generate &quot;element&quot; &quot;name&quot; --project=&quot;library-name&quot;\nExporting the generated things\nInside the library (projects/&quot;library-name) theres a public_api.ts which is the file that exports the elements inside the library. In case we generated other things, that file needs to be modified adding the extra exports with the generated elements. In addition, changing the library version is possible in the file package.json.\nBuilding our library\nOnce we added the necessary exports, in order to use the library in other applications, we need to build the library. The command ng build &quot;library-name&quot; is going to build the library, generating in &quot;project-name&quot;/dist/&quot;library-name&quot; the necessary files.\nng build &quot;library-name&quot;\nPacking the library\nIn this step we are going to pack the build library. In order to do so, we need to go inside dist/&quot;library-name&quot; and then run either npm pack or yarn pack to generate a &quot;library-name-version.tgz&quot; file.\nListing 25. Packing using npm\nnpm pack\nListing 26. Packing using yarn\nyarn pack\nPublishing to npm repository (optional)\nAdd a README.md and LICENSE file. The text inside README.md will be used in you npm package web page as documentation.\nrun npm adduser if you do not have a npm account to create it, otherwise run npm login and introduce your credentials.\nrun npm publish inside dist/&quot;library-name&quot; folder.\nCheck that the library is published: https://npmjs.com/package/library-name\nInstalling our library in other projects\nIn this step we are going to install/add the library on other projects.\nnpm\nIn order to add the library in other applications, there are two ways:\nOption 1: From inside the application where the library is going to get used, using the command npm install &quot;path-to-tgz&quot;/&quot;library-name-version.tgz&quot; allows us to install the .tgz generated in Packing the library.\nOption 2: run npm install &quot;library-name&quot; to install it from npm repository.\nyarn\nTo add the package using yarn:\nOption 1: From inside the application where the library is going to get used, using the command yarn add &quot;path-to-tgz&quot;/&quot;library-name-version.tgz&quot; allows us to install the .tgz generated in Packing the library.\nOption 2: run yarn add &quot;library-name&quot; to install it from npm repository.\nUsing the library\nFinally, once the library was installed with either packet manager, you can start using the elements from inside like they would be used in a normal element inside the application. Example app.component.ts:\nimport { Component, OnInit } from &apos;@angular/core&apos;;\nimport { MyLibraryService } from &apos;my-library&apos;;\n@Component({\nselector: &apos;app-root&apos;,\ntemplateUrl: &apos;./app.component.html&apos;,\nstyleUrls: [&apos;./app.component.scss&apos;]\n})\nexport class AppComponent implements OnInit {\ntoUpper: string;\nconstructor(private myLibraryService: MyLibraryService) {}\ntitle = &apos;devon4ng library test&apos;;\nngOnInit(): void {\nthis.toUpper = this.myLibraryService.firstLetterToUpper(&apos;test&apos;);\n}\n}\nExample app.component.html:\n&lt;!--The content below is only a placeholder and can be replaced.--&gt;\n&lt;div style=&quot;text-align:center&quot;&gt;\n&lt;h1&gt;\nWelcome to {{ title }}!\n&lt;/h1&gt;\n&lt;img width=&quot;300&quot; alt=&quot;Angular Logo&quot; src=&quot;data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNTAgMjUwIj4KICAgIDxwYXRoIGZpbGw9IiNERDAwMzEiIGQ9Ik0xMjUgMzBMMzEuOSA2My4ybDE0LjIgMTIzLjFMMTI1IDIzMGw3OC45LTQzLjcgMTQuMi0xMjMuMXoiIC8+CiAgICA8cGF0aCBmaWxsPSIjQzMwMDJGIiBkPSJNMTI1IDMwdjIyLjItLjFWMjMwbDc4LjktNDMuNyAxNC4yLTEyMy4xTDEyNSAzMHoiIC8+CiAgICA8cGF0aCAgZmlsbD0iI0ZGRkZGRiIgZD0iTTEyNSA1Mi4xTDY2LjggMTgyLjZoMjEuN2wxMS43LTI5LjJoNDkuNGwxMS43IDI5LjJIMTgzTDEyNSA1Mi4xem0xNyA4My4zaC0zNGwxNy00MC45IDE3IDQwLjl6IiAvPgogIDwvc3ZnPg==&quot;&gt;\n&lt;/div&gt;\n&lt;h2&gt;Here is my library service being used: {{toUpper}}&lt;/h2&gt;\n&lt;lib-my-library&gt;&lt;/lib-my-library&gt;\nExample app.module.ts:\nimport { BrowserModule } from &apos;@angular/platform-browser&apos;;\nimport { NgModule } from &apos;@angular/core&apos;;\nimport { AppRoutingModule } from &apos;./app-routing.module&apos;;\nimport { AppComponent } from &apos;./app.component&apos;;\nimport { MyLibraryModule } from &apos;my-library&apos;;\n@NgModule({\ndeclarations: [\nAppComponent\n],\nimports: [\nBrowserModule,\nAppRoutingModule,\nMyLibraryModule\n],\nproviders: [],\nbootstrap: [AppComponent]\n})\nexport class AppModule { }\nThe result from using the library:\ndevon4ng libraries\nIn devonfw/devon4ng-library you can find some useful libraries:\nAuthorization module: This devon4ng Angular module adds rights-based authorization to your Angular app.\nCache module: Use this devon4ng Angular module when you want to cache requests to server. You may configure it to store in cache only the requests you need and to set the duration you want.\n16.5. Angular Material Theming\nAngular Material library offers UI components for developers, those components follows Google Material desing baselines but characteristics like colors can be modified in order to adapt them to the needs of the client: corporative colors, corporative identity, dark themes, &#x2026;&#x200B;\n16.5.1. Theming basics\nIn Angular Material, a theme is created mixing multiple colors. Colors and its light and dark variants conform a palette. In general, a theme consists of the following palettes:\nprimary: Most used across screens and componets.\naccent: Floating action button and interactive elements.\nwarn: Error state.\nforeground: Text and icons.\nbackground: Element backgrounds.\nFigure 45. Palettes and variants.\nIn angular material, a palette is represented as a scss map.\nFigure 46. Scss map and palettes.\nTip\nSome components can be forced to use primary, accent or warn palettes using the attribute color, for example: &lt;mat-toolbar color=&quot;primary&quot;&gt;.\n16.5.2. Prebuilt themes\nAvailable prebuilt themes:\ndeeppurple-amber.css\nFigure 47. deeppurple-amber theme.\nindigo-pink.css\nFigure 48. indigo-pink theme.\npink-bluegrey.css\nFigure 49. ink-bluegrey theme.\npurple-green.css\nFigure 50. purple-green theme.\nThe prebuilt themes can be added using @import.\n@import &apos;@angular/material/prebuilt-themes/deeppurple-amber.css&apos;;\n16.5.3. Custom themes\nSomethimes prebuild themes do not meet the needs of a project, because color schemas are too specific or do not incorporate branding colors, in those situations custom themes can be built to offer a better solution to the client.\nFor this topic, we are going to use a basic layout project that can be found in devon4ng repository.\nBasics\nBefore starting writing custom themes, there are some necessary things that have to be mentioned:\nAdd a default theme: The project mentioned before has just one global scss stylesheet styles.scss that includes indigo-pink.scss which will be the default theme.\nAdd @import &apos;~@angular/material/theming&apos;; at the begining of the every stylesheet to be able to use angular material prebuilt color palettes and functions.\nAdd @include mat-core(); once per project, so if you are writing multiple themes in multiple files you could import those files from a &apos;central&apos; one (for example styles.scss). This includes all common styles that are used by multiple components.\nFigure 51. Theme files structure.\nBasic custom theme\nTo create a new custom theme, the .scss file containing it has to have imported the angular _theming.scss file (angular/material/theming) file and mat-core included. _theming.scss includes multiple color palettes and some functions that we are going to see below. The file for this basic theme is going to be named styles-custom-dark.scss.\nFirst, declare new variables for primary, accent and warn palettes. Those variables are going to store the result of the function mat-palette.\nmat-palette accepts four arguments: base color palette, main, lighter and darker variants (See [id_palette_variants]) and returns a new palette including some additional map values: default, lighter and darker ([id_scss_map]). Only the first argument is mandatory.\nListing 27. File styles-custom-dark.scss.\n$custom-dark-theme-primary: mat-palette($mat-pink);\n$custom-dark-theme-accent: mat-palette($mat-blue);\n$custom-dark-theme-warn: mat-palette($mat-red);\n);\nIn this example we are using colors available in _theming.scss: mat-pink, mat-blue, mat-red. If you want to use a custom color you need to define a new map, for instance:\nListing 28. File styles-custom-dark.scss custom pink.\n$my-pink: (\n50 : #fcf3f3,\n100 : #f9e0e0,\n200 : #f5cccc,\n300 : #f0b8b8,\n500 : #ea9999,\n900 : #db6b6b,\nA100 : #ffffff,\nA200 : #ffffff,\nA400 : #ffeaea,\nA700 : #ffd0d0,\ncontrast: (\n50 : #000000,\n100 : #000000,\n200 : #000000,\n300 : #000000,\n900 : #000000,\nA100 : #000000,\nA200 : #000000,\nA400 : #000000,\nA700 : #000000,\n)\n);\n$custom-dark-theme-primary: mat-palette($my-pink);\n...\nTip\nSome pages allows to create these palettes easily, for instance: http://mcg.mbitson.com\nUntil now, we just have defined primary, accent and warn palettes but what about foreground and background? Angular material has two functions to change both:\nmat-light-theme: Receives as arguments primary, accent and warn palettes and return a theme whose foreground is basically black (texts, icons, &#x2026;&#x200B;), the background is white and the other palettes are the received ones.\nFigure 52. Custom light theme.\nmat-dark-theme: Similar to mat-light-theme but returns a theme whose foreground is basically white and background black.\nFigure 53. Custom dark theme.\nFor this example we are going to use mat-dark-theme and save its result in $custom-dark-theme.\nListing 29. File styles-custom-dark.scss updated with mat-dark-theme.\n...\n$custom-dark-theme: mat-dark-theme(\n$custom-dark-theme-primary,\n$custom-dark-theme-accent,\n$custom-dark-theme-warn\n);\nTo apply the saved theme, we have to go to styles.scss and import our styles-custom-dark.scss and include a function called angular-material-theme using the theme variable as argument.\nListing 30. File styles.scss.\n...\n@import &apos;styles-custom-dark.scss&apos;;\n@include angular-material-theme($custom-dark-theme);\nIf we have multiple themes it is necessary to add the include statement inside a css class and use it in src/index.html &#x2192; app-root component.\nListing 31. File styles.scss updated with custom-dark-theme class.\n...\n@import &apos;styles-custom-dark.scss&apos;;\n.custom-dark-theme {\n@include angular-material-theme($custom-dark-theme);\n}\nListing 32. File src/index.html.\n...\n&lt;app-root class=&quot;custom-dark-theme&quot;&gt;&lt;/app-root&gt;\n...\nThis will apply $custom-dark-theme theme for the entire application.\nFull custom theme\nSometimes it is needed to custom different elementsw from background and foreground, in those situations we have to create a new function similar to mat-light-theme and mat-dark-theme. Let&#x2019;s focus con mat-light-theme:\nListing 33. Source code of mat-light-theme\n@function mat-light-theme($primary, $accent, $warn: mat-palette($mat-red)) {\n@return (\nprimary: $primary,\naccent: $accent,\nwarn: $warn,\nis-dark: false,\nforeground: $mat-light-theme-foreground,\nbackground: $mat-light-theme-background,\n);\n}\nAs we can se, mat-light-theme takes three arguments and returs a map including them as primary, accent and warn color; but there are three more keys in that map: is-dark, foreground and background.\nis-dark: Boolean true if it is a dark theme, false otherwise.\nbackground: Map that stores the color for multiple background elements.\nforeground: Map that stores the color for multiple foreground elements.\nTo show which elements can be colored lets create a new theme in a file styles-custom-cap.scss:\nListing 34. File styles-custom-cap.scss: Background and foreground variables.\n@import &apos;~@angular/material/theming&apos;;\n// custom background and foreground palettes\n$my-cap-theme-background: (\nstatus-bar: #0070ad,\napp-bar: map_get($mat-blue, 900),\nbackground: #12abdb,\nhover: rgba(white, 0.04),\ncard: map_get($mat-red, 800),\ndialog: map_get($mat-grey, 800),\ndisabled-button: $white-12-opacity,\nraised-button: map-get($mat-grey, 800),\nfocused-button: $white-6-opacity,\nselected-button: map_get($mat-grey, 900),\nselected-disabled-button: map_get($mat-grey, 800),\ndisabled-button-toggle: black,\nunselected-chip: map_get($mat-grey, 700),\ndisabled-list-option: black,\n);\n$my-cap-theme-foreground: (\nbase: yellow,\ndivider: $white-12-opacity,\ndividers: $white-12-opacity,\ndisabled: rgba(white, 0.3),\ndisabled-button: rgba(white, 0.3),\ndisabled-text: rgba(white, 0.3),\nhint-text: rgba(white, 0.3),\nsecondary-text: rgba(white, 0.7),\nicon: white,\nicons: white,\ntext: white,\nslider-min: white,\nslider-off: rgba(white, 0.3),\nslider-off-active: rgba(white, 0.3),\n);\nFunction which uses the variables defined before to create a new theme:\nListing 35. File styles-custom-cap.scss: Creating a new theme function.\n// instead of creating a theme with mat-light-theme or mat-dark-theme,\n// we will create our own theme-creating function that lets us apply our own foreground and background palettes.\n@function create-my-cap-theme($primary, $accent, $warn: mat-palette($mat-red)) {\n@return (\nprimary: $primary,\naccent: $accent,\nwarn: $warn,\nis-dark: false,\nforeground: $my-cap-theme-foreground,\nbackground: $my-cap-theme-background\n);\n}\nCalling the new function and storing its value in $custom-cap-theme.\nListing 36. File styles-custom-cap.scss: Storing the new theme.\n// We use create-my-cap-theme instead of mat-light-theme or mat-dark-theme\n$custom-cap-theme-primary: mat-palette($mat-green);\n$custom-cap-theme-accent: mat-palette($mat-blue);\n$custom-cap-theme-warn: mat-palette($mat-red);\n$custom-cap-theme: create-my-cap-theme(\n$custom-cap-theme-primary,\n$custom-cap-theme-accent,\n$custom-cap-theme-warn\n);\nAfter defining our new theme, we can import it from styles.scss.\nListing 37. File styles.scss updated with custom-cap-theme class.\n...\n@import &apos;styles-custom-cap.scss&apos;;\n.custom-cap-theme {\n@include angular-material-theme($custom-cap-theme);\n}\nMultiple themes and overlay-based components\nCertain components (e.g. menu, select, dialog, etc.) that are inside of a global overlay container,require an additional step to be affected by the theme&#x2019;s css class selector.\nListing 38. File app.module.ts\nimport {OverlayContainer} from &apos;@angular/cdk/overlay&apos;;\n@NgModule({\n// ...\n})\nexport class AppModule {\nconstructor(overlayContainer: OverlayContainer) {\noverlayContainer.getContainerElement().classList.add(&apos;custom-cap-theme&apos;);\n}\n}\n16.5.4. Useful resources\nAngular Material&#x2019;s oficial theming guide\nMaterial Desing: Color theme creation\nPalette generator\nSCSS tutorial\n16.6. Angular Progressive Web App\nProgresive web applications (PWAs) are web application that offer better user experience than the traditional ones. In general, they solve problems related with reliability and speed:\nReliability: PWAs are stable. In this context stability means than even with slow connections or even with no network at all, the application still works. To achieve this, some basic resources like styles, fonts, requests, &#x2026;&#x200B; are stored; due to this caching, it is not possible to assure that the content is always up-to-date.\nSpeed: When an users opens an application, he or she will expect it to load almost inmediately (almost 53% of users abandon sites that take longer that 3 seconds, source: https://developers.google.com/web/progressive-web-apps/#fast).\nPWAs uses a script called service worker, which runs in background and essentially act as proxy between web app and network, intercepting requests and acting depending on the network conditions.\n16.6.1. Assumptions\nThis guide assumes that you already have installed:\nNode.js\nnpm package manager\nAngular CLI\n16.6.2. Sample Application\nFigure 54. Basic angular PWA.\nTo explain how to build PWAs using angular, a basic application is going to be built. This app will be able to ask for resources and save in the cache in order to work even offline.\nStep 1: Create a new project\nThis step can be completed with one simple command: ng new &lt;name&gt;, where &lt;names&gt; is the name for the app. In this case, the app is going to be named basic-ng-pwa.\nStep 2: Create a service\nWeb applications usually uses external resources, making necessary the addition of services which can get those resources. This application gets a dish from My Thai Star&#x2019;s back-end and shows it. To do so, a new service is going to be created.\ngo to project folder: cd basic-ng-pwa\nrun ng generate service data\nModify data.service.ts, environment.ts, environment.prod.ts\nTo retrieve data with this service, you have to import the module HttpClient and add it to the service&#x2019;s contructor. Once added, use it to create a function getDishes() that sends http request to My Thai Start&#x2019;s back-end. The URL of the back-end can be stored as an environment variable MY_THAI_STAR_DISH.\ndata.service.ts\n...\nimport { HttpClient } from &apos;@angular/common/http&apos;;\nimport { MY_THAI_STAR_DISH } from &apos;../environments/environment&apos;;\n...\nexport class DataService {\nconstructor(private http: HttpClient) {}\n/* Get data from Back-end */\ngetDishes() {\nreturn this.http.get(MY_THAI_STAR_DISH);\n}\n...\n}\nenvironments.ts\n...\nexport const MY_THAI_STAR_DISH =\n&apos;http://de-mucdevondepl01:8090/api/services/rest/dishmanagement/v1/dish/1&apos;;\n...\nenvironments.prod.ts\n...\nexport const MY_THAI_STAR_DISH =\n&apos;http://de-mucdevondepl01:8090/api/services/rest/dishmanagement/v1/dish/1&apos;;\n...\nStep 3: Use the service\nThe component AppComponent implements the interface OnInit and inside its method ngOnInit() the suscription to the services is done. When a dish arrives, it is saved and shown (app.component.html).\n...\nimport { DataService } from &apos;./data.service&apos;;\nexport class AppComponent implements OnInit {\ndish: { name: string; description: string } = { name: &apos;&apos;, description: &apos;&apos;};\n...\nngOnInit() {\nthis.data\n.getDishes()\n.subscribe(\n(dishToday: { dish: { name: string; description: string } }) =&gt; {\nthis.dish = {\nname: dishToday.dish.name,\ndescription: dishToday.dish.description,\n};\n},\n);\n}\n}\nStep 4: Structures, styles and updates\nThis step shows code interesting inside the sample app. The complete content can be found in devon4ng samples.\nindex.html\nTo use the Montserrat font add the following link inside the tag header.\n&lt;link href=&quot;https://fonts.googleapis.com/css?family=Montserrat&quot; rel=&quot;stylesheet&quot;&gt;\nstyles.scss\nbody {\n...\nfont-family: &apos;Montserrat&apos;, sans-serif;\n}\napp.component.ts\nThis file is also used to reload the app if there are any changes.\nSwUpdate: This object comes inside the @angular/pwa package and it is used to detect changes and reload the page if needed.\n...\nimport { SwUpdate } from &apos;@angular/service-worker&apos;;\nexport class AppComponent implements OnInit {\n...\nconstructor(updates: SwUpdate, private data: DataService) {\nupdates.available.subscribe((event) =&gt; {\nupdates.activateUpdate().then(() =&gt; document.location.reload());\n});\n}\n...\n}\nStep 5: Make it Progressive.\nTurining an angular app into a PWA is pretty easy, just one module has to be added. To do so, run: ng add @angular/pwa. This command also adds two important files, explained below.\n&#xA0;\n&#xA0;\nmanifest.json\nmanifest.json is a file that allows to control how the app is displayed in places where native apps are displayed.\nFields\nname: Name of the web application.\nshort_name: Short version of name.\ntheme_color: Default theme color for an application context.\nbackground_color: Expected background color of the web application.\ndisplay: Preferred display mode.\nscope: Navigation scope of tghis web application&#x2019;s application context.\nstart_url: URL loaded when the user launches the web application.\nicons: Array of icons that serve as representations of the web app.\nAdditional information can be found here.\n&#xA0;\n&#xA0;\nngsw-config.json\nnsgw-config.json specifies which files and data URLs have to be cached and updated by the Angular service worker.\nFields\nindex: File that serves as index page to satisfy navigation requests.\nassetGroups: Resources that are part of the app version that update along with the app.\nname: Identifies the group.\ninstallMode: How the resources are cached (prefetch or lazy).\nupdateMode: Caching behaviour when a new version of the app is found (prefetch or lazy).\nresources: Resources to cache. There are three groups.\nfiles: Lists patterns that match files in the distribution directory.\nurls: URL patterns matched at runtime.\ndataGroups: UsefulIdentifies the group. for API requests.\nname: Identifies the group.\nurls: URL patterns matched at runtime.\nversion: Indicates that the resources being cached have been updated in a backwards-incompatible way.\ncacheConfig: Policy by which matching requests will be cached\nmaxSize: The maximum number of entries, or responses, in the cache.\nmaxAge: How long responses are allowed to remain in the cache.\nd: days. (5d = 5 days).\nh: hours\nm: minutes\ns: seconds. (5m20s = 5 minutes and 20 seconds).\nu: milliseconds\ntimeout: How long the Angular service worker will wait for the network to respond before using a cached response. Same dataformat as maxAge.\nstrategy: Caching strategies (performance or freshness).\nnavigationUrls: List of URLs that will be redirected to the index file.\nAdditional information can be found here.\nStep 6: Configure the app\nmanifest.json\nDefault configuration.\n&#xA0;\n&#xA0;\nngsw-config.json\nAt assetGroups &#x2192; resources &#x2192; urls: In this field the google fonts api is added in order to use Montserrat font even without network.\n&quot;urls&quot;: [\n&quot;https://fonts.googleapis.com/**&quot;\n]\nAt the root of the json: A data group to cache API calls.\n{\n...\n&quot;dataGroups&quot;: [{\n&quot;name&quot;: &quot;mythaistar-dishes&quot;,\n&quot;urls&quot;: [\n&quot;http://de-mucdevondepl01:8090/api/services/rest/dishmanagement/v1/dish/1&quot;\n],\n&quot;cacheConfig&quot;: {\n&quot;maxSize&quot;: 100,\n&quot;maxAge&quot;: &quot;1h&quot;,\n&quot;timeout&quot;: &quot;10s&quot;,\n&quot;strategy&quot;: &quot;freshness&quot;\n}\n}]\n}\nStep 7: Check that your app is a PWA\nTo check if an app is a PWA lets compare its normal behaviour against itself but built for production. Run in the project&#x2019;s root folder the commands below:\nng build --prod to build the app using production settings.\nnpm install http-server to install an npm module that can serve your built application. Documentation here.\nGo to the dist/basic-ng-pwa/ folder running cd dist/basic-ng-pwa.\nhttp-server -o to serve your built app.\nFigure 55. Http server running on localhost:8081.\n&#xA0;\nIn another console instance run ng serve to open the common app (not built).\nFigure 56. Angular server running on localhost:4200.\n&#xA0;\nThe first difference can be found on Developer tools &#x2192; application, here it is seen that the PWA application (left) has a service worker and the common (right) one does not.\nFigure 57. Application service worker comparison.\n&#xA0;\nIf the &quot;offline&quot; box is checked, it will force a disconnection from network. In situations where users do not have connectivity or have a slow, one the PWA can still be accesed and used.\nFigure 58. Offline application.\n&#xA0;\nFinally, browser extensions like Lighthouse can be used to test whether an application is progressive or not.\nFigure 59. Lighthouse report.\n16.7. APP_INITIALIZER\n16.7.1. What is the APP_INITIALIZER pattern\nThe APP_INITIALIZER pattern allows an aplication to choose which configuration is going to be used in the start of the application, this is useful because it allows to setup different configurations, for example, for docker or a remote configuration. This provides benefits since this is done on runtime, so theres no need to recompile the whole application to switch from configuration.\n16.7.2. What is APP_INITIALIZER\nAPP_INITIALIZER allows to provide a service in the initialization of the application in a @NgModule. It also allows to use a factory, allowing to create a singleton in the same service. An example can be found in MyThaiStar /core/config/config.module.ts:\nNote\nThe provider expects the return of a Promise, if it is using Observables, a change with the method toPromise() will allow a switch from Observable to Promise\nimport { NgModule, APP_INITIALIZER } from &apos;@angular/core&apos;;\nimport { HttpClientModule } from &apos;@angular/common/http&apos;;\nimport { ConfigService } from &apos;./config.service&apos;;\n@NgModule({\nimports: [HttpClientModule],\nproviders: [\nConfigService,\n{\nprovide: APP_INITIALIZER,\nuseFactory: ConfigService.factory,\ndeps: [ConfigService],\nmulti: true,\n},\n],\n})\nexport class ConfigModule {}\nThis is going to allow the creation of a ConfigService where, using a singleton, the service is going to load an external config depending on a route. This dependence with a route, allows to setup diferent configuration for docker etc. This is seen in the ConfigService of MyThaiStar:\nimport { Injectable } from &apos;@angular/core&apos;;\nimport { HttpClient } from &apos;@angular/common/http&apos;;\nimport { Config, config } from &apos;./config&apos;;\n@Injectable()\nexport class ConfigService {\nconstructor(private httpClient: HttpClient) {}\nstatic factory(appLoadService: ConfigService) {\nreturn () =&gt; appLoadService.loadExternalConfig();\n}\n// this method gets external configuration calling /config endpoint\n//and merges into config object\nloadExternalConfig(): Promise&lt;any&gt; {\nif (!environment.loadExternalConfig) {\nreturn Promise.resolve({});\n}\nconst promise = this.httpClient\n.get(&apos;/config&apos;)\n.toPromise()\n.then((settings) =&gt; {\nObject.keys(settings || {}).forEach((k) =&gt; {\nconfig[k] = settings[k];\n});\nreturn settings;\n})\n.catch((error) =&gt; {\nreturn &apos;ok, no external configuration&apos;;\n});\nreturn promise;\n}\ngetValues(): Config {\nreturn config;\n}\n}\nAs it is mentioned earlier, you can see the use of a factory to create a singleton at the start. After that, loadExternalConfig is going to look for a boolean inside the corresponding environment file inside the path src/environments/, this boolean loadExternalConfig is going to easily allow to switch to a external config. If it is true, it generates a promise that overwrites the parameters of the local config, allowing to load the external config. Finally, the last method getValues() is going to allow to return the file config with the values (overwritten or not). The local config file from MyThaiStar can be seen here:\nexport enum BackendType {\nIN_MEMORY,\nREST,\nGRAPHQL,\n}\ninterface Role {\nname: string;\npermission: number;\n}\ninterface Lang {\nlabel: string;\nvalue: string;\n}\nexport interface Config {\nversion: string;\nbackendType: BackendType;\nrestPathRoot: string;\nrestServiceRoot: string;\npageSizes: number[];\npageSizesDialog: number[];\nroles: Role[];\nlangs: Lang[];\n}\nexport const config: Config = {\nversion: &apos;dev&apos;,\nbackendType: BackendType.REST,\nrestPathRoot: &apos;http://localhost:8081/mythaistar/&apos;,\nrestServiceRoot: &apos;http://localhost:8081/mythaistar/services/rest/&apos;,\npageSizes: [8, 16, 24],\npageSizesDialog: [4, 8, 12],\nroles: [\n{ name: &apos;CUSTOMER&apos;, permission: 0 },\n{ name: &apos;WAITER&apos;, permission: 1 },\n],\nlangs: [\n{ label: &apos;English&apos;, value: &apos;en&apos; },\n{ label: &apos;Deutsch&apos;, value: &apos;de&apos; },\n{ label: &apos;Espa&#xF1;ol&apos;, value: &apos;es&apos; },\n{ label: &apos;Catal&#xE0;&apos;, value: &apos;ca&apos; },\n{ label: &apos;Fran&#xE7;ais&apos;, value: &apos;fr&apos; },\n{ label: &apos;Nederlands&apos;, value: &apos;nl&apos; },\n{ label: &apos;&#x939;&#x93F;&#x928;&#x94D;&#x926;&#x940;&apos;, value: &apos;hi&apos; },\n{ label: &apos;Polski&apos;, value: &apos;pl&apos; },\n{ label: &apos;&#x420;&#x443;&#x441;&#x441;&#x43A;&#x438;&#x439;&apos;, value: &apos;ru&apos; },\n{ label: &apos;&#x431;&#x44A;&#x43B;&#x433;&#x430;&#x440;&#x441;&#x43A;&#x438;&apos;, value: &apos;bg&apos; },\n],\n};\nFinally, inside a environment file src/environments/environment.ts the use of the boolean loadExternalConfig is seen:\n// The file contents for the current environment will overwrite these during build.\n// The build system defaults to the dev environment which uses `environment.ts`, but if you do\n// `ng build --env=prod` then `environment.prod.ts` will be used instead.\n// The list of which env maps to which file can be found in `.angular-cli.json`.\nexport const environment: {\nproduction: boolean;\nloadExternalConfig: boolean;\n} = { production: false, loadExternalConfig: false };\n16.7.3. Creating a APP_INITIALIZER configuration\nThis section is going to be used to create a new APP_INITIALIZER basic example. For this, a basic app with angular is going to be generated using ng new &quot;appname&quot; substituting appname for the name of the app choosed.\n16.7.4. Setting up the config files\nDocker external configuration (Optional)\nThis section is only done if theres a docker configuration in the app you are setting up this type of configuration.\n1.- Create in the root folder /docker-external-config.json. This external config is going to be used when the application is loaded with docker (if the boolean to load the external configuration is set to true). Here you need to add all the config parameter you want to load with docker:\n{\n&quot;version&quot;: &quot;docker-version&quot;\n}\n2.- In the root, in the file /Dockerfile angular is going to copy the docker-external-config.json that was created before into the nginx html route:\n....\nCOPY docker-external-config.json /usr/share/nginx/html/docker-external-config.json\n....\nExternal json configuration\n1.- Create a json file in the route /src/external-config.json. This external config is going to be used when the application is loaded with the start script (if the boolean to load the external configuration is set to true). Here you need to add all the config parameter you want to load:\n{\n&quot;version&quot;: &quot;external-config&quot;\n}\n2.- The file named /angular.json located at the root is going to be modified to add the file external-config.json that was just created to both &quot;assets&quot; inside Build and Test:\n....\n&quot;build&quot;: {\n....\n&quot;assets&quot;: [\n&quot;src/assets&quot;,\n&quot;src/data&quot;,\n&quot;src/favicon.ico&quot;,\n&quot;src/manifest.json&quot;,\n&quot;src/external-config.json&quot;\n]\n....\n&quot;test&quot;: {\n....\n&quot;assets&quot;: [\n&quot;src/assets&quot;,\n&quot;src/data&quot;,\n&quot;src/favicon.ico&quot;,\n&quot;src/manifest.json&quot;,\n&quot;src/external-config.json&quot;\n]\n....\n16.7.5. Setting up the proxies\nThis step is going to setup two proxies. This is going to allow to load the config desired by the context, in case that it is using docker to load the app or in case it loads the app with angular. Loading diferent files is made posible by the fact that the ConfigService method loadExternalConfig() looks for the path /config.\nDocker (Optional)\n1.- This step is going to be for docker. Add docker-external-config.json to nginx configuration (/nginx.conf) that is in the root of the application:\n....\nlocation ~ ^/config {\nalias /usr/share/nginx/html/docker-external-config.json;\n}\n....\nExternal Configuration\n1.- Now the file /proxy.conf.json, needs to be created/modified this file can be found in the root of the application. In this file you can add the route of the external configuration in target and the name of the file in ^/config::\n....\n&quot;/config&quot;: {\n&quot;target&quot;: &quot;http://localhost:4200&quot;,\n&quot;secure&quot;: false,\n&quot;pathRewrite&quot;: {\n&quot;^/config&quot;: &quot;/external-config.json&quot;\n}\n}\n....\n2.- The file package.json found in the root of the application is gonna use the start script to load the proxy config that was just created:\n&quot;scripts&quot;: {\n....\n&quot;start&quot;: &quot;ng serve --proxy-config proxy.conf.json -o&quot;,\n....\n16.7.6. Adding the loadExternalConfig boolean to the environments\nIn order to load an external config we need to add the loadExternalConfig boolean to the environments. To do so, inside the folder environments/ the files are going to get modified adding this boolean to each environment that is going to be used. In this case, only two environments are going to be modified (environment.ts and environment.prod.ts). Down below theres an example of the modification being done in the environment.prod.ts:\nexport const environment: {\nproduction: boolean;\nloadExternalConfig: boolean;\n} = { production: false, loadExternalConfig: false };\nIn the file in first instance theres the declaration of the types of the variables. After that, theres the definition of those variables. This variable loadExternalConfig is going to be used by the service, allowing to setup a external config just by switching the loadExternalConfig to true.\n16.7.7. Creating core configuration service\nIn order to create the whole configuration module three are going to be created:\n1.- Create in the core app/core/config/ a config.ts\nexport interface Config {\nversion: string;\n}\nexport const config: Config = {\nversion: &apos;dev&apos;\n};\nTaking a look to this file, it creates a interface (Config) that is going to be used by the variable that exports (export const config: Config). This variable config is going to be used by the service that is going to be created.\n2.- Create in the core app/core/config/ a config.service.ts:\nimport { Injectable } from &apos;@angular/core&apos;;\nimport { HttpClient } from &apos;@angular/common/http&apos;;\nimport { Config, config } from &apos;./config&apos;;\n@Injectable()\nexport class ConfigService {\nconstructor(private httpClient: HttpClient) {}\nstatic factory(appLoadService: ConfigService) {\nreturn () =&gt; appLoadService.loadExternalConfig();\n}\n// this method gets external configuration calling /config endpoint\n// and merges into config object\nloadExternalConfig(): Promise&lt;any&gt; {\nif (!environment.loadExternalConfig) {\nreturn Promise.resolve({});\n}\nconst promise = this.httpClient\n.get(&apos;/config&apos;)\n.toPromise()\n.then((settings) =&gt; {\nObject.keys(settings || {}).forEach((k) =&gt; {\nconfig[k] = settings[k];\n});\nreturn settings;\n})\n.catch((error) =&gt; {\nreturn &apos;ok, no external configuration&apos;;\n});\nreturn promise;\n}\ngetValues(): Config {\nreturn config;\n}\n}\nAs it was explained in previous steps, at first, there is a factory that uses the method loadExternalConfig(), this factory is going to be used in later steps in the module. After that, the loadExternalConfig() method checks if the boolean in the environment is false. If it is false it will return the promise resolved with the normal config. Else, it is going to load the external config in the path (/config), and overwrite the values from the external config to the config thats going to be used by the app, this is all returned in a promise.\n3.- Create in the core a module for the config app/core/config/ a config.module.ts:\nimport { NgModule, APP_INITIALIZER } from &apos;@angular/core&apos;;\nimport { HttpClientModule } from &apos;@angular/common/http&apos;;\nimport { ConfigService } from &apos;./config.service&apos;;\n@NgModule({\nimports: [HttpClientModule],\nproviders: [\nConfigService,\n{\nprovide: APP_INITIALIZER,\nuseFactory: ConfigService.factory,\ndeps: [ConfigService],\nmulti: true,\n},\n],\n})\nexport class ConfigModule {}\nAs seen earlier, the ConfigService is added to the module. In this addition, the app is initialized(provide) and it uses the factory that was created in the ConfigService loading the config with or without the external values depending on the boolean in the config.\nUsing the Config Service\nAs a first step, in the file /app/app.module.ts the ConfigModule created earlier in the other step is going to be imported:\nimports: [\n....\nConfigModule,\n....\n]\nAfter that, the ConfigService is going to be injected into the app.component.ts\n....\nimport { ConfigService } from &apos;./core/config/config.service&apos;;\n....\nexport class AppComponent {\n....\nconstructor(public configService: ConfigService) { }\n....\nFinally, for this demonstration app, the component app/app.component.html is going to show the version of the config it is using at that moment.\n&lt;div style=&quot;text-align:center&quot;&gt;\n&lt;h1&gt;\nWelcome to {{ title }}!\n&lt;/h1&gt;\n&lt;/div&gt;\n&lt;h2&gt;Here is the configuration version that is using angular right now: {{configService.getValues().version}}&lt;/h2&gt;\nFinal steps\nThe script start that was created earlier in the package.json (npm start) is going to be used to start the application. After that, modifying the boolean loadExternalConfig inside the corresponding environment file inside /app/environments/ should show the different config versions.\n16.8. Component Decomposition\nWhen implementing a new requirement there are a few design decisions, which need to be considered.\nA decomposition in Smart and Dumb Components should be done first.\nThis includes the definition of state and responsibilities.\nImplementing a new dialog will most likely be done by defining a new Smart Component with multiple Dumb Component children.\nIn the component tree this would translate to the definition of a new subtree.\nFigure 60. Component Tree with highlighted subtree\n16.8.1. Defining Components\nThe following gives an example for component decomposition.\nShown is a screenshot from a styleguide to be implemented.\nIt is a widget called Listpicker.\nThe basic function is an input field accepting direct input.\nSo typing otto puts otto inside the FormControl.\nWith arrow down key or by clicking the icon displayed in the inputs right edge a dropdown is opened.\nInside possible values can be selected and filtered beforehand.\nAfter pressing arrow down key the focus should move into the filter input field.\nUp and down arrow keys can be used to select an element from the list.\nTyping into the filter input field filters the list from which the elements can be selected.\nThe current selected element is highlighted with green background color.\nFigure 61. Component decomposition example before\nWhat should be done, is to define small reusable Dumb Components.\nThis way the complexity becomes manageable.\nIn the example every colored box describes a component with the purple box being a Smart Component.\nFigure 62. Component decomposition example after\nThis leads to the following component tree.\nFigure 63. Component decomposition example component tree\nNote the uppermost component is a Dumb Component.\nIt is a wrapper for the label and the component to be displayed inside a form.\nThe Smart Component is Listpicker.\nThis way the widget can be reused without a form needed.\nA widgets is a typical Smart Component to be shared across feature modules.\nSo the SharedModule is the place for it to be defined.\n16.8.2. Defining state\nEvery UI has state.\nThere are different kinds of state, for example\nView State: e.g. is a panel open, a css transition pending, etc.\nApplication State: e.g. is a payment pending, current URL, user info, etc.\nBusiness Data: e.g. products loaded from backend\nIt is good practice to base the component decomposition on the state handled by a component and to define a simplified state model beforehand.\nStarting with the parent - the Smart Component:\nWhat overall state does the dialog have: e.g. loading, error, valid data loaded, valid input, invalid input, etc.\nEvery defined value should correspond to an overall appearance of the whole dialog.\nWhat events can occur to the dialog: e.g. submitting a form, changing a filter, pressing buttons, pressing keys, etc.\nFor every Dumb Component:\nWhat data does a component display: e.g. a header text, user information to be displayed, a loading flag, etc.\nThis will be a slice of the overall state of the parent Smart Component.\nIn general a Dumb Component presents a slice of its parent Smart Components state to the user.\nWhat events can occur: keyboard events, mouse events, etc.\nThese events are all handled by its parent Smart Component - every event is passed up the tree to be handled by a Smart Component.\nThese information should be reflected inside the modeled state.\nThe implementation is a TypeScript type - an interface or a class describing the model.\nSo there should be a type describing all state relevant for a Smart Component.\nAn instance of that type is send down the component tree at runtime.\nNot every Dumb Component will need the whole state.\nFor instance a single Dumb Component could only need a single string.\nThe state model for the previous Listpicker example is shown in the following listing.\nListing 39. Listpicker state model\nexport class ListpickerState {\nitems: {}[]|undefined;\ncolumns = [&apos;key&apos;, &apos;value&apos;];\nkeyColumn = &apos;key&apos;;\ndisplayValueColumn = &apos;value&apos;;\nfilteredItems: {}[]|undefined;\nfilter = &apos;&apos;;\nplaceholder = &apos;&apos;;\ncaseSensitive = true;\nisDisabled = false;\nisDropdownOpen = false;\nselectedItem: {}|undefined;\ndisplayValue = &apos;&apos;;\n}\nListpicker holds an instance of ListpickerState which is passed down the component tree via @Input() bindings in the Dumb Components.\nEvents emitted by children - Dumb Components - create a new instance of ListpickerState based on the current instance and the event and its data.\nSo a state transition is just setting a new instance of ListpickerState.\nAngular Bindings propagate the value down the tree after exchanging the state.\nListing 40. Listpicker State transition\nexport class ListpickerComponent {\n// initial default values are set\nstate = new ListpickerState();\n/** User changes filter */\nonFilterChange(filter: string): void {\n// apply filter ...\nconst filteredList = this.filterService.filter(...);\n// important: A new instance is created, instead of altering the existing one.\n// This makes change detection easier and prevents hard to find bugs.\nthis.state = Object.assing({}, this.state, {\nfilteredItems: filteredList,\nfilter: filter\n});\n}\n}\nNote:\nIt is not always necessary to define the model as independent type.\nSo there would be no state property and just properties for every state defined directly in the component class.\nWhen complexity grows and state becomes larger this is usually a good idea.\nIf the state should be shared between Smart Components a store is to be used.\n16.8.3. When are Dumb Components needed\nSometimes it is not necessary to perform a full decomposition. The architecture does not enforce it generally. What you should keep in mind is, that there is always a point when it becomes recommendable.\nFor example a template with 800 loc is:\nnot understandable\nnot maintanable\nnot testable\nnot reusable\nSo when implementing a template with more than 50 loc you should think about decomposition.\n16.9. Consuming REST services\nA good introduction to working with Angular HttpClient can be found in Angular Docs\nThis guide will cover, how to embed Angular HttpClient in the application architecture.\nFor backend request a special service with the suffix Adapter needs to be defined.\n16.9.1. Defining Adapters\nIt is a good practice to have a Angular service whose single responsibility is to call the backend and parse the received value to a transfer data model (e.g. Swagger generated TOs).\nThose services need to have the suffix Adapter to make them easy to recognize.\nFigure 64. Adapters handle backend communication\nAs illustrated in the figure a Use Case service does not use Angular HttpClient directly but uses an adapter.\nA basic adapter could look like this:\nListing 41. Example adapter\nimport { Injectable } from &apos;@angular/core&apos;;\nimport { HttpClient } from &apos;@angular/common/http&apos;;\nimport { Observable } from &apos;rxjs/Observable&apos;;\nimport { FlightTo } from &apos;./flight-to&apos;;\n@Injectable()\nexport class FlightsAdapter {\nconstructor(\nprivate httpClient: HttpClient\n) {}\ngetFlights(): Observable&lt;FlightTo&gt; {\nreturn this.httpClient.get&lt;FlightTo&gt;(&apos;/relative/url/to/flights&apos;);\n}\n}\nThe adapters should use a well-defined transfer data model.\nThis could be generated from server endpoints with CobiGen, Swagger, typescript-maven-plugin, etc.\nIf inside the application there is a business model defined, the adapter has to parse to the transfer model.\nThis is illustrated in the following listing.\nListing 42. Example adapter mapping from business model to transfer model\nimport { Injectable } from &apos;@angular/core&apos;;\nimport { HttpClient } from &apos;@angular/common/http&apos;;\nimport { Observable } from &apos;rxjs/Observable&apos;;\nimport { map } from &apos;rxjs/operators&apos;;\nimport { FlightTo } from &apos;./flight-to&apos;;\nimport { Flight } from &apos;../../../model/flight&apos;;\n@Injectable()\nexport class FlightsAdapter {\nconstructor(\nprivate httpClient: HttpClient\n) {}\nupdateFlight(flight: Flight): Observable&lt;Flight&gt; {\nconst to = this.mapFlight(flight);\nreturn this.httpClient.post&lt;FlightTo&gt;(&apos;/relative/url/to/flights&apos;, to).pipe(\nmap(to =&gt; this.mapFlightTo(to))\n);\n}\nprivate mapFlight(flight: Flight): FlightTo {\n// mapping logic\n}\nprivate mapFlightTo(flightTo: FlightTo): Flight {\n// mapping logic\n}\n}\n16.9.2. Token management\n16.10. Error Handler in angular\nAngular allows us to set up a custom error handler that can be used to control the different errors and them in a correct way. Using a global error handler will avoid mistakes and provide a use friendly interface allowing us to indicate the user what problem is happening.\n16.10.1. What is ErrorHandler\nErrorHandler is the class that Angular uses by default to control the errors. This means that, even if the application doesnt have a ErrorHandler it is going to use the one setup by default in Angular. This can be tested by trying to find a page not existing in any app, instantly Angular will print the error in the console.\n16.10.2. Creating your custom ErrorHandler step by step\nIn order to create a custom ErrorHandler three steps are going to be needed:\nCreating the custom ErrorHandler class\nIn this first step the custom ErrorHandler class is going to be created inside the folder /app/core/errors/errors-handler.ts:\nimport { ErrorHandler, Injectable, Injector } from &apos;@angular/core&apos;;\nimport { HttpErrorResponse } from &apos;@angular/common/http&apos;;\n@Injectable()\nexport class ErrorsHandler implements ErrorHandler {\nconstructor(private injector: Injector) {}\nhandleError(error: Error | HttpErrorResponse) {\n// To do: Use injector to get the necessary services to redirect or\n// show a message to the user\nconst classname = error.constructor.name;\nswitch ( classname ) {\ncase &apos;HttpErrorResponse&apos;:\nconsole.error(&apos;HttpError:&apos; + error.message);\nif (!navigator.onLine) {\nconsole.error(&apos;Theres no internet connection&apos;);\n// To do: control here in internet what you wanna do if user has no internet\n} else {\nconsole.error(&apos;Server Error:&apos; + error.message);\n// To do: control here if the server gave an error\n}\nbreak;\ndefault:\nconsole.error(&apos;Error:&apos; + error.message);\n// To do: control here if the client/other things gave an error\n}\n}\n}\nThis class can be used to control the different type of errors. If wanted, the classname variable could be used to add more switch cases. This would allow control of more specific situations.\nCreating a ErrorInterceptor\nInside the same folder created in the last step we are going to create the ErrorInterceptor(errors-handler-interceptor.ts). This ErrorInterceptor is going to retry any failed calls to the server to make sure it is not being found before showing the error:\nimport { HttpInterceptor, HttpRequest, HttpHandler, HttpEvent } from &apos;@angular/common/http&apos;;\nimport { Injectable } from &apos;@angular/core&apos;;\nimport { Observable } from &apos;rxjs&apos;;\nimport { retry } from &apos;rxjs/operators&apos;;\n@Injectable()\nexport class ErrorsHandlerInterceptor implements HttpInterceptor {\nconstructor() {}\nintercept(req: HttpRequest&lt;any&gt;, next: HttpHandler): Observable&lt;HttpEvent&lt;any&gt;&gt; {\nreturn next.handle(req).pipe(\nretryWhen((errors: Observable&lt;any&gt;) =&gt; errors.pipe(\ndelay(500),\ntake(5),\nconcatMap((error: any, retryIndex: number) =&gt; {\nif (++retryIndex === 5) {\nthrow error;\n}\nreturn of(error);\n})\n))\n);\n}\n}\nThis custom made interceptor is implementing the HttpInterceptor and inside the method intercept using the method pipe,retryWhen,delay,take and concatMap from RxJs it is going to do the next things if there is errors:\nWith delay(500) do a delay to allow some time in between requests\nWith take(5) retry five times.\nWith concatMap if the index that take() gives is not 5 it returns the error, else, it throws the error.\nCreating a Error Module\nFinally, creating a module(errors-handler.module.ts) is necessary to include the interceptor and the custom error handler. In this case, the module is going to be created in the same folder as the last two:\nimport { NgModule, ErrorHandler } from &apos;@angular/core&apos;;\nimport { CommonModule } from &apos;@angular/common&apos;;\nimport { ErrorsHandler } from &apos;./errors-handler&apos;;\nimport { HTTP_INTERCEPTORS } from &apos;@angular/common/http&apos;;\nimport { ErrorsHandlerInterceptor } from &apos;./errors-handler-interceptor&apos;;\n@NgModule({\ndeclarations: [], // Declare here component if you want to use routing to error component\nimports: [\nCommonModule\n],\nproviders: [\n{\nprovide: ErrorHandler,\nuseClass: ErrorsHandler,\n},\n{\nprovide: HTTP_INTERCEPTORS,\nuseClass: ErrorsHandlerInterceptor,\nmulti: true,\n}\n]\n})\nexport class ErrorsHandlerModule { }\nThis module simply is providing the services that are implemented by our custom classes and then telling angular to use our custom made classes instead of the default ones. After doing this, the module has to be included in the app module app.module.ts in order to be used.\n....\nimports: [\nErrorsHandlerModule,\n....\n16.10.3. Handling Errors\nAs a final step, handling these errors is necessary. Theres different ways that can be used to control the errors, here are a few:\nCreating a custom page and using with Router to redirect to a page showing an error.\nCreating a service in the server side or Backend to create a log with the error and calling it with HttpClient.\nShowing a custom made SnackBar with the error message.\nUsing SnackBarService and NgZone\nIf the SnackBar is used directly, some errors can ocurr, this is due to SnackBar being out of the Angular zone. In order to use this service properly, NgZone is necessary. The method run() from NgZone will allow the service to be inside the Angular Zone. An example on how to use it:\nimport { ErrorHandler, Injectable, Injector, NgZone } from &apos;@angular/core&apos;;\nimport { HttpErrorResponse } from &apos;@angular/common/http&apos;;\nimport { MatSnackBar } from &apos;@angular/material&apos;;\n@Injectable()\nexport class ErrorsHandler implements ErrorHandler {\nconstructor(private injector: Injector, private zone: NgZone) {}\nhandleError(error: Error | HttpErrorResponse) {\n// Use injector to get the necessary services to redirect or\nconst snackBar: MatSnackBar = this.injector.get(MatSnackBar);\nconst classname = error.constructor.name;\nlet message: string;\nswitch ( classname ) {\ncase &apos;HttpErrorResponse&apos;:\nmessage = !(navigator.onLine) ? &apos;There is no internet connection&apos; : error.message;\nbreak;\ndefault:\nmessage = error.message;\n}\nthis.zone.run(\n() =&gt; snackBar.open(message, &apos;danger&apos;, { duration : 4000})\n);\n}\n}\nUsing Injector the MatSnackBar is obtained, then the correct message is obtained inside the switch. Finally, using NgZone and run(), we open the SnackBar passing the message, and the paremeters wanted.\n16.11. File Structure\n16.11.1. Toplevel\nThe toplevel file structure is defined by Angular CLI. You might put this &quot;toplevel file structure&quot; into a subdirectory to facilitate your build, but this is not relevant for this guide. So the applications file structure relevant to this guide is the folder /src/app inside the part managed by Angular CLI.\nListing 43. Toplevel file structure shows feature modules\n/src\n&#x2514;&#x2500;&#x2500; /app\n&#x251C;&#x2500;&#x2500; /account-management\n&#x251C;&#x2500;&#x2500; /billing\n&#x251C;&#x2500;&#x2500; /booking\n&#x251C;&#x2500;&#x2500; /core\n&#x251C;&#x2500;&#x2500; /shared\n&#x251C;&#x2500;&#x2500; /status\n|\n&#x251C;&#x2500;&#x2500; app.module.ts\n&#x251C;&#x2500;&#x2500; app.component.spec.ts\n&#x251C;&#x2500;&#x2500; app.component.ts\n&#x2514;&#x2500;&#x2500; app.routing-module.ts\nBesides the definition of app module the app folder has feature modules on toplevel.\nThe special modules shared and core are present as well.\n16.11.2. Feature Modules\nA feature module contains the modules definition and two folders representing both layers.\nListing 44. Feature module file structure has both layers\n/src\n&#x2514;&#x2500;&#x2500; /app\n&#x2514;&#x2500;&#x2500; /account-management\n&#x251C;&#x2500;&#x2500; /components\n&#x251C;&#x2500;&#x2500; /services\n|\n&#x251C;&#x2500;&#x2500; account-management.module.ts\n&#x251C;&#x2500;&#x2500; account-management.component.spec.ts\n&#x251C;&#x2500;&#x2500; account-management.component.ts\n&#x2514;&#x2500;&#x2500; account-management.routing-module.ts\nAdditionally an entry component is possible. This would be the case in lazy loading scenarios.\nSo account-management.component.ts would be only present if account-management is lazy loaded.\nOtherwise, the module&#x2019;s routes would be defined Component-less\n(see vsavkin blog post).\n16.11.3. Components Layer\nThe component layer reflects the distinction between Smart Components and Dumb Components.\nListing 45. Components layer file structure shows Smart Components on toplevel\n/src\n&#x2514;&#x2500;&#x2500; /app\n&#x2514;&#x2500;&#x2500; /account-management\n&#x2514;&#x2500;&#x2500; /components\n&#x251C;&#x2500;&#x2500; /account-overview\n&#x251C;&#x2500;&#x2500; /confirm-modal\n&#x251C;&#x2500;&#x2500; /create-account\n&#x251C;&#x2500;&#x2500; /forgot-password\n&#x2514;&#x2500;&#x2500; /shared\nEvery folder inside the /components folder represents a smart component. The only exception is /shared.\n/shared contains Dumb Components shared across Smart Components inside the components layer.\nListing 46. Smart components contain Dumb components\n/src\n&#x2514;&#x2500;&#x2500; /app\n&#x2514;&#x2500;&#x2500; /account-management\n&#x2514;&#x2500;&#x2500; /components\n&#x2514;&#x2500;&#x2500; /account-overview\n&#x251C;&#x2500;&#x2500; /user-info-panel\n| &#x251C;&#x2500;&#x2500; /address-tab\n| &#x251C;&#x2500;&#x2500; /last-activities-tab\n| |\n| &#x251C;&#x2500;&#x2500; user-info-panel.component.html\n| &#x251C;&#x2500;&#x2500; user-info-panel.component.scss\n| &#x251C;&#x2500;&#x2500; user-info-panel.component.spec.ts\n| &#x2514;&#x2500;&#x2500; user-info-panel.component.ts\n|\n&#x251C;&#x2500;&#x2500; /user-header\n&#x251C;&#x2500;&#x2500; /user-toolbar\n|\n&#x251C;&#x2500;&#x2500; account-overview.component.html\n&#x251C;&#x2500;&#x2500; account-overview.component.scss\n&#x251C;&#x2500;&#x2500; account-overview.component.spec.ts\n&#x2514;&#x2500;&#x2500; account-overview.component.ts\nInside the folder of a Smart Component the component is defined.\nBesides that are folders containing the Dumb Components the Smart Component consists of.\nThis can be recursive - a Dumb Component can consist of other Dumb Components.\nThis is reflected by the file structure as well. This way the structure of a view becomes very readable.\nAs mentioned before, if a Dumb Component is used by multiple Smart Components inside the components layer\nit is put inside the /shared folder inside the components layer.\nWith this way of thinking the shared module makes a lot of sense. If a Dumb Component is used by multiple Smart Components\nfrom different feature modules, the Dumb Component is placed into the shared module.\nListing 47. The shared module contains Dumb Components shared across Smart Components from different feature modules\n/src\n&#x2514;&#x2500;&#x2500; /app\n&#x2514;&#x2500;&#x2500; /shared\n&#x2514;&#x2500;&#x2500; /user-panel\n|\n&#x251C;&#x2500;&#x2500; user-panel.component.html\n&#x251C;&#x2500;&#x2500; user-panel.component.scss\n&#x251C;&#x2500;&#x2500; user-panel.component.spec.ts\n&#x2514;&#x2500;&#x2500; user-panel.component.ts\nThe layer folder /components is not necessary inside the shared module.\nThe shared module only contains components!\n16.12. Internationalization\nNowadays, a common scenario in front-end applications is to have the ability to translate labels and locate numbers, dates, currency and so on when the user clicks over a language selector or similar. devon4ng and specifically Angular has a default mechanism in order to fill the gap of such features, and besides there are some wide used libraries that make even easier to translate applications.\nMore info at Angular i18n official documentation\n16.12.1. devon4ng i18n approach\nThe official approach could be a bit complicated, therefore the recommended one is to use the recommended library NGX Translate from http://www.ngx-translate.com/.\nInstall NGX Translate\nIn order to include this library in your devon4ng Angular &gt;= 4.3 project you will need to execute in a terminal:\n$ npm install @ngx-translate/core @ngx-translate/http-loader --save\n# or if you use yarn\n$ yarn add @ngx-translate/core @ngx-translate/http-loader\n@ngx-translate/core is the core library to provide i18n capabilities.\n@ngx-translate/http-loader is a loader for ngx-translate that loads translations using http.\nConfigure NGX Translate\nDepending on the volume of the devon4ng application we will include the NGX Translate library in the app.module.ts or in the core.module.ts transversal to the application.\nimport { BrowserModule } from &apos;@angular/platform-browser&apos;;\nimport { NgModule } from &apos;@angular/core&apos;;\nimport { HttpClientModule, HttpClient } from &apos;@angular/common/http&apos;;\nimport { TranslateModule, TranslateLoader } from &apos;@ngx-translate/core&apos;;\nimport { TranslateHttpLoader } from &apos;@ngx-translate/http-loader&apos;;\nNext, an exported function for factories has to be created:\n// AoT requires an exported function for factories\nexport function HttpLoaderFactory(http: HttpClient) {\nreturn new TranslateHttpLoader(http);\n}\n@NgModule({\nimports: [\nBrowserModule,\nHttpClientModule,\nTranslateModule.forRoot({\nloader: {\nprovide: TranslateLoader,\nuseFactory: HttpLoaderFactory,\ndeps: [HttpClient]\n}\n})\n],\nbootstrap: [AppComponent]\n})\nexport class AppModule { } // or CoreModule\nThe TranslateHttpLoader also has two optional parameters:\nprefix: string = &quot;/assets/i18n/&quot;\nsuffix: string = &quot;.json&quot;\nBy using those default parameters, it will load the translations files for the lang &quot;en&quot; from: /assets/i18n/en.json. In general, any translation file will loaded from the /assets/i18n/ folder.\nThose parameters can be changed in the HttpLoaderFactory method just defined. For example if you want to load the &quot;en&quot; translations from /public/lang-files/en-lang.json you would use:\nexport function HttpLoaderFactory(http: HttpClient) {\nreturn new TranslateHttpLoader(http, &quot;/public/lang-files/&quot;, &quot;-lang.json&quot;);\n}\nFor now this loader only support the json format.\nNote\nIf you&#x2019;re still on Angular &lt; 4.3, please use Http from @angular/http with http-loader@0.1.0.\nUsage\nIn order to translate any label in any HTML template you will need to use the translate pipe available:\n{{ &apos;HELLO&apos; | translate }}\nAn optional parameter from the component TypeScript class could be included as follows:\n{{ &apos;HELLO&apos; | translate:param }}\nSo, param has to be defined in the class. The default language used is defined as follows:\n// imports\n@Component({\nselector: &apos;app&apos;,\ntemplate: `\n&lt;div&gt;{{ &apos;HELLO&apos; | translate }}&lt;/div&gt; // Without param\n&lt;div&gt;{{ &apos;HELLO&apos; | translate:param }}&lt;/div&gt; // With param\n`\n})\nexport class AppComponent {\n// This param will be used in the translation\nparam = { value: &apos;world&apos; };\nconstructor(translate: TranslateService) {\n// this language will be used as a fallback when a translation isn&apos;t found in the current language\ntranslate.setDefaultLang(&apos;en&apos;);\n// the lang to use, if the lang isn&apos;t available, it will use the current loader to get them\ntranslate.use(&apos;en&apos;);\n}\n}\nIn order to change the language used you will need to create a button or selector that calls the this.translate.use(language: string) method from TranslateService. For example:\ntoggleLanguage(option) {\nthis.translate.use(option);\n}\nThe translations will be included in the en.json, es.json, de.json, etc. files inside the /assets/i18n folder. For example en.json would be (using the previous param):\n{\n&quot;HELLO&quot;: &quot;hello&quot;\n}\nOr with an optional param:\n{\n&quot;HELLO&quot;: &quot;hello {{value}}&quot;\n}\nThe TranslateParser understands nested JSON objects. This means that you can have a translation that looks like this:\n{\n&quot;HOME&quot;: {\n&quot;HELLO&quot;: &quot;hello {{value}}&quot;\n}\n}\nIn order to access access the value, use the dot notation, in this case HOME.HELLO.\nUsing the service, pipe or directive\nService\nIf you need to access translations in any component or service you can do it injecting the Translateservice into them:\ntranslate.get(&apos;HELLO&apos;, {value: &apos;world&apos;}).subscribe((res: string) =&gt; {\nconsole.log(res);\n//=&gt; &apos;hello world&apos;\n});\nPipe\nThe use of pipes can be possible too:\ntemplate:\n&lt;div&gt;{{ &apos;HELLO&apos; | translate:param }}&lt;/div&gt;\ncomponent:\nparam = {value: &apos;world&apos;};\nDirectives\nFinally, it can also be used with directives:\n&lt;div [translate]=&quot;&apos;HELLO&apos;&quot; [translateParams]=&quot;{value: &apos;world&apos;}&quot;&gt;&lt;/div&gt;\nor, using the content of your element as a key\n&lt;div translate [translateParams]=&quot;{value: &apos;world&apos;}&quot;&gt;HELLO&lt;/div&gt;\nImportant\nYou can find a complete example at https://github.com/devonfw/devon4ng-application-template.\nPlease, visit https://github.com/ngx-translate/core for more info.\n16.13. Routing\nA basic introduction to the Angular Router can be found in Angular Docs.\nThis guide will show common tasks and best practices.\n16.13.1. Defining Routes\nFor each feature module and the app module all routes should be defined in a seperate module with the suffix RoutingModule.\nThis way the routing modules are the only place where routes are defined.\nThis pattern achieves a clear seperation of concernes.\nThe following figure illustrates this.\nFigure 65. Routing module declaration\nIt is important to define routes inside app routing module with .forRoot() and in feature routing modules with .forChild().\nExample 1 - No Lazy Loading\nIn this example two modules need to be configured with routes - AppModule and FlightModule.\nThe following routes will be configured\n/ will redirect to /search\n/search displays FlightSearchComponent (FlightModule)\n/search/print/:flightId/:date displays FlightPrintComponent (FlightModule)\n/search/details/:flightId/:date displays FlightDetailsComponent (FlightModule)\nAll other routes will display ErrorPage404 (AppModule)\nListing 48. app-routing.module.ts\nconst routes: Routes = [\n{ path: &apos;&apos;, redirectTo: &apos;search&apos;, pathMatch: &apos;full&apos; },\n{ path: &apos;**&apos;, component: ErrorPage404 }\n];\n@NgModule({\nimports: [RouterModule.forRoot(routes)],\nexports: [RouterModule]\n})\nexport class AppRoutingModule { }\nListing 49. flight-search-routing.module.ts\nconst routes: Routes = [\n{\npath: &apos;search&apos;, children: [\n{ path: &apos;&apos;, component: FlightSearchComponent },\n{ path: &apos;print/:flightId/:date&apos;, component: FlightPrintComponent },\n{ path: &apos;details/:flightId/:date&apos;, component: FlightDetailsComponent }\n]\n}\n];\n@NgModule({\nimports: [RouterModule.forChild(routes)],\nexports: [RouterModule],\n})\nexport class FlightSearchRoutingModule { }\nTip\nThe import order inside AppModule is important.\nAppRoutingModule needs to be imported after FlightModule.\nExample 2 - Lazy Loading\nLazy Loading is a good practice when the application has multiple feature areas and a user might not visit every dialog.\nOr at least he might not need every dialog up front.\nThe following example will configure the same routes as example 1 but will lazy load FlightModule.\nListing 50. app-routing.module.ts\nconst routes: Routes = [\n{ path: &apos;/search&apos;, loadChildren: &apos;app/flight-search/flight-search.module#FlightSearchModule&apos; },\n{ path: &apos;**&apos;, component: ErrorPage404 }\n];\n@NgModule({\nimports: [RouterModule.forRoot(routes)],\nexports: [RouterModule]\n})\nexport class AppRoutingModule { }\nListing 51. flight-search-routing.module.ts\nconst routes: Routes = [\n{\npath: &apos;&apos;, children: [\n{ path: &apos;&apos;, component: FlightSearchComponent },\n{ path: &apos;print/:flightId/:date&apos;, component: FlightPrintComponent },\n{ path: &apos;details/:flightId/:date&apos;, component: FlightDetailsComponent }\n]\n}\n];\n@NgModule({\nimports: [RouterModule.forChild(routes)],\nexports: [RouterModule],\n})\nexport class FlightSearchRoutingModule { }\n16.13.2. Triggering Route Changes\nWith Angular you have two ways of triggering route changes.\nDeclarative with bindings in component HTML templates\nProgrammatic with Angular Router service inside component classes\nOn the one hand, architecture-wise it is a much cleaner solution to trigger route changes in Smart Components.\nThis way you have every UI event that should trigger a navigation handled in one place - in a Smart Component.\nIt becomes very easy to look inside the code for every navigation, that can occure.\nRefactoring is also much easier, as there are no navigation events &quot;hidden&quot; in the HTML templates\nOn the other hand, in terms of accessibility and SEO it is a better solution to rely on bindings in the view - e.g. by using Angulars router-link directive.\nThis way screen readers and the Google crawler can move through the page easily.\nTip\nIf you do not have to support accessibility (screen readers, etc.) and to care about SEO (Google rank, etc.),\nthen you should aim for triggering navigations only in Smart Components.\nFigure 66. Triggering navigation\n16.13.3. Guards\nGuards are Angular services implemented on routes which determines whether a user can naviagate to/from the route. There are examples below which will explain things better. We have the following types of Guards:\nCanActivate: It is used to determine whether a user can visit a route. The most common scenario for this guard is to check if the user is authenticated. For example, if we want only logged in users to be able to go to a particular route, we will implement the CanActivate guard on this route.\nCanActivateChild: Same as above, only implemented on child routes.\nCanDeactivate: It is used to determine if a user can naviagate away from a route. Most common example is when a user tries to go to a different page after filling up a form and does not save/submit the changes, we can use this guard to confirm whether the user really wants to leave the page without saving/submiting.\nResolve: For resolving dynamic data.\nCanLoad: It is used to determine whether an Angular module can be loaded lazily. Example below will be helpful to understand it.\nLet&#x2019;s have a look at some examples.\nExample 1 - CanActivate and CanActivateChild guards\nCanActivate guard\nAs mentioned earlier, a guard is an Angular service and services are simply TypeScript classes. So we begin by creating a class. This class has to implement the CanActivate interface (imported from angular/router), and therefore, must have a canActivate function. The logic of this function determines whether the requested route can be navigated to or not. It returns either a boolean value or an Observable or a Promise which resolves to a boolean value. If it is true, the route is loaded, else not.\nListing 52. CanActivate example\n...\nimport {CanActivate} from &quot;@angular/router&quot;;\n@Injectable()\nclass ExampleAuthGuard implements CanActivate {\nconstructor(private authService: AuthService) {}\ncanActivate(route: ActivatedRouterSnapshot, state: RouterStateSnapshot) {\nif (this.authService.isLoggedIn()) {\nreturn true;\n} else {\nwindow.alert(&apos;Please log in first&apos;);\nreturn false;\n}\n}\n}\nIn the above example, let&#x2019;s assume we have a AuthService which has a isLoggedIn() method which returns a boolean value depending on whether the user is logged in. We use it to return true or false from the canActivate function.\nThe canActivate function accepts two parameters (provided by Angular). The first parameter of type ActivatedRouterSnapshot is the snapshot of the route the user is trying to naviagate to (where the guard is implemented); we can extract the route parameters from this instance. The second parameter of type RouterStateSnapshot is a snapshot of the router state the user is trying to naviagate to; we can fetch the URL from it&#x2019;s url property.\nTip\nWe can also redirect the user to another page (maybe a login page) if the authService returns false. To do that, inject Router and use it&#x2019;s naviagate function to redirect to the appropriate page.\nSince it is a service, it needs to be provided in our module:\nListing 53. provide the guard in a module\n@NgModule({\n...\nproviders: [\n...\nExampleAuthGuard\n]\n})\nNow this guard is ready to use on our routes. We implement it where we define our array of routes in the application:\nListing 54. Implementing the guard\n...\nconst routes: Routes = [\n{ path: &apos;&apos;, redirectTo: &apos;home&apos;, pathMatch: &apos;full&apos; },\n{ path: &apos;home&apos;, component: HomeComponent },\n{ path: &apos;page1&apos;, component: Page1Component, canActivate: [ExampleAuthGuard] }\n];\nAs you can see, the canActivate property accepts an array of guards. So we can implement more than one guard on a route.\nCanActivateChild guard\nTo use the guard on nested (children) routes, we add it to the canActivateChild property like so:\nListing 55. Implementing the guard on child routes\n...\nconst routes: Routes = [\n{ path: &apos;&apos;, redirectTo: &apos;home&apos;, pathMatch: &apos;full&apos; },\n{ path: &apos;home&apos;, component: HomeComponent },\n{ path: &apos;page1&apos;, component: Page1Component, canActivateChild: [ExampleAuthGuard], children: [\n{path: &apos;sub-page1&apos;, component: SubPageComponent},\n{path: &apos;sub-page2&apos;, component: SubPageComponent}\n] }\n];\nExample 2 - CanLoad guard\nSimilar to CanActivate, to use this guard we implement the CanLoad interface and overwrite it&#x2019;s canLoad function. Again, this function returns either a boolean value or an Observable or a Promise which resolves to a boolean value. The fundamental difference between CanActivate and CanLoad is that CanLoad is used to determine whether an entire module can be lazily loaded or not. If the guard returns false for a module protected by CanLoad, the entire module is not loaded.\nListing 56. CanLoad example\n...\nimport {CanLoad, Route} from &quot;@angular/router&quot;;\n@Injectable()\nclass ExampleCanLoadGuard implements CanLoad {\nconstructor(private authService: AuthService) {}\ncanLoad(route: Route) {\nif (this.authService.isLoggedIn()) {\nreturn true;\n} else {\nwindow.alert(&apos;Please log in first&apos;);\nreturn false;\n}\n}\n}\nAgain, let&#x2019;s assume we have a AuthService which has a isLoggedIn() method which returns a boolean value depending on whether the user is logged in. The canLoad function accepts a parameter of type Route which we can use to fetch the path a user is trying to navigate to (using the path property of Route).\nThis guard needs to be provided in our module like any other service.\nTo implement the guard, we use the canLoad property:\nListing 57. Implementing the guard\n...\nconst routes: Routes = [\n{ path: &apos;home&apos;, component: HomeComponent },\n{ path: &apos;admin&apos;, loadChildren: &apos;app/admin/admin.module#AdminModule&apos;, canLoad: [ExampleCanLoadGuard] }\n];\n16.14. Testing\nThis guide will cover the basics of testing logic inside your code with UnitTests.\nThe guide assumes that you are familiar with Angular CLI (see the guide)\nFor testing your Angular application with UnitTests there are two main strategies:\nIsolated UnitTests\nIsolated unit tests examine an instance of a class all by itself without any dependence on Angular or any injected values.\nThe amount of code and effort needed to create such tests in minimal.\nAngular Testing Utilities\nLet you test components including their interaction with Angular.\nThe amount of code and effort needed to create such tests is a little higher.\n16.14.1. Testing Concept\nThe following figure shows you an overview of the application architecture devided in testing areas.\nFigure 67. Testing Areas\nThere are three areas, which need to be covered by different testing strategies.\nComponents:\nSmart Components need to be tested because they contain view logic.\nAlso the interaction with 3rd party components needs to be tested.\nWhen a 3rd party component changes with an upgrade a test will be failing and warn you, that there is something wrong with the new version.\nMost of the time Dumb Components do not need to be teste because they mainly display data and do not contain any logic.\nSmart Components are alway tested with Angular Testing Utilities.\nFor example selectors, which select data from the store and transform it further, need to be tested.\nStores:\nA store contains methods representing state transitions.\nIf these methods contain logic, they need to be tested.\nStores are always testet using Isolated UnitTests.\nServices:\nServices contain Business Logic, which needs to be tested.\nUseCase Services represent a whole business use case.\nFor instance this could be initializing a store with all the data that is needed for a dialog - loading, transforming, storing.\nOften Angular Testing Utilities are the optimal solution for testing UseCase Services, because they allow for an easy stubbing of the backend.\nAll other services should be tested with Isolated UnitTests as they are much easier to write and maintain.\n16.14.2. Testing Smart Components\nTesting Smart Components should assure the following.\nBindings are correct.\nSelectors which load data from the store are correct.\nAsynchronous behavior is correct (loading state, error state, &quot;normal&quot; state).\nOftentimes through testing one realizes, that important edge cases are forgotten.\nDo these test become very complex, it is often an indicator for poor code quality in the component.\nThen the implementation is to be adjusted / refactored.\nWhen testing values received from the native DOM, you will test also that 3rd party libraries did not change with a version upgrade.\nA failing test will show you what part of a 3rd party library has changed.\nThis is much better than the users doing this for you.\nFor example a binding might fail because the property name was changed with a newer version of a 3rd party library.\nIn the function beforeEach() the TestBed imported from Angular Testing Utilities needs to be initialized.\nThe goal should be to define a minimal test-module with TestBed.\nThe following code gives you an example.\nListing 58. Example test setup for Smart Components\ndescribe(&apos;PrintFlightComponent&apos;, () =&gt; {\nlet fixture: ComponentFixture&lt;PrintCPrintFlightComponentomponent&gt;;\nlet store: FlightStore;\nlet printServiceSpy: jasmine.SpyObj&lt;FlightPrintService&gt;;\nbeforeEach(() =&gt; {\nconst urlParam = &apos;1337&apos;;\nconst activatedRouteStub = { params: of({ id: urlParam }) };\nprintServiceSpy = jasmine.createSpyObj(&apos;FlightPrintService&apos;, [&apos;initializePrintDialog&apos;]);\nTestBed.configureTestingModule({\nimports: [\nTranslateModule.forRoot(),\nRouterTestingModule\n],\ndeclarations: [\nPrintFlightComponent,\nPrintContentComponent,\nGeneralInformationPrintPanelComponent,\nPassengersPrintPanelComponent\n],\nproviders: [\nFlightStore,\n{provide: FlightPrintService, useValue: printServiceSpy},\n{provide: ActivatedRoute, useValue: activatedRouteStub}\n]\n});\nfixture = TestBed.createComponent(PrintFlightComponent);\nstore = fixture.debugElement.injector.get(FlightStore);\nfixture.detectChanges();\n});\n// ... test cases\n})\nIt is important:\nUse RouterTestingModule` instead of RouterModule\nUse TranslateModule.forRoot() without translations\nThis way you can test language-neutral without translation marks.\nDo not add a whole module from your application - in declarations add the tested Smart Component with all its Dumb Components\nThe store should never be stubbed.\nIf you need a complex test setup, just use the regular methods defined on the store.\nStub all services used by the Smart Component.\nThese are mostly UseCase services.\nThey should not be tested by these tests.\nOnly the correct call to their functions should be assured.\nThe logic inside the UseCase services is tested with seperate tests.\ndetectChanges() performance an Angular Change Detection cycle (Angular refreshes all the bindings present in the view)\ntick() performance a virtual marco task, tick(1000) is equal to the virtual passing of 1s.\nThe following test cases show the testing strategy in action.\nListing 59. Example\nit(&apos;calls initializePrintDialog for url parameter 1337&apos;, fakeAsync(() =&gt; {\nexpect(printServiceSpy.initializePrintDialog).toHaveBeenCalledWith(1337);\n}));\nit(&apos;creates correct loading subtitle&apos;, fakeAsync(() =&gt; {\nstore.setPrintStateLoading(123);\ntick();\nfixture.detectChanges();\nconst subtitle = fixture.debugElement.query(By.css(&apos;app-header-element .print-header-container span:last-child&apos;));\nexpect(subtitle.nativeElement.textContent).toBe(&apos;PRINT_HEADER.FLIGHT STATE.IS_LOADING&apos;);\n}));\nit(&apos;creates correct subtitle for loaded flight&apos;, fakeAsync(() =&gt; {\nstore.setPrintStateLoadedSuccess({\nid: 123,\ndescription: &apos;Description&apos;,\niata: &apos;FRA&apos;,\nname: &apos;Frankfurt&apos;,\n// ...\n});\ntick();\nfixture.detectChanges();\nconst subtitle = fixture.debugElement.query(By.css(&apos;app-header-element .print-header-container span:last-child&apos;));\nexpect(subtitle.nativeElement.textContent).toBe(&apos;PRINT_HEADER.FLIGHT &quot;FRA (Frankfurt)&quot; (ID: 123)&apos;);\n}));\nThe examples show the basic testing method\nSet the store to a well-defined state\ncheck if the component displays the correct values\n&#x2026;&#x200B; via checking values inside the native DOM.\n16.14.3. Testing state transitions performed by stores\nStores are always tested with Isolated UnitTests.\nActions triggered by dispatchAction() calls are asynchronously performed to alter the state.\nA good solution to test such a state transition is to use the done callback from Jasmine.\nListing 60. Example for testing a store\nlet sut: FlightStore;\nbeforeEach(() =&gt; {\nsut = new FlightStore();\n});\nit(&apos;setPrintStateLoading sets print state to loading&apos;, (done: Function) =&gt; {\nsut.setPrintStateLoading(4711);\nsut.state$.pipe(first()).subscribe(result =&gt; {\nexpect(result.print.isLoading).toBe(true);\nexpect(result.print.loadingId).toBe(4711);\ndone();\n});\n});\nit(&apos;toggleRowChecked adds flight with given id to selectedValues Property&apos;, (done: Function) =&gt; {\nconst flight: FlightTO = {\nid: 12\n// dummy data\n};\nsut.setRegisterabgleichListe([flight]);\nsut.toggleRowChecked(12);\nsut.state$.pipe(first()).subscribe(result =&gt; {\nexpect(result.selectedValues).toContain(flight);\ndone();\n});\n});\n16.14.4. Testing services\nWhen testing services both strategies - Isolated UnitTests and Angular Testing Utilities - are valid options.\nThe goal of such tests are\nassuring the behavior for valid data.\nassuring the behavior for invalid data.\ndocumenting functionality\nsavely performing refactorings\nthinking about edge case behavior while testing\nFor simple services Isolated UnitTests can be written.\nWriting these tests takes lesser effort and they can be written very fast.\nThe following listing gives an example of such tests.\nListing 61. Testing a simple services with Isolated UnitTests\nlet sut: IsyDatePipe;\nbeforeEach(() =&gt; {\nsut = new IsyDatePipe();\n});\nit(&apos;transform should return empty string if input value is empty&apos;, () =&gt; {\nexpect(sut.transform(&apos;&apos;)).toBe(&apos;&apos;);\n});\nit(&apos;transform should return empty string if input value is null&apos;, () =&gt; {\nexpect(sut.transform(undefined)).toBe(&apos;&apos;);\n});\n// ...more tests\nFor testing Use Case services the Angular Testing Utilities should be used.\nThe following listing gives an example.\nListing 62. Test setup for testing use case services with Angular Testing Utilities\nlet sut: FlightPrintService;\nlet store: FlightStore;\nlet httpController: HttpTestingController;\nlet flightCalculationServiceStub: jasmine.SpyObj&lt;FlightCalculationService&gt;;\nconst flight: FlightTo = {\n// ... valid dummy data\n};\nbeforeEach(() =&gt; {\nflightCalculationServiceStub = jasmine.createSpyObj(&apos;FlightCalculationService&apos;, [&apos;getFlightType&apos;]);\nflightCalculationServiceStub.getFlightType.and.callFake((catalog: string, type: string, key: string) =&gt; of(`${key}_long`));\nTestBed.configureTestingModule({\nimports: [\nHttpClientTestingModule,\nRouterTestingModule,\n],\nproviders: [\nFlightPrintService,\nFlightStore,\nFlightAdapter,\n{provide: FlightCalculationService, useValue: flightCalculationServiceStub}\n]\n});\nsut = TestBed.get(FlightPrintService);\nstore = TestBed.get(FlightStore);\nhttpController = TestBed.get(HttpTestingController);\n});\nWhen using TestBed, it is important\nto import HttpClientTestingModule for stubbing the backend\nto import RouterTestingModule for stubbing the Angular router\nnot to stub stores, adapters and business services\nto stub services from libraries like FlightCalculationService - the correct implementation of libraries should not be tested by these tests.\nTesting backend communication looks like this:\nListing 63. Testing backend communication with Angular HttpTestingController\nit(&apos;loads flight if not present in store&apos;, fakeAsync(() =&gt; {\nsut.initializePrintDialog(1337);\nconst processRequest = httpController.expectOne(&apos;/path/to/flight&apos;);\nprocessRequest.flush(flight);\nhttpController.verify();\n}));\nit(&apos;does not load flight if present in store&apos;, fakeAsync(() =&gt; {\nconst flight = {...flight, id: 4711};\nstore.setRegisterabgleich(flight);\nsut.initializePrintDialog(4711);\nhttpController.expectNone(&apos;/path/to/flight&apos;);\nhttpController.verify();\n}));\nThe first test assures a correct XHR request is performed if initializePrintDialog() is called and no data is in the store.\nThe second test assures no XHR request ist performed if the needed data is already in the store.\nThe next steps are checks for the correct implementation of logic.\nListing 64. Example testing a Use Case service\nit(&apos;creates flight destination for valid key in svz&apos;, fakeAsync(() =&gt; {\nconst flightTo: FlightTo = {\n...flight,\nid: 4712,\nprofile: &apos;77&apos;\n};\nstore.setFlight(flightTo);\nlet result: FlightPrintContent|undefined;\nsut.initializePrintDialog(4712);\nstore.select(s =&gt; s.print.content).subscribe(content =&gt; result = content);\ntick();\nexpect(result!.destination).toBe(&apos;77_long (ID: 77)&apos;);\n}));\n16.15. Update Angular CLI\n16.15.1. Angular CLI common issues\nThere are constant updates for the official Angular framework dependencies. These dependencies are directly related with the Angular CLI package. Since this package comes installed by default inside the devonfw distribution folder for Windows OS and the distribution is updated every few months it needs to be updated in order to avoid known issues.\n16.15.2. Angular CLI update guide\nFor Linux users is as easy as updating the global package:\n$ npm unistall -g @angular/cli\n$ npm install -g @angular/cli\nFor Windows users the process is only a bit harder. Open the devonfw bundled console and do as follows:\n$ cd [devonfw_dist_folder]\n$ cd software/nodejs\n$ npm uninstall @angular/cli --no-save\n$ npm install @angular/cli --no-save\nAfter following these steps you should have the latest Angular CLI version installed in your system. In order to check it run in the distribution console:\nNote\nAt the time of this writing, the Angular CLI is at 1.7.4 version.\n&#x3BB; ng version\n_ _ ____ _ ___\n/ \\ _ __ __ _ _ _| | __ _ _ __ / ___| | |_ _|\n/ &#x25B3; \\ | &apos;_ \\ / _` | | | | |/ _` | &apos;__| | | | | | |\n/ ___ \\| | | | (_| | |_| | | (_| | | | |___| |___ | |\n/_/ \\_\\_| |_|\\__, |\\__,_|_|\\__,_|_| \\____|_____|___|\n|___/\nAngular CLI: 7.2.3\nNode: 10.13.0\nOS: win32 x64\nAngular:\n...\n16.16. Working with Angular CLI\nAngular CLI provides a facade for building, testing, linting, debugging and generating code.\nUnder the hood Angular CLI uses specific tools to achieve these tasks.\nThe user does no need to maintain them and can rely on Angular to keep them up to date and maybe switch to other tools which come up in the future.\nThe Angular CLI provides a wiki with common tasks you encounter when working on applications with the Angular CLI.\nThe Angular CLI Wiki can be found here.\nIn this guide we will go through the most important tasks.\nTo go into more details, please visit the Angular CLI wiki.\n16.16.1. Installing Angular CLI\nAngular CLI should be added as global and local dependency.\nThe following commands add Angular CLI as global Dependency.\nyarn command\nyarn global add @angular/cli\nnpm command\nnpm install -g @angular/cli\nYou can check a successful installtion with ng --version.\nThis should print out the version installed.\nFigure 68. Printing Angular CLI Version\n16.16.2. Running a live development server\nThe Angular CLI can be used to start a live development server.\nFirst your application will be compiled and then the server will be started.\nIf you change the code of a file, the server will reload the displayed page.\nRun your application with the following command:\nng serve -o\n16.16.3. Running Unit Tests\nAll unit tests can be executed with the command:\nng test\nTo make a single run and create a code coverage file use the following command:\nng test -sr -cc\nTip\nYou can configure the output format for code coverage files to match your requirements in the file karma.conf.js which can be found on toplevel of your project folder.\nFor instance, this can be useful for exporting the results to a SonarQube.\n16.16.4. Linting the code quality\nYou can lint your files with the command\nng lint --type-check\nTip\nYou can adjust the linting rules in the file tslint.json which can be found on toplevel of your project folder.\n16.16.5. Generating Code\nCreating a new Angular CLI project\nFor creating a new Angular CLI project the command ng new is used.\nThe following command creates a new application named my-app.\nng create my-app\nCreating a new feature module\nA new feature module can be created via ng generate module` command.\nThe following command generates a new feature module named todo.\nng generate module todo\nFigure 69. Generate a module with Angular CLI\nTip\nThe created feature module needs to be added to the AppModule by hand.\nOther option would be to define a lazy route in AppRoutingModule to make this a lazy loaded module.\nCreating a new component\nTo create components the command ng generate component can be used.\nThe following command will generate the component todo-details inside the components layer of todo module.\nIt will generate a class, a html file, a css file and a test file.\nAlso, it will register this component as declaration inside the nearest module - this ist TodoModule.\nng generate component todo/components/todo-details\nFigure 70. Generate a component with Angular CLI\nTip\nIf you want to export the component, you have to add the component to exports array of the module.\nThis would be the case if you generate a component inside shared module.\n16.16.6. Configuring an Angular CLI project\nInside an Angular CLI project the file .angular-cli.json can be used to configure the Angular CLI.\nThe following options are very important to understand.\nThe property defaults` can be used to change the default style extension.\nThe following settings will make the Angular CLI generate .less files, when a new component is generated.\n&quot;defaults&quot;: {\n&quot;styleExt&quot;: &quot;less&quot;,\n&quot;component&quot;: {}\n}\nThe property apps contains all applications maintained with Angular CLI.\nMost of the time you will have only one.\nassets configures all the static files, that the application needs - this can be images, fonts, json files, etc.\nWhen you add them to assets the Angular CLI will put these files to the build target and serve them while debugging.\nThe following will put all files in /i18n to the output folder /i18n\n&quot;assets&quot;: [\n{ &quot;glob&quot;: &quot;**/*.json&quot;, &quot;input&quot;: &quot;./i18n&quot;, &quot;output&quot;: &quot;./i18n&quot; }\n]\nstyles property contains all style files that will be globally available.\nThe Angular CLI will create a styles bundle that goes directly into index.html with it.\nThe following will make all styles in styles.less globally available.\n&quot;styles&quot;: [\n&quot;styles.less&quot;\n]\nenvironmentSource and environments are used to configure configuration with the Angular CLI.\nInside the code always the file specified in environmentSource will be referenced.\nYou can define different environments - eg. production, staging, etc. - which you list in enviroments.\nAt compile time the Angular CLI will override all values in environmentSource with the values from the matching environment target.\nThe following code will build the application for the environment staging.\nng build --environment=staging\n&#x2190;&#xA0;Previous:&#xA0;Guides&#xA0;| &#x2191;&#xA0;Up:&#xA0;devon4ng&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Ionic&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devon4ng.asciidoc_architecture.html","title":"13. Architecture","body":"\n13. Architecture\n13.1. Architecture\nThe following principles and guidelines are based on Angular Styleguide - especially Angular modules (see Angular Docs).\nIt extends those where additional guidance is needed to define an architecture which is\nmaintainable across applications and teams\neasy to understand, especially when coming from a classic Java/.Net perspective - so whenever possible the same principles apply both to the server and the client\npattern based to solve common problems\nbased on best of breed solutions coming from open source and Capgemini project experiences\ngives as much guidance as necessary and as little as possible\n13.1.1. Overview\nWhen using Angular the web client architecture is driven by the framework in a certain way Google and the Angular community think about web client architecture.\nAngular gives an opinion on how to look at architecture.\nIt is component based like devon4j but uses different terms which are common language in web application development.\nThe important term is module which is used instead of component. The primary reason is the naming collision with the Web Components standard (see Web Components).\nTo clarify this:\nA component describes an UI element containing HTML, CSS and JavaScript - structure, design and logic encapsulated inside a reusable container called component.\nA module describes an applications feature area. The application flight-app may have a module called booking.\nAn application developed using Angular consists of multiple modules.\nThere are feature modules and special modules described by the Angular Styleguide - core and shared.\nAngular or Angular Styleguide give no guidance on how to structure a module internally.\nThis is where this architecture comes in.\nLayers\nThe architecture describes two layers. The terminology is based on common language in web development.\nFigure 27. Layers\nComponents Layer encapsulates components which present the current application state.\nComponents are separated into Smart and Dumb Components.\nThe only logic present is view logic inside Smart Components.\nServices Layer is more or less what we call &apos;business logic layer&apos; on the server side.\nThe layer defines the applications state, the transitions between state and classic business logic.\nStores contain application state over time to which Smart Components subscribe to.\nAdapters are used to perform XHRs, WebSocket connections, etc.\nThe business model is described inside the module.\nUse case services perform business logic needed for use cases.\nA use case services interacts with the store and adapters.\nMethods of use case services are the API for Smart Components.\nThose methods are Actions in reactive terminology.\nModules\nAngular requires a module called app which is the main entrance to an application at runtime - this module gets bootstrapped.\nAngular Styleguide defines feature modules and two special modules - core and shared.\nFigure 28. Modules\nA feature module is basically a vertical cut through both layers.\nThe shared module consists of components shared across feature modules.\nThe core module holds services shared across modules.\nSo core module is a module only having a services layer\nand shared module is a module only having a components layer.\n13.2. Meta Architecture\n13.2.1. Introduction\nPurpose of this document\nIn our business applications, the client easily gets underestimated. Sometimes the client is more complex to develop and design than the server. While the server architecture is nowadays easily to agree as common sense, for clients this is not as obvious and stable especially as it typically depends on the client framework used. Finding a concrete architecture applicable for all clients may therefore be difficult to accomplish.\nThis document tries to define on a high abstract level, a reference architecture which is supposed to be a mental image and frame for orientation regarding the evaluation and appliance of different client frameworks. As such it defines terms and concepts required to be provided for in any framework and thus gives a common ground of understanding for those acquainted with the reference architecture. This allows better comparison between the various frameworks out there, each having their own terms for essentially the same concepts. It also means that for each framework we need to explicitly map how it implements the concepts defined in this document.\nThe architecture proposed herein is neither new nor was it developed from scratch. Instead it is the gathered and consolidated knowledge and best practices of various projects (s. References).\nGoal of the Client Architecture\nThe goal of the client architecture is to support the non-functional requirements for the client, i.e. mostly maintainability, scalability, efficiency and portability. As such it provides a component-oriented architecture following the same principles listed already in the devonfw architecture overview. Furthermore it ensures a homogeneity regarding how different concrete UI technologies are being applied in the projects, solving the common requirements in the same way.\nArchitecture Views\nAs for the server we distinguish between the business and the technical architecture. Where the business architecture is different from project to project and relates to the concrete design of dialog components given concrete requirements, the technical architecture can be applied to multiple projects.\nThe focus of this document is to provide a technical reference architecture on the client on a very abstract level defining required layers and components. How the architecture is implemented has to be defined for each UI technology.\nThe technical infrastructure architecture is out of scope for this document and although it needs to be considered, the concepts of the reference architecture should work across multiple TI architecture, i.e. native or web clients.\n13.2.2. devonfw Reference Client Architecture\nThe following gives a complete overview of the proposed reference architecture. It will be built up incrementally in the following sections.\nFigure 1 Overview\nClient Architecture\nOn the highest level of abstraction we see the need to differentiate between dialog components and their container they are managed in, as well as the access to the application server being the backend for the client (e.g. an devon4j instance). This section gives a summary of these components and how they relate to each other. Detailed architectures for each component will be supplied in subsequent sections\nFigure 2 Overview of Client Architecture\nDialog Component\nA dialog component is a logical, self-contained part of the user interface. It accepts user input and actions and controls communication with the user. Dialog components use the services provided by the dialog container in order to execute the business logic. They are self-contained, i.e. they possess their own user interface together with the associated logic, data and states.\nDialog components can be composed of other dialog components forming a hierarchy\nDialog components can interact with each other. This includes communication of a parent to its children, but also between components independent of each other regarding the hierarchy.\nDialog Container\nDialog components need to be managed in their lifecycle and how they can be coupled to each other. The dialog container is responsible for this along with the following:\nBootstrapping the client application and environment\nConfiguration of the client\nInitialization of the application server access component\nDialog Component Management\nControlling the lifecycle\nControlling the dialog flow\nProviding means of interaction between the dialogs\nProviding application server access\nProviding services to the dialog components\n(e.g. printing, caching, data storage)\nShutdown of the application\nApplication Server Access\nDialogs will require a backend application server in order to execute their business logic. Typically in an devonfw application the service layer will provide interfaces for the functionality exposed to the client. These business oriented interfaces should also be present on the client backed by a proxy handling the concrete call of the server over the network. This component provides the set of interfaces as well as the proxy.\nDialog Container Architecture\nThe dialog container can be further structured into the following components with their respective tasks described in own sections:\nFigure 3 Dialog Container Architecture\nApplication\nThe application component represents the overall client in our architecture. It is responsible for bootstrapping all other components and connecting them with each other. As such it initializes the components below and provides an environment for them to work in.\nConfiguration Management\nThe configuration management manages the configuration of the client, so the client can be deployed in different environments. This includes configuration of the concrete application server to be called or any other environment-specific property.\nDialog Management\nThe Dialog Management component provides the means to define, create and destroy dialog components. It therefore offers basic lifecycle capabilities for a component. In addition it also allows composition of dialog components in a hierarchy. The lifecycle is then managed along the hierarchy, meaning when creating/destroying a parent dialog, this affects all child components, which are created/destroyed as well.\nService Registry\nApart from dialog components, a client application also consists of services offered to these. A service can thereby encompass among others:\nAccess to the application server\nAccess to the dialog container functions for managing dialogs or accessing the configuration\nDialog independent client functionality such as Printing, Caching, Logging, Encapsulated business logic such as tax calculation\nDialog component interaction\nThe service registry offers the possibility to define, register and lookup these services. Note that these services could be dependent on the dialog hierarchy, meaning different child instances could obtain different instances / implementations of a service via the service registry, depending on which service implementations are registered by the parents.\nServices should be defined as interfaces allowing for different implementations and thus loose coupling.\nDialog Component Architecture\nA dialog component has to support all or a subset of the following tasks:\n(T1)\tDisplaying the user interface incl. internationalization\n(T2)\tDisplaying business data incl. changes made to the data due to user interactions and localization of the data\n(T3)\tAccepting user input including possible conversion from e.g. entered Text to an Integer\n(T4)\tDisplaying the dialog state\n(T5)\tValidation of user input\n(T6)\tManaging the business data incl. business logic altering it due to user interactions\n(T7)\tExecution of user interactions\n(T8)\tManaging the state of the dialog (e.g. Edit vs. View)\n(T9)\tCalling the application server in the course of user interactions\nFollowing the principle of separation of concerns, we further structure a dialog component in an own architecture allowing us the distribute responsibility for these tasks along the defined components:\nFigure 4 Overview of dialog component architecture\nPresentation Layer\nThe presentation layer generates and displays the user interface, accepts user input and user actions and binds these to the dialog core layer (T1-5). The tasks of the presentation layer fall into two categories:\nProvision of the visual representation (View component)\nThe presentation layer generates and displays the user interface and accepts user input and user actions. The logical processing of the data, actions and states is performed in the dialog core layer. The data and user interface are displayed in localized and internationalized form.\nBinding of the visual representation to the dialog core layer\nThe presentation layer itself does not contain any dialog logic. The data or actions entered by the user are then processed in the dialog core layer. There are three aspects to the binding to the dialog core layer. We refer to &#x201C;data binding&#x201D;, &#x201C;state binding&#x201D; and &#x201C;action binding&#x201D;. Syntactical and (to a certain extent) semantic validations are performed during data binding (e.g. cross-field plausibility checks). Furthermore, the formatted, localized data in the presentation layer is converted into the presentation-independent, neutral data in the dialog core layer (parsing) and vice versa (formatting).\nDialog Core Layer\nThe dialog core layer contains the business logic, the control logic, and the logical state of the dialog. It therefore covers tasks T5-9:\nMaintenance of the logical dialog state and the logical data\nThe dialog core layer maintains the logical dialog state and the logical data in a form which is independent of the presentation. The states of the presentation (e.g. individual widgets) must not be maintained in the dialog core layer, e.g. the view state could lead to multiple presentation states disabling all editable widgets on the view.\nImplementation of the dialog and dialog control logic\nThe component parts in the dialog core layer implement the client specific business logic and the dialog control logic. This includes, for example, the manipulation of dialog data and dialog states as well as the opening and closing of dialogs.\nCommunication with the application server\nThe dialog core layer calls the interfaces of the application server via the application server access component services.\nThe dialog core layer should not depend on the presentation layer enforcing a strict layering and thus minimizing dependencies.\nInteractions between dialog components\nDialog components can interact in the following ways:\nEmbedding of dialog components\nAs already said dialog components can be hierarchically composed. This composition works by embedding on dialog component within the other. Apart from the lifecycle managed by the dialog container, the embedding needs to cope for the visual embedding of the presentation and core layer.\nEmbedding dialog presentation\nThe parent dialog needs to either integrate the embedded dialog in its layout or open it in an own model window.\nEmbedding dialog core\nThe parent dialog needs to be able to access the embedded instance of its children. This allows initializing and changing their data and states. On the other hand the children might require context information offered by the parent dialog by registering services in the hierarchical service registry.\nDialog flow\nApart from the embedding of dialog components representing a tight coupling, dialogs can interact with each other by passing the control of the UI, i.e. switching from one dialog to another.\nWhen interacting, dialog components should interact only between the same or lower layers, i.e. the dialog core should not access the presentation layer of another dialog component.\n13.2.3. Appendix\nNotes about Quasar Client\nThe Quasar client architecture as the consolidated knowledge of our CSD projects is the major source for the above drafted architecture. However, the above is a much simplified and more agile version thereof:\nQuasar Client tried to abstract from the concrete UI library being used, so it could decouple the business from the technical logic of a dialog. The presentation layer should be the only one knowing the concrete UI framework used. This level of abstraction was dropped in this reference architecture, although it might of course still make sense in some projects. For fast-moving agile projects in the web however introducing such a level of abstraction takes effort with little gained benefits. With frameworks like Angular 2 we would even introduce one additional seemingly artificial and redundant layer, since it already separates the dialog core from its presentation.\nIn the past and in the days of Struts, JSF, etc. the concept of session handling was important for the client since part of the client was sitting on a server with a session relating it to its remote counterpart on the users PC. Quasar Client catered for this need, by very prominently differentiating between session and application in the root of the dialog component hierarchy. However, in the current days of SPA applications and the lowered importance of servers-side web clients, this prominent differentiation was dropped. When still needed the referenced documents will provide in more detail how to tailor the respective architecture to this end.\n13.2.4. References\nArchitecture Guidelines for Application Design:\nhttps://troom.capgemini.com/sites/vcc/engineering/Cross%20Cutting/ArchitectureGuide/Architecture_Guidelines_for_Application_Design_v2.0.docx\nQuasar Client Architekturen:\nhttps://troom.capgemini.com/sites/vcc/Shared%20Documents/CrossCuttingContent/TopicOrientedCCC/QuasarOverview/NCE%20Quasar%20Review%20Workshop%202009-11-17/Quasar%20Development/Quasar-Client-Architectures.doc\n&#x2191;&#xA0;Up:&#xA0;devon4ng&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Layers&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devon4ng.asciidoc_cookbook.html","title":"20. Cookbook","body":"\n20. Cookbook\n20.1. Abstract Class Store\nThe following solution presents a base class for implementing stores which handle state and its transitions.\nWorking with the base class achieves:\ncommon API across all stores\nlogging (when activated in the constructor)\nstate transitions are asynchronous by design - sequential order problems are avoided\nListing 84. Usage Example\n@Injectable()\nexport class ModalStore extends Store&lt;ModalState&gt; {\nconstructor() {\nsuper({ isOpen: false }, !environment.production);\n}\ncloseDialog() {\nthis.dispatchAction(&apos;Close Dialog&apos;, (currentState) =&gt; ({...currentState, isOpen: false}));\n}\nopenDialog() {\nthis.dispatchAction(&apos;Open Dialog&apos;, (currentState) =&gt; ({...currentState, isOpen: true}));\n}\n}\nListing 85. Abstract Base Class Store\nimport { OnDestroy } from &apos;@angular/core&apos;;\nimport { BehaviorSubject } from &apos;rxjs/BehaviorSubject&apos;;\nimport { Observable } from &apos;rxjs/Observable&apos;;\nimport { intersection, difference } from &apos;lodash&apos;;\nimport { map, distinctUntilChanged, observeOn } from &apos;rxjs/operators&apos;;\nimport { Subject } from &apos;rxjs/Subject&apos;;\nimport { queue } from &apos;rxjs/scheduler/queue&apos;;\nimport { Subscription } from &apos;rxjs/Subscription&apos;;\ninterface Action&lt;T&gt; {\nname: string;\nactionFn: (state: T) =&gt; T;\n}\n/** Base class for implementing stores. */\nexport abstract class Store&lt;T&gt; implements OnDestroy {\nprivate actionSubscription: Subscription;\nprivate actionSource: Subject&lt;Action&lt;T&gt;&gt;;\nprivate stateSource: BehaviorSubject&lt;T&gt;;\nstate$: Observable&lt;T&gt;;\n/**\n* Initializes a store with initial state and logging.\n* @param initialState Initial state\n* @param logChanges When true state transitions are logged to the console.\n*/\nconstructor(initialState: T, public logChanges = false) {\nthis.stateSource = new BehaviorSubject&lt;T&gt;(initialState);\nthis.state$ = this.stateSource.asObservable();\nthis.actionSource = new Subject&lt;Action&lt;T&gt;&gt;();\nthis.actionSubscription = this.actionSource.pipe(observeOn(queue)).subscribe(action =&gt; {\nconst currentState = this.stateSource.getValue();\nconst nextState = action.actionFn(currentState);\nif (this.logChanges) {\nthis.log(action.name, currentState, nextState);\n}\nthis.stateSource.next(nextState);\n});\n}\n/**\n* Selects a property from the stores state.\n* Will do distinctUntilChanged() and map() with the given selector.\n* @param selector Selector function which selects the needed property from the state.\n* @returns Observable of return type from selector function.\n*/\nselect&lt;TX&gt;(selector: (state: T) =&gt; TX): Observable&lt;TX&gt; {\nreturn this.state$.pipe(\nmap(selector),\ndistinctUntilChanged()\n);\n}\nprotected dispatchAction(name: string, action: (state: T) =&gt; T) {\nthis.actionSource.next({ name, actionFn: action });\n}\nprivate log(actionName: string, before: T, after: T) {\nconst result: { [key: string]: { from: any, to: any} } = {};\nconst sameProbs = intersection(Object.keys(after), Object.keys(before));\nconst newProbs = difference(Object.keys(after), Object.keys(before));\nfor (const prop of newProbs) {\nresult[prop] = { from: undefined, to: (&lt;any&gt;after)[prop] };\n}\nfor (const prop of sameProbs) {\nif ((&lt;any&gt;before)[prop] !== (&lt;any&gt;after)[prop]) {\nresult[prop] = { from: (&lt;any&gt;before)[prop], to: (&lt;any&gt;after)[prop] };\n}\n}\nconsole.log(this.constructor.name, actionName, result);\n}\nngOnDestroy() {\nthis.actionSubscription.unsubscribe();\n}\n}\n20.2. Angular Electron\n20.2.1. Add Electron to an Angular application\nThis cookbook recipe explains how to integrate Electron in an Angular 6+ application. Electron is a framework for creating native applications with web technologies like JavaScript, HTML, and CSS. As an example, very well known applications as Visual Studio Code, Atom, Slack or Skype (and many more) are using Electron too.\nNote\nAt the moment of this writing Angular 7.2.3 and Electron 4.0.2 were the versions available.\nHere are the steps to achieve this goal. Follow them in order.\nAdd Electron and other relevant dependencies\nThere are two different approaches to add the dependencies in the package.json file:\nWriting the dependencies directly in that file.\nInstalling using npm install or yarn add.\nImportant\nPlease remember if the project has a package-lock.json or yarn.lock file use npm or yarn respectively.\nIn order to add the dependencies directly in the package.json file, include the following lines in the devDependencies section:\n&quot;devDependencies&quot;: {\n...\n&quot;electron&quot;: &quot;^4.0.2&quot;,\n&quot;electron-builder&quot;: &quot;^20.38.5&quot;,\n&quot;electron-reload&quot;: &quot;^1.4.0&quot;,\n&quot;npm-run-all&quot;: &quot;^4.1.5&quot;,\n&quot;npx&quot;: &quot;^10.2.0&quot;,\n&quot;wait-on&quot;: &quot;^3.2.0&quot;,\n&quot;webdriver-manager&quot;: &quot;^12.1.1&quot;\n...\n},\nAs indicated above, instead of this npm install can be used:\n$ npm install -D electron electron-builder electron-reload npm-run-all npx wait-on webdriver-manager\nOr with yarn:\n$ yarn add -D electron electron-builder electron-reload npm-run-all npx wait-on webdriver-manager\nAdd Electron build configuration\nIn order to configure electron builds properly a electron-builder.json must be included in the root folder of the application. For more information and fine tuning please refer to the Electron Builder official documentation.\nThe contents of the file will be something similar to the following:\n{\n&quot;productName&quot;: &quot;app-name&quot;,\n&quot;directories&quot;: {\n&quot;output&quot;: &quot;release/&quot;\n},\n&quot;files&quot;: [\n&quot;**/*&quot;,\n&quot;!**/*.ts&quot;,\n&quot;!*.code-workspace&quot;,\n&quot;!LICENSE.md&quot;,\n&quot;!package.json&quot;,\n&quot;!package-lock.json&quot;,\n&quot;!src/&quot;,\n&quot;!e2e/&quot;,\n&quot;!hooks/&quot;,\n&quot;!angular.json&quot;,\n&quot;!_config.yml&quot;,\n&quot;!karma.conf.js&quot;,\n&quot;!tsconfig.json&quot;,\n&quot;!tslint.json&quot;\n],\n&quot;win&quot;: {\n&quot;icon&quot;: &quot;dist/assets/icons&quot;,\n&quot;target&quot;: [&quot;portable&quot;]\n},\n&quot;mac&quot;: {\n&quot;icon&quot;: &quot;dist/assets/icons&quot;,\n&quot;target&quot;: [&quot;dmg&quot;]\n},\n&quot;linux&quot;: {\n&quot;icon&quot;: &quot;dist/assets/icons&quot;,\n&quot;target&quot;: [&quot;AppImage&quot;]\n}\n}\nTheres two important things in this file:\n&quot;output&quot;: this is where electron builder is going to build our application\n&quot;icon&quot;: in every OS possible theres an icon parameter, the route to the icon folder that will be created after building with angular needs to be used here. This will make it so the electron builder can find the icons and build.\nCreate the necessary typescript configurations\nIn order to initiate electron in an angular app we need to modify the tsconfig.json file and create a new one named tsconfig-serve.json in the root folder.\ntsconfig.json\nThis file needs to be modified to add the main.ts and src/**/* folders excluding the node_modules:\n{\n....\n},\n&quot;include&quot;: [\n&quot;main.ts&quot;,\n&quot;src/**/*&quot;\n],\n&quot;exclude&quot;: [\n&quot;node_modules&quot;\n]\n....\n}\ntsconfig-serve.json\nIn the root, tsconfig-serve.json needs to be created. This typescript config file is going to be used when we serve electron:\n{\n&quot;compilerOptions&quot;: {\n&quot;sourceMap&quot;: true,\n&quot;declaration&quot;: false,\n&quot;moduleResolution&quot;: &quot;node&quot;,\n&quot;emitDecoratorMetadata&quot;: true,\n&quot;experimentalDecorators&quot;: true,\n&quot;target&quot;: &quot;es5&quot;,\n&quot;typeRoots&quot;: [\n&quot;node_modules/@types&quot;\n],\n&quot;lib&quot;: [\n&quot;es2017&quot;,\n&quot;es2016&quot;,\n&quot;es2015&quot;,\n&quot;dom&quot;\n]\n},\n&quot;include&quot;: [\n&quot;main.ts&quot;\n],\n&quot;exclude&quot;: [\n&quot;node_modules&quot;,\n&quot;**/*.spec.ts&quot;\n]\n}\nModify angular.json\nangular.json has to to be modified so the project is build inside /dist without an intermediate folder.\n{\n....\n&quot;architect&quot;: {\n....\n&quot;build&quot;: {\noutputPath&quot;: &quot;dist&quot;,\n....\n}\nAdd Angular Electron directives\nIn order to use Electron&#x2019;s webview tag and its methods inside an Angular application our project needs the directive webview.directive.ts file. We recommend to create this file inside a shared module folder, although it has to be declared inside the main module app.module.ts.\nListing 86. File webview.directive.ts\nimport { Directive } from &apos;@angular/core&apos;;\n@Directive({\nselector: &apos;[webview]&apos;,\n})\nexport class WebviewDirective {}\nAdd access Electron APIs\nTo call Electron APIs from the Renderer process, install ngx-electron module.\nWith npm:\n$ npm install ngx-electron --save\nOr with yarn:\n$ yarn add ngx-electron --save\nThis package contains a module named NgxElectronModule which exposes Electron APIs through a service called ElectronService\nUpdate app.module.ts and app-routing.module.ts\nAs an example, the webview.directive.ts file is located inside a shared module:\nListing 87. File app.module.ts\n// imports\nimport { NgxElectronModule } from &apos;ngx-electron&apos;;\nimport { WebviewDirective } from &apos;./shared/directives/webview.directive&apos;;\n@NgModule({\ndeclarations: [AppComponent, WebviewDirective],\nimports: [\n...\nNgxElectronModule\n...\n],\nproviders: [],\nbootstrap: [AppComponent],\n})\nexport class AppModule {}\nHere NgxElectronModule is also added so ElectronService can be injected wherever is needed.\nAfter that is done, the use of hash has to be allowed so electron can reload content properly. On the app-routing.module.ts:\n....\nimports: [RouterModule.forRoot(routes,\n{\n....\nuseHash: true,\n},\n)],\nUsage\nIn order to use Electron in any component class the ElectronService must be injected:\nimport { ElectronService } from &apos;ngx-electron&apos;;\n...\nconstructor(\n// other injected services\npublic electronService: ElectronService,\n) {\n// previous code...\nif (electronService.isElectronApp) {\n// Do electron stuff\n} else {\n// Do other web stuff\n}\n}\nTip\nA list of all accesible APIs can be found at Thorsten Hans&apos; ngx-electron repository.\nCreate the electron window in main.ts\nIn order to use electron, a file needs to be created at the root of the application (main.ts). This file will create a window with different settings checking if we are using --serve as an argument:\nimport { app, BrowserWindow, screen } from &apos;electron&apos;;\nimport * as path from &apos;path&apos;;\nimport * as url from &apos;url&apos;;\nlet win: any;\nlet serve: any;\nconst args: any = process.argv.slice(1);\nserve = args.some((val) =&gt; val === &apos;--serve&apos;);\nfunction createWindow(): void {\nconst electronScreen: any = screen;\nconst size: any = electronScreen.getPrimaryDisplay().workAreaSize;\n// Create the browser window.\nwin = new BrowserWindow({\nx: 0,\ny: 0,\nwidth: size.width,\nheight: size.height,\n// Needed if you are using service workers\nwebPreferences: {\nnodeIntegration: true,\nnodeIntegrationInWorker: true,\n}\n});\nif (serve) {\n// tslint:disable-next-line:no-require-imports\nrequire(&apos;electron-reload&apos;)(__dirname, {\nelectron: require(`${__dirname}/node_modules/electron`),\n});\nwin.loadURL(&apos;http://localhost:4200&apos;);\n} else {\nwin.loadURL(\nurl.format({\npathname: path.join(__dirname, &apos;dist/index.html&apos;),\nprotocol: &apos;file&apos;,\nslashes: true,\n}),\n);\n}\n// Uncoment the following line if you want to open the DevTools by default\n// win.webContents.openDevTools();\n// Emitted when the window is closed.\nwin.on(&apos;closed&apos;, () =&gt; {\n// Dereference the window object, usually you would store window\n// in an array if your app supports multi windows, this is the time\n// when you should delete the corresponding element.\n// tslint:disable-next-line:no-null-keyword\nwin = null;\n});\n}\ntry {\n// This method will be called when Electron has finished\n// initialization and is ready to create browser windows.\n// Some APIs can only be used after this event occurs.\napp.on(&apos;ready&apos;, createWindow);\n// Quit when all windows are closed.\napp.on(&apos;window-all-closed&apos;, () =&gt; {\n// On OS X it is common for applications and their menu bar\n// to stay active until the user quits explicitly with Cmd + Q\nif (process.platform !== &apos;darwin&apos;) {\napp.quit();\n}\n});\napp.on(&apos;activate&apos;, () =&gt; {\n// On OS X it&apos;s common to re-create a window in the app when the\n// dock icon is clicked and there are no other windows open.\nif (win === null) {\ncreateWindow();\n}\n});\n} catch (e) {\n// Catch Error\n// throw e;\n}\nAdd the electron window and improve the package.json scripts\nInside package.json the electron window that will be transformed to main.js when building needs to be added.\n{\n....\n&quot;main&quot;: &quot;main.js&quot;,\n&quot;scripts&quot;: {\n....\n}\nThe scripts section in the package.json can be improved to avoid running too verbose commands. As a very complete example we can take a look to the My Thai Star&#x2019;s scripts section and copy the lines useful in your project.\n&quot;scripts&quot;: {\n&quot;postinstall&quot;: &quot;npx electron-builder install-app-deps&quot;,\n&quot;.&quot;: &quot;sh .angular-gui/.runner.sh&quot;,\n&quot;ng&quot;: &quot;ng&quot;,\n&quot;start&quot;: &quot;ng serve --proxy-config proxy.conf.json -o&quot;,\n&quot;start:electron&quot;: &quot;npm-run-all -p serve electron:serve&quot;,\n&quot;compodoc&quot;: &quot;compodoc -p src/tsconfig.app.json -s&quot;,\n&quot;test&quot;: &quot;ng test --browsers Chrome&quot;,\n&quot;test:ci&quot;: &quot;ng test --browsers ChromeHeadless --watch=false&quot;,\n&quot;test:firefox&quot;: &quot;ng test --browsers Firefox&quot;,\n&quot;test:ci:firefox&quot;: &quot;ng test --browsers FirefoxHeadless --watch=false&quot;,\n&quot;test:firefox-dev&quot;: &quot;ng test --browsers FirefoxDeveloper&quot;,\n&quot;test:ci:firefox-dev&quot;: &quot;ng test --browsers FirefoxDeveloperHeadless --watch=false&quot;,\n&quot;test:electron&quot;: &quot;ng test&quot;,\n&quot;lint&quot;: &quot;ng lint&quot;,\n&quot;e2e&quot;: &quot;ng e2e&quot;,\n&quot;ngsw-config&quot;: &quot;npx ngsw-config dist ngsw-config.json&quot;,\n&quot;ngsw-copy&quot;: &quot;cp node_modules/@angular/service-worker/ngsw-worker.js dist/&quot;,\n&quot;serve&quot;: &quot;ng serve&quot;,\n&quot;serve:open&quot;: &quot;npm run start&quot;,\n&quot;serve:pwa&quot;: &quot;npm run build:pwa &amp;&amp; http-server dist -p 8080&quot;,\n&quot;serve:prod&quot;: &quot;ng serve --open --prod&quot;,\n&quot;serve:prodcompose&quot;: &quot;ng serve --open --configuration=prodcompose&quot;,\n&quot;serve:node&quot;: &quot;ng serve --open --configuration=node&quot;,\n&quot;build&quot;: &quot;ng build&quot;,\n&quot;build:pwa&quot;: &quot;ng build --configuration=pwa --prod --build-optimizer &amp;&amp; npm run ngsw-config &amp;&amp; npm run ngsw-copy&quot;,\n&quot;build:prod&quot;: &quot;ng build --prod --build-optimizer&quot;,\n&quot;build:prodcompose&quot;: &quot;ng build --configuration=prodcompose &quot;,\n&quot;build:electron&quot;: &quot;npm run electron:serve-tsc &amp;&amp; ng build --base-href ./&quot;,\n&quot;build:electron:dev&quot;: &quot;npm run build:electron -- -c dev&quot;,\n&quot;build:electron:prod&quot;: &quot;npm run build:electron -- -c production&quot;,\n&quot;electron:start&quot;: &quot;npm-run-all -p serve electron:serve&quot;,\n&quot;electron:serve-tsc&quot;: &quot;tsc -p tsconfig-serve.json&quot;,\n&quot;electron:serve&quot;: &quot;wait-on http-get://localhost:4200/ &amp;&amp; npm run electron:serve-tsc &amp;&amp; electron . --serve&quot;,\n&quot;electron:local&quot;: &quot;npm run build:electron:prod &amp;&amp; electron .&quot;,\n&quot;electron:linux&quot;: &quot;npm run build:electron:prod &amp;&amp; npx electron-builder build --linux&quot;,\n&quot;electron:windows&quot;: &quot;npm run build:electron:prod &amp;&amp; npx electron-builder build --windows&quot;,\n&quot;electron:mac&quot;: &quot;npm run build:electron:prod &amp;&amp; npx electron-builder build --mac&quot;\n},\nHere the important thing to look out for is that the base href when building electron can be changed as needed. In our case:\n&quot;build:electron&quot;: &quot;npm run postinstall:electron &amp;&amp; npm run electron:serve-tsc &amp;&amp; ng build --base-href \\&quot;\\&quot; &quot;,\nNote\nSome of these lines are intended to be shortcuts used in other scripts. Do not hesitate to modify them depending on your needs.\nSome usage examples:\n$ npm run electron:start # Serve Angular app and run it inside electron\n$ npm run electron:local # Serve Angular app for production and run it inside electron\n$ npm run electron:windows # Build Angular app for production and package it for Windows OS\n$ yarn run electron:start # Serve Angular app and run it inside electron\n$ yarn run electron:local # Serve Angular app for production and run it inside electron\n$ yarn run electron:windows # Build Angular app for production and package it for Windows OS\n20.3. Angular Mock Service\nWe&#x2019;ve all been there: A new idea comes, let&#x2019;s quickly prototype it. But wait, there&#x2019;s no backend. What can we do?\nBelow you will find o solution that will get your started quick and easy. The idea is to write a simple mock service that helps us by feeding data into our components.\n20.3.1. The app we start with\nLet&#x2019;s say you have a simple boilerplate code, with your favorite styling library hooked up and you&#x2019;re ready to go. The Angular Material sample is a good starting place.\n20.3.2. The Components\nComponents - are the building blocks of our application. Their main role is to enable fragments of user interfaces. They will either display data (a list, a table, a chart, etc.), or &apos;collect&apos; user interaction (e.g: a form, a menu, etc.)\nComponents stay at the forefront of the application. They should also be reusable (as much as possible). Reusability is key for what we are trying to achieve - a stable, maintainable frontend where multiple people can contribute and collaborate.\nIn our project, we are at the beginning. That means we may have more ideas than plans. We are exploring possibilites. In order to code eficiently:\n1) We will not store mock data in the components.\n2) We will not fetch or save data directly in the components.\nLearn more about Angular Components\n20.3.3. The Service\nSo, how do we get data in our app? How do we propagate the data to the components and how can we send user interaction from the components to the our data &quot;manager&quot; logic.\nThe answer to all these questions is an Angular Service (that we will just call a service from now on).\nA service is an injectable logic that can be consumed by all the components that need it. It can carry manipulation functions and ,in our case, fetch data from a provider.\nFigure 88. Angular Components &amp; Services architecture.\nInside the Angular App, an Injector gives access to each component to their required services. It&#x2019;s good coding practice to use a distinct service to each data type you want to manipulate. The type is described in a interface.\nStill, our ideas drive in diferent ways, so we have to stay flexible. We cannot use a database at the moment, but we want a way to represent data on screen, which can grow organically.\nLearn more about Angular Services\n20.3.4. The Model\nFigure 89. Data box in relation to services and components.\nLet&#x2019;s consider a &apos;box of data&apos; represented in JSON. Phisicly this means a folder with some JSON/TS files in it. They are located in the app/mock folder. The example uses only one mock data file. The file is typed according to our data model.\nPro tip: separate your files based on purpose. In your source code, put the mock files in the mock folder, components in the components folder, services in the services folder and data models in the models folder.\nFigure 90. Project structure.\nAligned with the Angular way of development, we are implementing a model-view-controler pattern.\nThe model is represented by the interfaces we make. These interfaces describe the data structures we will use in our application. In this example, there is one data model, coresponding with the &apos;type&apos; of data that was mocked. In the models folder you will find the .ts script file that describes chemical elements. The corresponding mock file defines a set is chemical emlements objects, in accordance to our interface definition.\n20.3.5. Use case\nEnough with the theory, let&#x2019;s see what we have here. The app presents 3 pages as follows:\nA leader bord with the top 3 elements\nA data table with all the elements\nA details page that reads a route paramenter and displays the details of the element.\nThere are a lot of business cases which have these requirements:\nA leader board can be understood as &quot;the most popular items in a set&quot;, &quot;the latest updated items&quot;, &quot;you favorite items&quot; etc.\nA data table with CRUD operations is very useful (in our case we only view details or delete an item, but they illustrate two important things: the details view shows how to navigate and consume a parametric route, the delete action shows how to invoke service operations over the loaded data - this means that the component is reusable and when the data comes with and API, only the service will need it&#x2019;s implementation changed)\nCheck out the Angular Mock Service sample from the samples folder and easily get started with fast data roundtrips between your mock data and your components.\n&#x2190;&#xA0;Previous:&#xA0;NgRx&#xA0;| &#x2191;&#xA0;Up:&#xA0;devon4ng&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;devon4net&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devon4ng.asciidoc_guides.html","title":"15. Guides","body":"\n15. Guides\n15.1. Package Managers\nThere are two major package managers currently used for JavaScript / TypeScript projects which leverage node.js as a build platform.\nnpm\nyarn\nOur recommendation is to use yarn but both package managers are fine.\nImportant\nWhen using npm it is important to use a version greater 5.0 as npm 3 has major drawbacks compared to yarn.\nThe following guide assumes that you are using npm &gt;= 5 or yarn.\nBefore you start reading further, please take a look at the docs:\nyarn getting started\nnpm getting started\nThe following guide will describe best practices for working with yarn / npm.\n15.1.1. Semantic Versioning\nWhen working with package managers it is very important to understand the concept of semantic versioning.\nTable 21. Version example 1.2.3\nVersion\n1.\n2.\n3\nVersion name when incrementing\nMajor (2.0.0)\nMinor (1.3.0)\nPatch (1.2.4)\nHas breaking changes\nyes\nno\nno\nHas features\nyes\nyes\nno\nHas bugfixes\nyes\nyes\nyes\nThe table gives an overview of the most important parts of semantic versioning.\nIn the header version 1.2.3 is displayed.\nThe first row shows the name and the resulting version when incrementing a part of the version.\nThe next rows show specifics of the resulting version - e.g. a major version can have breaking changes, features and bugfixes.\nPackages from npm and yarn leverage semantic versioning and instead of selecting a fixed version one can specify a selector.\nThe most common selectors are:\n^1.2.3\nAt least 1.2.3 - 1.2.4 or 1.3.0 can be used, 2.0.0 can not be used\n~1.2.3\nAt lease 1.2.3 - 1.2.4 can be used, 2.0.0 and 1.3.0 can not be used\n&gt;=1.2.3\nAt least 1.2.3 - every version greater can also be used\nThis achieves a lower number of duplicates.\nTo give an example:\nIf package A needs version 1.3.0 of package C and package B needs version 1.4.0 of package C one would end up with 4 packages.\nIf package A needs version ^1.3.0 of package C and package B needs version 1.4.0 of package C one would end up with 3 packages.\nA would use the same version of C as B - 1.4.0.\n15.1.2. Do not modify package.json and lock files by hand\nDependencies are always added using a yarn or npm command.\nAltering the package.json, package-json.lock or yarn.lock file by hand is not recommended.\nAlways use a yarn or npm command to add a new dependency.\nAdding the package express with yarn to dependencies.\nyarn add express\nAdding the package express with npm to dependencies.\nnpm install express\n15.1.3. What does the lock file do\nThe purpose of files yarn.lock and package-json.lock is to freeze versions for a short time.\nThe following problem is solved:\nDeveloper A upgrades the dependency express to fixed version 4.16.3.\nexpress has sub-dependency accepts with version selector ~1.3.5\nHis local node_modules folder receives accepts in version 1.3.5\nOn his machine everything is working fine\nAfterward version 1.3.6 of accepts is published - it contains a major bug\nDeveloper B now clones the repo and loads the dependencies.\nHe receives version 1.3.6 of accepts and blames developer A for upgrading to a broken version.\nBoth yarn.lock and package-json.lock freeze all the dependencies.\nFor example in yarn lock you will find.\nListing 12. yarn.lock example (excerp)\naccepts@~1.3.5:\nversion &quot;1.3.5&quot;\nresolved &quot;[...URL to registry]&quot;\ndependencies:\nmime-types &quot;~2.1.18&quot;\nnegotiator &quot;0.6.1&quot;\nmime-db@~1.33.0:\nversion &quot;1.33.0&quot;\nresolved &quot;[...URL to registry]&quot;\nmime-types@~2.1.18:\nversion &quot;2.1.18&quot;\nresolved &quot;[...URL to registry]&quot;\ndependencies:\nmime-db &quot;~1.33.0&quot;\nnegotiator@0.6.1:\nversion &quot;0.6.1&quot;\nresolved &quot;[...URL to registry]&quot;\nThe described problem is solved by the example yarn.lock file.\naccepts is frozen at version ~1.3.5\nAll of its sub-dependencies are also frozen.\nIt needs mime-types at version ~2.1.18 which is frozen at 2.1.18.\nmime-types needs mime-db at ~1.33.0 which is frozen at 1.33.0\nEvery developer will receive the same versions of every dependency.\nImportant\nYou have to make sure all your developers are using the same npm/yarn version - this includes the CI build.\n15.2. Package Managers Workflow\n15.2.1. Introduction\nThis document aims to provide you the necessary documentation and sources in order to help you understand the importance of dependencies between packages.\nProjects in node.js make use of modules, chunks of reusable code made by other people or teams. These small chunks of reusable code are called packages [1]. Packages are used to solve specific problems or tasks. These relations between your project and the external packages are called dependencies.\nFor example, imagine we are doing a small program that takes your birthday as an input and tells you how many days are left until your birthday. We search in the repository if someone has published a package to retrieve the actual date and manage date types, and maybe we could search for another package to show a calendar, because we want to optimize our time, and we wish the user to click a calendar button and choose the day in the calendar instead of typing it.\nAs you can see, packages are convenient. In some cases, they may be even needed, as they can manage aspects of your program you may not be proficient in, or provide an easier use of them.\nFor more comprehensive information visit npm definition\nPackage.json\nDependencies in your project are stored in a file called package.json. Every package.json must contain, at least, the name and version of your project.\nPackage.json is located in the root of your project.\nImportant\nIf package.json is not on your root directory refer to Problems you may encounter section\nIf you wish to learn more information about package.json, click on the following links:\nYarn Package.json\nnpm Package.json\nContent of package.json\nAs you noticed, package.json is a really important file in your project. It contains essential information about our project, therefore you need to understand what&#x2019;s inside.\nThe structure of package.json is divided in blocks, inside the first one you can find essential information of your project such as the name, version, license and optionally some Scripts.\n{\n&quot;name&quot;: &quot;exampleproject&quot;,\n&quot;version&quot;: &quot;0.0.0&quot;,\n&quot;license&quot;: &quot;MIT&quot;,\n&quot;scripts&quot;: {\n&quot;ng&quot;: &quot;ng&quot;,\n&quot;start&quot;: &quot;ng serve&quot;,\n&quot;build&quot;: &quot;ng build&quot;,\n&quot;test&quot;: &quot;ng test&quot;,\n&quot;lint&quot;: &quot;ng lint&quot;,\n&quot;e2e&quot;: &quot;ng e2e&quot;\n}\nThe next block is called dependencies and contains the packages that project needs in order to be developed, compiled and executed.\n&quot;private&quot;: true,\n&quot;dependencies&quot;: {\n&quot;@angular/animations&quot;: &quot;^4.2.4&quot;,\n&quot;@angular/common&quot;: &quot;^4.2.4&quot;,\n&quot;@angular/forms&quot;: &quot;^4.2.4&quot;,\n...\n&quot;zone.js&quot;: &quot;^0.8.14&quot;\n}\nAfter dependencies we find devDependencies, another kind of dependencies present in the development of the application but unnecessary for its execution. One example is typescript. Code is written in typescript, and then, transpiled to javascript. This means the application is not using typescript in execution and consequently not included in the deployment of our application.\n&quot;devDependencies&quot;: {\n&quot;@angular/cli&quot;: &quot;1.4.9&quot;,\n&quot;@angular/compiler-cli&quot;: &quot;^4.2.4&quot;,\n...\n&quot;@types/node&quot;: &quot;~6.0.60&quot;,\n&quot;typescript&quot;: &quot;~2.3.3&quot;\n}\nHaving a peer dependency means that your package needs a dependency that is the same exact dependency as the person installing your package\n&quot;peerDependencies&quot;: {\n&quot;package-123&quot;: &quot;^2.7.18&quot;\n}\nOptional dependencies are just that: optional. If they fail to install, Yarn will still say the install process was successful.\n&quot;optionalDependencies&quot;: {\n&quot;package-321&quot;: &quot;^2.7.18&quot;\n}\nFinally you can have bundled dependencies which are packages bundled together when publishing your package in a repository.\n{\n&quot;bundledDependencies&quot;: [\n&quot;package-4&quot;\n]\n}\nHere is the link to an in-depth explanation of dependency types&#x200B;.\nScripts\nScripts are a great way of automating tasks related to your package, such as simple build processes or development tools.\nFor example:\n{\n&quot;name&quot;: &quot;exampleproject&quot;,\n&quot;version&quot;: &quot;0.0.0&quot;,\n&quot;license&quot;: &quot;MIT&quot;,\n&quot;scripts&quot;: {\n&quot;build-project&quot;: &quot;node hello-world.js&quot;,\n}\nYou can run that script by running the command yarn (run) script or npm run script, check the example below:\n$ yarn (run) build-project # run is optional\n$ npm run build-project\nThere are special reserved words for scripts, like preinstall, which will execute the script automatically\nbefore the package you install are installed.\nChech different uses for scripts in the following links:\nYarn scripts documentation\nnpm scripts documentation\nOr you can go back to\nContent of package.json&#x200B;.\nManaging dependencies\nIn order to manage dependencies we recommend using package managers in your projects.\nA big reason is their usability. Adding or removing a package is really easy, and by doing so, packet manager update the package.json and copies (or removes) the package in the needed location, with a single comand.\nAnother reason, closely related to the first one, is reducing human error by automating the package management process.\nTwo of the package managers you can use in node.js projects are &quot;yarn&quot; and &quot;npm&quot;. While you can use both, we encourage you to use only one of them while working on projects. Using both may lead to different dependencies between members of the team.\nnpm\nWe&#x2019;ll start by installing npm following this small guide here.\nAs stated on the web, npm comes inside of node.js, and must be updated after installing node.js, in the same guide you used earlier are written the instructions to update npm.\nHow npm works\nIn order to explain how npms works, let&#x2019;s take a command as an example:\n$ npm install @angular/material @angular/cdk\nThis command tells npm to look for the packages @angular/material and @angular/cdk in the npm registry, download and decompress them in the folder node_modules along with their own dependencies. Additionally, npm will update package.json and create a new file called package-lock.json.\nAfter initializating and installing the first package there will be a new folder called node_modules in your project. This folder is where your packages are unzipped and stored, following a tree scheme.\nTake in consideration both npm and yarn need a package.json in the root of your project in order to work properly. If after creating your project don&#x2019;t have it, download again the package.json from the repository or you&#x2019;ll have to start again.\nBrief overview of commands\nIf we need to create a package.json from scratch, we can use the comand init. This command asks the user for basic information about the project and creates a brand new package.json.\n$ npm init\nInstall (or i) installs all modules listed as dependencies in package.json locally. You can also specify a package, and install that package. Install can also be used with the parameter -g, which tells npm to install the Global package.\n$ npm install\n$ npm i\n$ npm install Package\nNote\nEarlier versions of npm did not add dependencies to package.json unless it was used with the flag --save, so npm install package would be npm install --save package, you have one example below.\n$ npm install --save Package\nNpm needs flags in order to know what kind of dependency you want in your project, in npm you need to put the flag -D or --save-dev to install devdependencies, for more information consult the links at the end of this section.\n$ npm install -D package\n$ npm install --save-dev package\n&#x200B;\nThe next command uninstalls the module you specified in the command.\n$ npm uninstall Package\nls command shows us the dependencies like a nested tree, useful if you have few packages, not so useful when you need a lot of packages.\n$ npm ls\nnpm@@VERSION@ /path/to/npm\n&#x2514;&#x2500;&#x252C; init-package-json@0.0.4\n&#x2514;&#x2500;&#x2500; promzard@0.1.5\nexample tree\nWe recommend you to learn more about npm commands in the following link, navigating to the section cli commands.\nAbout Package-lock.json\nPackage-lock.json describes the dependency tree resulting of using package.json and npm.\nWhenever you update, add or remove a package, package-lock.json is deleted and redone with\nthe new dependencies.\n&quot;@angular/animations&quot;: {\n&quot;version&quot;: &quot;4.4.6&quot;,\n&quot;resolved&quot;: &quot;https://registry.npmjs.org/@angular/animations/-/animations-4.4.6.tgz&quot;,\n&quot;integrity&quot;: &quot;sha1-+mYYmaik44y3xYPHpcl85l1ZKjU=&quot;,\n&quot;requires&quot;: {\n&quot;tslib&quot;: &quot;1.8.0&quot;\n}\nThis lock file is checked everytime the command npm i (or npm install) is used without specifying a package,\nin the case it exists and it&#x2019;s valid, npm will install the exact tree that was generated, such that subsequent\ninstalls are able to generate identical dependency trees.\nWarning\nIt is not recommended to modify this file yourself. It&#x2019;s better to leave its management to npm.\nMore information is provided by the npm team at package-lock.json\nYarn\nYarn is an alternative to npm, if you wish to install yarn follow the guide getting started with yarn and download the correct version for your operative system. Node.js is also needed you can find it here.\nWorking with yarn\nYarn is used like npm, with small differences in syntax, for example npm install module is changed to yarn add module.\n$ yarn add @covalent\nThis command is going to download the required packages, modify package.json, put the package in the folder node_modules and makes a new yarn.lock with the new dependency.\nHowever, unlike npm, yarn maintains a cache with packages you download inside. You don&#x2019;t need to download every file every time you do a general installation. This means installations faster than npm.\nSimilarly to npm, yarn creates and maintains his own lock file, called yarn.lock. Yarn.lock gives enough information about the project for dependency tree to be reproduced.\nyarn commands\nHere we have a brief description of yarn&#x2019;s most used commands:\n$ yarn add Package\n$ yarn add --dev Package\nAdds a package locally to use in your package. Adding the flags --dev or -D will add them to devDependencies instead of the default dependencies, if you need more information check the links at the end of the section.\n$ yarn init\nInitializes the development of a package.\n$ yarn install\nInstalls all the dependencies defined in a package.json file, you can also write &quot;yarn&quot; to achieve the same effect.\n$ yarn remove Package\nYou use it when you wish to remove a package from your project.\n$ yarn global add Package\nInstalls the Global package.\nPlease, refer to the documentation to learn more about yarn commands and their attributes: yarn commands\nyarn.lock\nThis file has the same purpose as Package-lock.json, to guide the packet manager, in this case yarn,\nto install the dependency tree specified in yarn.lock.\nYarn.lock and package.json are\nessential files when collaborating in a project more co-workers and may be a\nsource of errors if programmers do not use the same manager.\nYarn.lock follows the same structure as package-lock.json, you can find an example of dependency below:\n&quot;@angular/animations@^4.2.4&quot;:\nversion &quot;4.4.6&quot;\nresolved &quot;https://registry.yarnpkg.com/@angular/animations/-/animations-4.4.6.tgz#fa661899a8a4e38cb7c583c7a5c97ce65d592a35&quot;\ndependencies:\ntslib &quot;^1.7.1&quot;\nWarning\nAs with package-lock.json, it&#x2019;s strongly not adviced to modify this file. Leave its management to yarn\nYou can learn more about yarn.lock here: yarn.lock\nGlobal package\nGlobal packages are packages installed in your operative system instead of your local project,\nglobal packages useful for developer tooling that is not part of any individual project but instead is used for local commands.\nA good example of global package is angular/cli, a command line interface for angular used in our projects. You can install\na global package in npm with &quot;npm install -g package&quot; and &quot;yarn global add package&quot; with yarn, you have a npm example below:\nListing 13. npm global package\nnpm install &#x2013;g @angular/cli\nGlobal npm\nGlobal yarn\nPackage version\nDependencies are critical to the success of a package. You must be extra careful about\nwhich version packages are using, one package in a different version may break your code.\nVersioning in npm and yarn, follows a semantic called semver, following the logic\nMAJOR.MINOR.PATCH, like for example, @angular/animations: 4.4.6.\nDifferent versions\nSometimes, packages are installed with a different version from the one initially installed.\nThis happens because package.json also contains the range of versions we allow yarn or npm to\ninstall or update to, example:\n&quot;@angular/animations&quot;: &quot;^4.2.4&quot;\nAnd here the installed one:\n&quot;@angular/animations&quot;: {\n&quot;version&quot;: &quot;4.4.6&quot;,\n&quot;resolved&quot;: &quot;https://registry.npmjs.org/@angular/animations/-/animations-4.4.6.tgz&quot;,\n&quot;integrity&quot;: &quot;sha1-+mYYmaik44y3xYPHpcl85l1ZKjU=&quot;,\n&quot;requires&quot;: {\n&quot;tslib&quot;: &quot;1.8.0&quot;\n}\nAs you can see, the version we initially added is 4.2.4, and the version finally installed after\na global installation of all packages, 4.4.6.\nInstalling packages without package-lock.json or yarn.lock using their respective packet managers, will always\nend with npm or yarn installing the latest version allowed by package.json.\n&quot;@angular/animations&quot;: &quot;^4.2.4&quot; contains not only the version we added, but also the range we allow npm and yarn\nto update. Here are some examples:\n&quot;@angular/animations&quot;: &quot;&lt;4.2.4&quot;\nThe version installed must be lower than 4.2.4 .\n&quot;@angular/animations&quot;: &quot;&gt;=4.2.4&quot;\nThe version installed must be greater than or equal to 4.2.4 .\n&quot;@angular/animations&quot;: &quot;=4.2.4&quot;\nthe version installed must be equal to 4.2.4 .\n&quot;@angular/animations&quot;: &quot;^4.2.4&quot;\nThe version installed cannot modify the first non zero digit, for example in this case\nit cannot surpass 5.0.0 or be lower than 4.2.4 .\nYou can learn more about this in Versions\nProblems you may encounter\nIf you can&#x2019;t find package.json, you may have deleted the one you had previously,\nwhich means you have to download the package.json from the repository.\nIn the case you are creating a new project you can create a new package.json. More information\nin the links below. Click on Package.json if you come from that section. \nCreating new package.json in yarn\nCreating new package.json in npm\nImportant\nUsing npm install or yarn without package.json in your projects will\nresult in compilation errors. As we mentioned earlier,\nPackage.json contains essential information about your project.\nIf you have package.json, but you don&#x2019;t have package-lock.json or yarn.lock the use of\ncommand &quot;npm install&quot; or &quot;yarn&quot; may result in a different dependency tree.\nIf you are trying to import a module and visual code studio is not able to find it,\nis usually caused by error adding the package to the project, try to add the module again with yarn or npm,\nand restart Visual Studio Code.\nBe careful with the semantic versioning inside your package.json of the packages,\nor you may find a new update on one of your dependencies breaking your code.\nTip\nIn the following link\nthere is a solution to a problematic update to one package.\nA list of common errors of npm can be found in: npm errors\nRecomendations\nUse yarn or npm in your project, reach an agreement with your team in order to choose one, this will avoid\nundesired situations like forgetting to upload an updated yarn.lock or package-lock.json.\nBe sure to have the latest version of your project when possible.\nTip\nPull your project every time it&#x2019;s updated. Erase your node_modules folder and reinstall all\ndependencies. This assures you to be working with the same dependencies your team has.\nAD Center recommends the use of yarn.\n&#x2190;&#xA0;Previous:&#xA0;Layers&#xA0;| &#x2191;&#xA0;Up:&#xA0;devon4ng&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Angular&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devon4ng.asciidoc_ionic.html","title":"17. Ionic","body":"\n17. Ionic\n17.1. Ionic 4 Getting started\nIonic is a front-end focused framework which offers different tools for developing hybrid mobile applications. The web technologies used for this purpose are CSS, Sass, HTML5 and Typescript.\n17.1.1. Why Ionic?\nIonic is used for developing hybrid applications, which means not having to rely on a specifyc IDE such as Android Studio or Xcode. Furthermore, development of native apps require learning different languages (Java/Kotlin for Android and Objective-C/Swift for Apple), with Ionic, a developer does not have to code the same functionality for multiple platforms, just use the adecuate libraries and components.\n17.1.2. Basic environment set up\nInstall Ionic CLI\nAlthough the devonfw distribution comes with and already installed Ionic CLI, here are the steps to install it. The instalation of Ionic is easy, just one command has to be written:\nnpm install -g ionic\nUpdate Ionic CLI\nTo update the installed version of the CLI run:\nnpm install -g ionic@latest\nIf the devonfw&#x2019;s ionic CLI has to be updated, the steps are a little bit different:\nopen the devonfw bundled console.\ncd [devonfw_dist_folder]\ncd software/nodejs\nnpm uninstall ionic --no-save\nnpm install ionic@latest --no-save\n17.2. Basic proyect set up\nThe set up of an ionic application is pretty inmediate and can be done in one line:\nionic start &lt;name&gt; &lt;template&gt; --type=angular\nionic start: Command to create an app.\n&lt;name&gt;: Name of the application.\n&lt;template&gt;: Model of the application.\n--type=angular: With this flag, the app produced will be based on angular.\nTo create an empty project, the following command can be used:\nionic start MyApp blank --type=angular\nThe image above shows the directory structure generated.\nThere are more templates available that can be seen with the command\nionic start --list\nThe templates surrounded by red line are based on angular and comes with Ionic v4, while the others belong to earlier versions (before v4).\n17.3. Ionic 4 to android\nThis page is written to help developers to go from the source code of an ionic application to an android one, with this in mind, topics such as: environment, commands, modifications,&#x2026;&#x200B; are covered.\n17.3.1. Assumptions\nThis document assumes that the reader has already:\nSource code of an ionic 4 application and wants to build it on an android device,\nA working installation of Node.js\nAn Ionic CLI installed and up-to-date.\nAndroid Studio and Android SDK.\n17.3.2. From ionic 4 to Android project\nWhen a native application is being dessigned, sometimes, functionalities that uses camera, geolocation, push notification, &#x2026;&#x200B; are requested. To resolve these requests, Capacitor can be used.\nIn general terms, Capacitor wraps apps made with Ionic (HTML, SCSS, Typescript) into WebViews that can be displayed in native applications (Android, IOS) and allows the developer to access native functionalities like the ones said before.\nInstalling capacitor is as easy as installing any node module, just a few commands have to be run in a console:\ncd name-of-ionic-4-app\nnpm install --save @capacitor/core @capacitor/cli\nThen, it is necessary to initialize capacitor with some information: app id, name of the app and the directory where your app is stored. To fill this information, run:\nnpx cap init\nModifications\nThroughout the development process, usually back-end and front-end are on a local computer, so it&#x2019;s a common practice to have diferent configuration files for each environment (commonly production and development). Ionic 4 uses an angular.json file to store those configurations and some rules to be applied.\nIf a back-end is hosted on http://localhost:8081, and that direction is used in every environment, the application built for android will not work because computer and device do not have the same localhost. Fortunately, different configurations can be defined.\nAndroid Studio uses 10.0.0.2 as alias for 127.0.0.1 (computer&#x2019;s localhost) so adding http//10.0.0.2:8081 in a new environment file and modifying angular.json accordingly, will make possible connect front-end and back-end.\n&quot;build&quot;: {\n...\n&quot;configurations&quot;: {\n...\n&quot;android&quot;: {\n&quot;fileReplacements&quot;: [\n{\n&quot;replace&quot;: &quot;src/environments/environment.ts&quot;,\n&quot;with&quot;: &quot;src/environments/environment.android.ts&quot;\n}\n]\n},\n}\n}\nBuild\nOnce configured, it is necessary to build the Ionic 4 app using this new configuration:\nionic build --configuration=android\nThe next commands copy the build application on a folder named android and open android studio.\nnpx cap add android\nnpx cap copy\nnpx cap open android\n17.3.3. From Android project to emulated device\nOnce Android Studio is opened, follow these steps:\nClick on &quot;Build&quot; &#x2192; Make project.\nClick on &quot;Build&quot; &#x2192; Make Module &apos;app&apos; (default name).\nClick on&quot; Build&quot; &#x2192; Build Bundle(s) / APK(s) &#x2192; Build APK(s).\nClick on run and choose a device.\nIf there are no devices available, a new one can be created:\nClick on &quot;Create new device&quot;\nSelect hardware and click &quot;Next&quot;. For example: Phone &#x2192; Nexus 5X.\nDownload a system image.\nClick on download.\nWait until the installation finished and then click &quot;Finish&quot;.\nClick &quot;Next&quot;.\nVerify configuration (default configuration should be enough) and click &quot;Next&quot;.\nCheck that the new device is created correctly.\n17.3.4. From Android project to real device\nTo test on a real android device, an easy aproach to comunicate a smartphone (front-end) and computer (back-end) is to configure a Wi-fi hotspot and connect the computer to it. A guide about this process can be found at https://support.google.com/nexus/answer/9059108?hl=en\nOnce connected, run ipconfig on a console if you are using windows or ifconfig on a linux machine to get the IP address of your machine&#x2019;s Wireless LAN adapter Wi-fi.\nThis obtained IP must be used instead of &quot;localhost&quot; or &quot;10.0.2.2&quot; at environment.android.ts.\nAfter this configuration, follow the build steps in &quot;From ionic 4 to Android project&quot; and the first three steps in &quot;From Android project to emulated device&quot;.\nSend APK to Android through USB\nTo send the built application to a device, you can connect computer and mobile through USB, but first, it is necessary to unlock developer options.\nOpen &quot;Settings&quot; and go to &quot;System&quot;.\nClick on &quot;About&quot;.\nClick &quot;Build number&quot; seven times to unlock developer options.\nGo to &quot;System&quot; again an then to &quot;Developer options&quot;\nCheck that the options are &quot;On&quot;.\nCheck that &quot;USB debugging&quot; is activated.\nAfter this, do the step four in &quot;From Android project to emulated device&quot; and choose the connected smartphone.\nSend APK to Android throught email\nWhen you build an APK, a dialog gives two options: locate or analyze. If the first one is chosen, Windows file explorer will be opened showing an APK that can be send using email. Download the APK on your phone and click it to install.\n17.3.5. Result\nIf everything goes correctly, the Ionic 4 application will be ready to be tested.\n17.4. Ionic Progressive Web App\nThis guide is a continuation of the guide Angular PWAs, therefore, valid concepts explained there are still valid in this page but focused on Ionic.\n17.4.1. Assumptions\nThis guide assumes that you already have installed:\nNode.js\nnpm package manager\nAngular CLI\nIonic 4 CLI\nCapacitor\nAlso, it is a good idea to read the document about PWA using Angular.\n17.4.2. Sample Application\nFigure 71. Basic ionic PWA.\nTo explain how to build progressive web apps (PWA) using Ionic 4, a basic application is going to be built. This app will be able to take photos even without network using PWA elements.\nStep 1: Create a new project\nThis step can be completed with one simple command: ionic start &lt;name&gt; &lt;template&gt;, where &lt;name&gt; is the name and &lt;template&gt; a model for the app. In this case, the app is going to be named basic-ion-pwa.\nStep 2: Structures and styles\nThe styles (scss) and structures (html) do not have anything specially relevant, just colors and ionic web components. The code can be found in devon4ng samples.\nStep 3: Add functionality\nAfter this step, the app will allow users take photos and display them in the main screen.\nFirst we have to import three important elements:\nDomSanitizer: Sanitizes values to be safe to use.\nSafeResourceUrl: Interface for values that are safe to use as URL.\nPlugins: Capacitor constant value used to access to the device&#x2019;s camera and toast dialogs.\nimport { DomSanitizer, SafeResourceUrl } from &apos;@angular/platform-browser&apos;;\nimport { Plugins, CameraResultType } from &apos;@capacitor/core&apos;;\nconst { Camera, Toast } = Plugins;\nThe process of taking a picture is enclosed in a takePicture method. takePicture calls the Camera&#x2019;s getPhoto function which returs an URL or an exception. If a photo is taken then the image displayed in the main page will be changed for the new picture, else, if the app is closed without changing it, a toast message will be displayed.\nexport class HomePage {\nimage: SafeResourceUrl;\n...\nasync takePicture() {\ntry {\nconst image = await Camera.getPhoto({\nquality: 90,\nallowEditing: true,\nresultType: CameraResultType.Uri,\n});\n// Change last picture shown\nthis.image = this.sanitizer.bypassSecurityTrustResourceUrl(image.webPath);\n} catch (e) {\nthis.show(&apos;Closing camera&apos;);\n}\n}\nasync show(message: string) {\nawait Toast.show({\ntext: message,\n});\n}\n}\nStep 4: PWA Elements\nWhen Ionic apps are not running natively, some resources like Camera do not work by default but can be enabled using PWA Elements. To use Capacitor&#x2019;s PWA elements run npm install @ionic/pwa-elements and modify src/main.ts as shown below.\n...\n// Import for PWA elements\nimport { defineCustomElements } from &apos;@ionic/pwa-elements/loader&apos;;\nif (environment.production) {\nenableProdMode();\n}\nplatformBrowserDynamic().bootstrapModule(AppModule)\n.catch(err =&gt; console.log(err));\n// Call the element loader after the platform has been bootstrapped\ndefineCustomElements(window);\nStep 5: Make it Progressive.\nTurining an ionic 4 app into a PWA is pretty easy, the same module used to turn Angular apps into PWAs has to be added, to do so, run: ng add @angular/pwa. This command also creates an icons folder inside src/assets and contains angular icons for multiple resolutions. If you want use other images, be sure that they have the same resolution, the names can be different but the file manifest.json has to be changed accordingly.\nStep 6: Configure the app\nmanifest.json\nDefault configuration.\nngsw-config.json\nAt assetGroups &#x2192; resources add a urls field and a pattern to match PWA Elements scripts and other resources (images, styles, &#x2026;&#x200B;):\n&quot;urls&quot;: [&quot;https://unpkg.com/@ionic/pwa-elements@1.0.2/dist/**&quot;]\nStep 7: Check that your app is a PWA\nTo check if an app is a PWA lets compare its normal behaviour against itself but built for production. Run in the project&#x2019;s root folder the commands below:\nionic build --prod to build the app using production settings.\nnpm install http-server to install an npm module that can serve your built application. Documentation here.\nGo to the www folder running cd www.\nhttp-server -o to serve your built app.\nFigure 72. Http server running on localhost:8081.\n&#xA0;\nIn another console instance run ionic serve to open the common app (not built).\nFigure 73. Ionic server running on localhost:8100.\n&#xA0;\nThe first difference can be found on Developer tools &#x2192; application, here it is seen that the PWA application (left) has a service worker and the common one does not.\nFigure 74. Application service worker comparison.\n&#xA0;\nIf the &quot;offline&quot; box is checked, it will force a disconnection from network. In situations where users do not have connectivity or have a slow, one the PWA can still be accesed and used.\nFigure 75. Offline application.\n&#xA0;\nFinally, plugins like Lighthouse can be used to test whether an application is progressive or not.\nFigure 76. Lighthouse report.\n&#x2190;&#xA0;Previous:&#xA0;Angular&#xA0;| &#x2191;&#xA0;Up:&#xA0;devon4ng&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Layouts&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devon4ng.asciidoc_layers.html","title":"14. Layers","body":"\n14. Layers\n14.1. Components Layer\nThe components layer encapsulates all components presenting the current application view state, which means data to be shown to the user.\nThe term component refers to a component described by the standard Web Components.\nSo this layer has all Angular components, directives and pipes defined for an application.\nThe main challenges are:\nhow to structure the components layer (see File Structure Guide)\ndecompose components into maintainable chunks (see Component Decomposition Guide)\nhandle component interaction\nmanage calls to the services layer\napply a maintainable data and eventflow throughout the component tree\n14.1.1. Smart and Dumb Components\nThe architecture applies the concept of Smart and Dumb Components (syn. Containers and Presenters).\nThe concept means that components are devided into Smart and Dumb Components.\nA Smart Component typically is a toplevel dialog inside the component tree.\na component, that can be routed to\na modal dialog\na component, which is placed inside AppComponent\nA Dumb Component can be used by one to many Smart Components.\nInside the component tree a Dumb Component is a child of a Smart Component.\nFigure 29. Component tree example\nAs shown the topmost component is always the AppComponent in Angular applications.\nThe component tree describes the hierarchy of components starting from AppComponent.\nThe figure shows Smart Components in blue and Dumb Components in green.\nAppComponent is a Smart Component by definition.\nInside the template of AppComponent placed components are static components inside the component tree.\nSo they are always displayed.\nIn the example OverviewComponent and DetailsComponent are rendered by Angular compiler depending on current URL the application displays.\nSo OverviewComponents subtree is displayed if the URL is /overview and DetailsComponents subtree is displayed if the URL is /details.\nTo clarify this distinction further the following table shows the main differences.\nTable 20. Smart vs Dumb Components\nSmart Components\nDumb Components\ncontain the current view state\nshow data via binding (@Input) and contain no view state\nhandle events emited by Dumb Components\npass events up the component tree to be handled by Smart Components (@Output)\ncall the services layer\nnever call the services layer\nuse services\ndo not use services\nconsists of n Dumb Components\nis independent of Smart Components\n14.1.2. Interaction of Smart and Dumb Components\nWith the usage of the Smart and Dumb Components pattern one of the most important part is component interaction.\nAngular comes with built in support for component interaction with @Input() and @Output() Decorators.\nThe following figure illustrates an unidirectional data flow.\nData always goes down the component tree - from a Smart Component down its children.\nEvents bubble up, to be handled by a Smart Component.\nFigure 30. Smart and Dumb Component Interaction\nAs shown a Dumb Components role is to define a signature by declaring Input and Output Bindings.\n@Input() defines what data is necessary for that component to work\n@Output() defines which events can be listened on by the parent component\nListing 8. Dumb Components define a signature\nexport class ValuePickerComponent {\n@Input() columns: string[];\n@Input() items: {}[];\n@Input() selected: {};\n@Input() filter: string;\n@Input() isChunked = false;\n@Input() showInput = true;\n@Input() showDropdownHeader = true;\n@Output() elementSelected = new EventEmitter&lt;{}&gt;();\n@Output() filterChanged = new EventEmitter&lt;string&gt;();\n@Output() loadNextChunk = new EventEmitter();\n@Output() escapeKeyPressed = new EventEmitter();\n}\nThe example shows the Dumb Component ValuePickerComponent.\nIt describes seven input bindings with isChunked, showHeader and showDropdownHeader being non mandatory as they have a default value.\nFour output bindings are present. Typically, a Dumb Component has very little code to no code inside the TypeScript class.\nListing 9. Smart Components use the Dumb Components signature inside the template\n&lt;div&gt;\n&lt;value-input\n...&gt;\n&lt;/value-input&gt;\n&lt;value-picker\n*ngIf=&quot;isValuePickerOpen&quot;\n[columns]=&quot;columns&quot;\n[items]=&quot;filteredItems&quot;\n[isChunked]=&quot;isChunked&quot;\n[filter]=&quot;filter&quot;\n[selected]=&quot;selectedItem&quot;\n[showDropdownHeader]=&quot;showDropdownHeader&quot;\n(loadNextChunk)=&quot;onLoadNextChunk()&quot;\n(elementSelected)=&quot;onElementSelected($event)&quot;\n(filterChanged)=&quot;onFilterChanged($event)&quot;\n(escapeKeyPressed)=&quot;onEscapePressedInsideChildTable()&quot;&gt;\n&lt;/value-picker&gt;\n&lt;/div&gt;\nInside the Smart Components template the events emitted by Dumb Components are handled.\nIt is a good practice to name the handlers with the prefix on* (e.g. onInputChanged()).\n14.2. Services Layer\nThe services layer is more or less what we call &apos;business logic layer&apos; on the server side.\nIt is the layer where the business logic is placed.\nThe main challenges are:\nDefine application state and an API for the components layer to use it\nHandle application state transitions\nPerform backend interaction (XHR, WebSocket, etc.)\nHandle business logic in a maintainable way\nConfiguration management\nAll parts of the services layer are described in this chapter.\nAn example which puts the concepts together can be found at the end Interaction of Smart Components through the services layer.\n14.2.1. Boundaries\nThere are two APIs for the components layer to interact with the services layer:\nA store can be subscribed to for receiving state updates over time\nA use case service can be called to trigger an action\nTo illustrate the fact the follwing figure shows an abstract overview.\nFigure 31. Boundaries to components layer\n14.2.2. Store\nA store is a class which defines and handles application state with its transitions over time.\nInteraction with a store is always synchronous.\nA basic implementation using rxjs can look like this.\nTip\nA more profound implementation taken from a real-life project can be found here (Abstract Class Store).\nListing 10. Store defined using rxjs\n@Injectable()\nexport class ProductSearchStore {\nprivate stateSource = new BehaviorSubject&lt;ProductSearchState&gt;(defaultProductSearchState);\nstate$ = this.stateSource.asObservable();\nsetLoading(isLoading: boolean) {\nconst currentState = this.stateSource.getValue();\nthis.stateSource.next({\nisLoading: isLoading,\nproducts: currentState.products,\nsearchCriteria: currentState.searchCriteria\n});\n}\n}\nIn the example ProductSearchStore handles state of type ProductSearchState.\nThe public API is the property state$ which is an observable of type ProductSearchState.\nThe state can be changed with method calls.\nSo every desired change to the state needs to be modeled with an method.\nIn reactive terminology this would be an Action.\nThe store does not use any services.\nSubscribing to the state$ observable leads to the subscribers receiving every new state.\nThis is basically the Observer Pattern:\nThe store consumer registeres itself to the observable via state$.subscribe() method call.\nThe first parameter of subscribe() is a callback function to be called when the subject changes.\nThis way the consumer - the observer - is registered.\nWhen next() is called with a new state inside the store, all callback functions are called with the new value.\nSo every observer is notified of the state change.\nThis equals the Observer Pattern push type.\nA store is the API for Smart Components to receive state from the service layer.\nState transitions are handled automatically with Smart Components registering to the state$ observable.\n14.2.3. Use Case Service\nA use case service is a service which has methods to perform asynchronous state transitions.\nIn reactive terminology this would be an Action of Actions - a thunk (redux) or an effect (@ngrx).\nFigure 32. Use case services are the main API to trigger state transitions\nA use case services method - an action - interacts with adapters, business services and stores.\nSo use case services orchestrate whole use cases.\nFor an example see use case service example.\n14.2.4. Adapter\nAn adapter is used to communicate with the backend.\nThis could be a simple XHR request, a WebSocket connection, etc.\nAn adapter is simple in the way that it does not add anything other than the pure network call.\nSo there is no caching or logging performed here.\nThe following listing shows an example.\nFor further information on backend interaction see Consuming REST Services\nListing 11. Calling the backend via an adapter\n@Injectable()\nexport class ProducsAdapter {\nprivate baseUrl = environment.baseUrl;\nconstructor(private http: HttpClient) { }\ngetAll(): Observable&lt;Product[]&gt; {\nreturn this.http.get&lt;Product[]&gt;(this.baseUrl + &apos;/products&apos;);\n}\n}\n14.2.5. Interaction of Smart Components through the services layer\nThe interaction of smart components is a classic problem which has to be solved in every UI technology.\nIt is basically how one dialog tells the other something has changed.\nAn example is adding an item to the shopping basket.\nWith this action there need to be multiple state updates.\nThe small logo showing how many items are currently inside the basket needs to be updated from 0 to 1\nThe price needs to be recalculated\nShipping costs need to be checked\nDiscounts need to be updated\nAds need to be updated with related products\netc.\nPattern\nTo handle this interaction in a scalable way we apply the following pattern.\nFigure 33. Smart Component interaction\nThe state of interest is encapsualted inside a store. All Smart Components interested in the state have to subscibe to the store&#x2019;s API served by the public observable. Thus, with every update to the store the subscribed components receive the new value. The components basically react to state changes. Altering a store can be done directly if the desired change is synchronous. Most actions are of asynchronous nature so the UseCaseService comes into play. Its actions are void methods, which implement a use case, i.e., adding a new item to the basket. It calls asynchronous actions and can perform multiple store updates over time.\nTo put this pattern into perspective the UseCaseService is a programmatic alternative to redux-thunk or @ngrx/effects. The main motivation here is to use the full power of TypeScript&#x2019;s --strictNullChecks and to let the learning curve not to become as steep as it would be when learning a new state management framework. This way actions are just void method calls.\nExample\nFigure 34. Smart Components interaction example\nThe example shows two Smart Components sharing the FlightSearchState by using the FlightSearchStore.\nThe use case shown is started by an event in the Smart Component FlightSearchComponent. The action loadFlight() is called. This could be submitting a search form.\nThe UseCaseService is FlightSearchService, which handles the use case Load Flights.\nUseCaseService example\nexport class FlightSearchService {\nconstructor(\nprivate flightSearchAdapter: FlightSearchAdapter,\nprivate store: FlightSearchStore\n) { }\nloadFlights(criteria: FlightSearchCriteria): void {\nthis.store.setLoadingFlights(true);\nthis.store.clearFlights();\nthis.flightSearchAdapter.getFlights(criteria.departureDate,\n{\nfrom: criteria.departureAirport,\nto: criteria.destinationAirport\n})\n.finally(() =&gt; this.store.setLoadingFlights(false))\n.subscribe((result: FlightTo[]) =&gt; this.store.setFlights(result, criteria));\n}\n}\nFirst the loading flag is set to true and the current flights are cleared. This leads the Smart Component showing a spinner indicating the loading action. Then the asynchronous XHR is triggert by calling the adapter. After completion the loading flag is set to false causing the loading indication no longer to be shown. If the XHR was successful, the data would be put into the store. If the XHR was not successful, this would be the place to handle a custom error. All general network issues should be handled in a dedicated class, i.e., an interceptor. So for example the basic handling of 404 errors is not done here.\n&#x2190;&#xA0;Previous:&#xA0;Architecture&#xA0;| &#x2191;&#xA0;Up:&#xA0;devon4ng&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Guides&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devon4ng.asciidoc_layouts.html","title":"18. Layouts","body":"\n18. Layouts\n18.1. Angular Material Layout\nThe purpose of this guide is to get a basic understanding of creating layouts using Angular Material in a devon4ng application. We will create an application with a header containing some menu links and a sidenav with some navigation links.\nFigure 77. This is what the finished application will look like\n18.1.1. Let&#x2019;s begin\nWe start with opening the console(running console.bat in the Devon distribution folder) and running the following command to start a project named devon4ng-mat-layout\nng new devon4ng-mat-layout\nSelect y when it asks whether it would like to add Angular routing and select SCSS when it asks for the stylesheet format.\nYou can also use the devonfw-ide CLI to create a new devon4ng application.\nOnce the creation process is complete, open your newly created application in Visual Studio Code. Try running the empty application by running the following command in the integrated terminal:\nng serve\nAngular will spin up a server and you can check your application by visiting http://localhost:4200/ in your browser.\nFigure 78. Blank application\n18.1.2. Adding Angular Material library to the project\nNext we will add Angular Material to our application. In the integrated terminal, press Ctrl + C to terminate the running application and run the following command:\nnpm install --save @angular/material @angular/cdk @angular/animations\nYou can also use Yarn to install the dependencies if you prefer that:\nyarn add @angular/material @angular/cdk @angular/animations\nOnce the dependencies are installed, we need to import the BrowserAnimationsModule in our AppModule for animations support.\nListing 65. Importing BrowserAnimationsModule in AppModule\nimport {BrowserAnimationsModule} from &apos;@angular/platform-browser/animations&apos;;\n@NgModule({\n...\nimports: [BrowserAnimationsModule],\n...\n})\nexport class AppModule { }\nAngular Material provides a host of components for designing our application. All the components are well structured into NgModules. For each component from the Angular Material library that we want to use, we have to import the respective NgModule.\nListing 66. We will be using the following components in our application:\nimport { MatIconModule, MatButtonModule, MatMenuModule, MatListModule, MatToolbarModule, MatSidenavModule } from &apos;@angular/material&apos;;\n@NgModule({\n...\nimports: [\n...\nMatIconModule,\nMatButtonModule,\nMatMenuModule,\nMatListModule,\nMatToolbarModule,\nMatSidenavModule,\n...\n],\n...\n})\nexport class AppModule { }\nA better approach is to import and then export all the required components in a shared module. But for the sake of simplicity, we are importing all the required components in the AppModule itself.\nNext, we include a theme in our application. Angular Material comes with four inbuilt themes: indigo-pink, deeppurple-amber, pink-bluegrey and purple-green. It is also possible to create our own custom theme, but that is beyond the scope of this guide. Including a theme is required to apply all of the core and theme styles to your application.\nWe will include the indigo-pink theme in our application by importing the indigo-pink.css file in our src/styles.scss:\nListing 67. In src/styles.scss:\n@import &quot;~@angular/material/prebuilt-themes/indigo-pink.css&quot;;\nSome Angular Material components depend on HammerJs for gestures. So it is a good idea to install HammerJs as a dependency in our application. To do so, run the following command in the terminal:\nnpm install --save hammerjs\nThen import it in the src/main.ts file\nimport &apos;hammerjs&apos;;\nTo use Material Design Icons along with the mat-icon component, we will load the Material Icons library in our src/index.html file\nListing 68. In src/index.html:\n&lt;link href=&quot;https://fonts.googleapis.com/icon?family=Material+Icons&quot; rel=&quot;stylesheet&quot;&gt;\n18.1.3. Development\nNow that we have all the Angular Material related dependencies set up in our project, we can start coding. Let&#x2019;s begin by adding a suitable margin and font to the body element of our single page application. We will add it in the src/styles.scss file to apply it globally:\nListing 69. In src/styles.scss:\nbody {\nmargin: 0;\nfont-family: &quot;Segoe UI&quot;, Roboto, sans-serif;\n}\nAt this point, if we run our application with ng serve, this is how it will look like:\nFigure 79. Application with Angular Material set up\nWe will clear the app.component.html file and setup a header with a menu button and some navigational links. We will use mat-toolbar, mat-button, mat-menu, mat-icon and mat-icon-button for this:\nListing 70. app.component.html:\n&lt;mat-toolbar color=&quot;primary&quot;&gt;\n&lt;button mat-icon-button aria-label=&quot;menu&quot;&gt;\n&lt;mat-icon&gt;menu&lt;/mat-icon&gt;\n&lt;/button&gt;\n&lt;button mat-button [matMenuTriggerFor]=&quot;submenu&quot;&gt;Menu 1&lt;/button&gt;\n&lt;button mat-button&gt;Menu 2&lt;/button&gt;\n&lt;button mat-button&gt;Menu 3&lt;/button&gt;\n&lt;mat-menu #submenu=&quot;matMenu&quot;&gt;\n&lt;button mat-menu-item&gt;Sub-menu 1&lt;/button&gt;\n&lt;button mat-menu-item [matMenuTriggerFor]=&quot;submenu2&quot;&gt;Sub-menu 2&lt;/button&gt;\n&lt;/mat-menu&gt;\n&lt;mat-menu #submenu2=&quot;matMenu&quot;&gt;\n&lt;button mat-menu-item&gt;Menu Item 1&lt;/button&gt;\n&lt;button mat-menu-item&gt;Menu Item 2&lt;/button&gt;\n&lt;button mat-menu-item&gt;Menu Item 3&lt;/button&gt;\n&lt;/mat-menu&gt;\n&lt;/mat-toolbar&gt;\nThe color attribute on the mat-toolbar element will give it the primary (indigo) color as defined by our theme. The color attribute works with most Angular Material components; the possible values are &apos;primary&apos;, &apos;accent&apos; and &apos;warn&apos;.\nThe mat-toolbar is a suitable component to represent a header. It serves as a placeholder for elements we want in our header.\nInside the mat-toolbar, we start with a button having mat-icon-button attribute, which itself contains a mat-icon element having the value menu. This will serve as a menu button which we can use to toggle the sidenav.\nWe follow it with some sample buttons having the mat-button attribute. Notice the first button has a property matMenuTriggerFor binded to a local reference submenu. As the property name suggests, the click of this button will display the mat-menu element with the specified local reference as a drop-down menu. The rest of the code is self explanatory.\nFigure 80. This is how our application looks with the first menu button (Menu 1) clicked.\nWe want to keep the sidenav toggling menu button on the left and move the rest to the right to make it look better. To do this we add a class to the menu icon button:\nListing 71. app.component.html:\n...\n&lt;button mat-icon-button aria-label=&quot;menu&quot; class=&quot;menu&quot;&gt;\n&lt;mat-icon&gt;menu&lt;/mat-icon&gt;\n&lt;/button&gt;\n...\nAnd in the app.component.scss file, we add the following style:\nListing 72. app.component.scss:\n.menu {\nmargin-right: auto;\n}\nThe mat-toolbar element already has it&#x2019;s display property set to flex. Setting the menu icon button&#x2019;s margin-right property to auto keeps itself on the left and pushes the other elements to the right.\nFigure 81. Final look of the header.\nNext, we will create a sidenav. But before that lets create a couple of components to navgate between, the links of which we will add to the sidenav.\nWe will use the ng generate component (or ng g c command for short) to create Home and Data components. We nest them in the pages sub-directory since they represent our pages.\nng g c pages/home\nng g c pages/data&apos;;\nLet us set up the routing such that when we visit http://localhost:4200/ root url we see the HomeComponent and when we visit http://localhost:4200/data url we see the DataComponent.\nWe had opted for routing while creating the application, so we have the routing module app-routing.module.ts setup for us. In this file, we have the empty routes array where we set up our routes.\nListing 73. app-routing.module.ts:\n...\nimport { HomeComponent } from &apos;./pages/home/home.component&apos;;\nimport { DataComponent } from &apos;./pages/data/data.component&apos;;\nconst routes: Routes = [\n{ path: &apos;&apos;, component: HomeComponent },\n{ path: &apos;data&apos;, component: DataComponent }\n];\n...\nWe need to provide a hook where the components will be loaded when their respective URLs are loaded. We do that by using the router-outlet directive in the app.component.html.\nListing 74. app.component.html:\n...\n&lt;/mat-toolbar&gt;\n&lt;router-outlet&gt;&lt;/router-outlet&gt;\nNow when we visit the defined URLs we see the appropriate components rendered on screen.\nLets change the contents of the components to have something better.\nListing 75. home.component.html:\n&lt;h2&gt;Home Page&lt;/h2&gt;\nListing 76. home.component.scss:\nh2 {\ntext-align: center;\nmargin-top: 50px;\n}\nListing 77. data.component.html:\n&lt;h2&gt;Data Page&lt;/h2&gt;\nListing 78. data.component.scss:\nh2 {\ntext-align: center;\nmargin-top: 50px;\n}\nThe pages look somewhat better now:\nFigure 82. Home page\nFigure 83. Data page\nLet us finally create the sidenav. To implement the sidenav we need to use 3 Angular Material components: mat-sidenav-container, mat-sidenav and mat-sidenav-content.\nThe mat-sidenav-container, as the name suggests, acts as a container for the sidenav and the associated content. So it is the parent element, and mat-sidenav and mat-sidenav-content are the children sibling elements. mat-sidenav represents the sidenav. We can put any content we want, though it is usually used to conatain a list of navigational links. The mat-sidenav-content element is for conataining our main page content. Since we need the sidenav application-wide, we will put it in the app.component.html.\nListing 79. app.component.html:\n...\n&lt;/mat-toolbar&gt;\n&lt;mat-sidenav-container&gt;\n&lt;mat-sidenav mode=&quot;over&quot; [disableClose]=&quot;false&quot; #sidenav&gt;\nSidenav\n&lt;/mat-sidenav&gt;\n&lt;mat-sidenav-content&gt;\n&lt;router-outlet&gt;&lt;/router-outlet&gt;\n&lt;/mat-sidenav-content&gt;\n&lt;/mat-sidenav-container&gt;\nThe mat-sidenav has a mode property, which accepts one of the 3 values: over, push and side. It decides the behavior of the sidenav. mat-sidenav also has a disableClose property which accents a boolean value. It toggles the behavior where we click on the backdrop or press the Esc key to close the sidenav. There are other properties which we can use to customize the appearance, behavior and position of the sidenav. You can find the properties documented online at https://material.angular.io/components/sidenav/api\nWe moved the router-outlet directive inside the mat-sidenav-content where it will render the routed component.\nBut if you check the running application in the browser, we don&#x2019;t see the sidenav yet. That is because it is closed. We want to have the sidenav opened/closed at the click of the menu icon button on the left side of the header we implemented earlier. Notice we have set a local reference #sidenav on the mat-sidenav element. We can access this element and call its toggle() function to toggle open or close the sidenav.\nListing 80. app.component.html:\n...\n&lt;button mat-icon-button aria-label=&quot;menu&quot; class=&quot;menu&quot; (click)=&quot;sidenav.toggle()&quot;&gt;\n&lt;mat-icon&gt;menu&lt;/mat-icon&gt;\n&lt;/button&gt;\n...\nFigure 84. Sidenav is implemented\nWe can now open the sidenav by clicking the menu icon button. But it does not look right. The sidenav is only as wide as its content. Also the page does not stretch the entire viewport due to lack of content.\nLet&#x2019;s add the following styles to make the page fill the viewport:\nListing 81. app.component.scss:\n...\nmat-sidenav-container {\nposition: absolute;\ntop: 64px;\nleft: 0;\nright: 0;\nbottom: 0;\n}\nThe sidenav&#x2019;s width will be corrected when we add the navigational links to it. That is the only thing remaining to be done. Lets implement it now:\nListing 82. app.component.html:\n...\n&lt;mat-sidenav [disableClose]=&quot;false&quot; mode=&quot;over&quot; #sidenav&gt;\n&lt;mat-nav-list&gt;\n&lt;a\nid=&quot;home&quot;\nmat-list-item\n[routerLink]=&quot;[&apos;./&apos;]&quot;\n(click)=&quot;sidenav.close()&quot;\nrouterLinkActive=&quot;active&quot;\n[routerLinkActiveOptions]=&quot;{exact: true}&quot;\n&gt;\n&lt;mat-icon matListAvatar&gt;home&lt;/mat-icon&gt;\n&lt;h3 matLine&gt;Home&lt;/h3&gt;\n&lt;p matLine&gt;sample home page&lt;/p&gt;\n&lt;/a&gt;\n&lt;a\nid=&quot;sampleData&quot;\nmat-list-item\n[routerLink]=&quot;[&apos;./data&apos;]&quot;\n(click)=&quot;sidenav.close()&quot;\nrouterLinkActive=&quot;active&quot;\n&gt;\n&lt;mat-icon matListAvatar&gt;grid_on&lt;/mat-icon&gt;\n&lt;h3 matLine&gt;Data&lt;/h3&gt;\n&lt;p matLine&gt;sample data page&lt;/p&gt;\n&lt;/a&gt;\n&lt;/mat-nav-list&gt;\n&lt;/mat-sidenav&gt;\n...\nWe use the mat-nav-list element to set a list of navigational links. We use the a tags with mat-list-item directive. We implement a click listener on each link to close the sidenav when it is clicked. The routerLink directive is used to provide the URLs to navigate to. The routerLinkActive directive is used to provide the class name which will be added to the link when it&#x2019;s URL is visited. Here we name the class`active`. To stye it, let&apos; modify the app.component.scss file:\nListing 83. app.component.scss:\n...\nmat-sidenav-container {\n...\na.active {\nbackground: #8e8d8d;\ncolor: #fff;\np {\ncolor: #4a4a4a;\n}\n}\n}\nNow we have a working application with a basic layout: a header with some menu and a sidenav with some navigational links.\nFigure 85. Finished application\n18.1.4. Conclusion\nThe purpose of this guide was to provide a basic understanding of creating layouts with Angular Material. The Angular Material library has a huge collection of ready to use components which can be found at https://material.angular.io/components/categories\nIt has provided documentation and example usage for each of its components. Going through the documentation will give a better understanding of using Angular Material components in our devon4ng applications.\n&#x2190;&#xA0;Previous:&#xA0;Ionic&#xA0;| &#x2191;&#xA0;Up:&#xA0;devon4ng&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;NgRx&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devon4ng.asciidoc_ngrx.html","title":"19. NgRx","body":"\n19. NgRx\n19.1. Introduction to NgRx\nNgRx is a state management framework for Angular based on the Redux pattern.\n19.1.1. The need for client side state management\nYou may wonder why you should bother with state management. Usually data resides in a backend storage system, e.g. a database, and is retrieved by the client on a per-need basis. To add, update, or delete entities from this store, clients have to invoke API endpoints at the backend. Mimicking database-like transactions on the client side may seem redundant. However, there are many use cases for which a global client-side state is appropriate:\nthe client has some kind of global state which should survive the destruction of a component, but does not warrant server side persistence, for example: volume level of media, expansion status of menus\nsever side data should not be retrieved every time it is needed, either because multiple components consume it, or because it should be cached, e.g. the personal watchlist in an online streaming app\nthe app provides a rich experience with offline functionality, e.g. a native app built with Ionic\nSaving global states inside the services they originates from results in a data flow that is hard to follow and state becoming inconsistent due to unordered state mutations. Following the single source of truth principle, there should be a central location holding all your application&#x2019;s state, just like a server side database does. State managament libraries for Angular provide tools for storing, retrieving, and updating client-side state.\n19.1.2. Why NgRx?\nAs stated in the introduction, devon4ng does not stipulate a particular state library, or require using one at all. However, NgRx has proven to be a robust, mature solution for this task, with good tooling and 3rd-party library support. Albeit introducing a level of indirection that requires additional effort even for simple features, the redux concept enforces a clear separation of concerns leading to a cleaner architecture.\nNonetheless, you should always compare different approaches to state management and pick the best one suiting your use case. Here&#x2019;s a (non-exhaustive) list of competing state management libraries:\nPlain Rxjs using the simple store described in Abstract Class Store\nNgXS reduces some boilerplate of NgRx by leveraging the power of decorators and moving side effects to the store\nMobX follows a more imperative approach in contrast to the functional Redux pattern\nAkita also uses an imperative approach with direct setters in the store, but keeps the concept of immutable state transitions\n19.1.3. Setup\nTo get a quick start, use the provided template for devon4ng + NgRx.\nTo manually install the core store package together with a set of useful extensions:\nNPM:\nnpm install @ngrx/store @ngrx/effects @ngrx/entity @ngrx/store-devtools --save\nYarn:\nyarn add @ngrx/store @ngrx/effects @ngrx/entity @ngrx/store-devtools\nWe recommend to add the NgRx schematics to your project so you can create code artifacts from the command line:\nNPM:\nnpm install @ngrx/schematics --save-dev\nYarn:\nyarn add @ngrx/schematics --dev\nAfterwards, make NgRx your default schematics provider, so you don&#x2019;t have to type the qualified package name every time:\nng config cli.defaultCollection @ngrx/schematics\nIf you have custom settings for Angular schematics, you have to configure them as described here.\n19.1.4. Concept\nFigure 86. NgRx architecture overview\nFigure 1 gives an overview of the NgRx data flow. The single source of truth is managed as an immutable state object by the store. Components dispatch actions to trigger state changes. Actions are handed over to reducers, which take the current state and action data to compute the next state. Actions are also consumed byeffects, which perform side-effects such as retrieving data from the backend, and may dispatch new actions as a result. Components subscribe to state changes using selectors.\nContinue with Creating a Simple Store.\n19.2. State, Selection and Reducers\n19.2.1. Creating a Simple Store\nIn the following pages we use the example of an online streaming service. We will model a particular feature, a watchlist that can be populated by the user with movies she or he wants to see in the future.\nInitializing NgRx\nIf you&#x2019;re starting fresh, you first have to initialize NgRx and create a root state. The fastest way to do this is using the schematic:\nng generate @ngrx/schematics:store State --root --module app.module.ts\nThis will automatically generate a root store and register it in the app module. Next we generate a feature module for the watchlist:\nng generate module watchlist\nand create a corresponding feature store:\nng generate store watchlist/Watchlist -m watchlist.module.ts\nThis generates a file watchlist/reducers/index.ts with the reducer function, and registers the store in the watchlist module declaration.\nWarning\nIf you&#x2019;re getting an error Schematic &quot;store&quot; not found in collection &quot;@schematics/angular&quot;, this means you forgot to register the NgRx schematics as default.\nNext, add the WatchlistModule to the AppModule imports so the feature store is registered when the application starts. We also added the store devtools which we will use later, resulting in the following file:\napp.module.ts\nimport { BrowserModule } from &apos;@angular/platform-browser&apos;;\nimport { NgModule } from &apos;@angular/core&apos;;\nimport { AppComponent } from &apos;./app.component&apos;;\nimport { EffectsModule } from &apos;@ngrx/effects&apos;;\nimport { AppEffects } from &apos;./app.effects&apos;;\nimport { StoreModule } from &apos;@ngrx/store&apos;;\nimport { reducers, metaReducers } from &apos;./reducers&apos;;\nimport { StoreDevtoolsModule } from &apos;@ngrx/store-devtools&apos;;\nimport { environment } from &apos;../environments/environment&apos;;\nimport { WatchlistModule } from &apos;./watchlist/watchlist.module&apos;;\n@NgModule({\ndeclarations: [\nAppComponent\n],\nimports: [\nBrowserModule,\nWatchlistModule,\nStoreModule.forRoot(reducers, { metaReducers }),\n// Instrumentation must be imported after importing StoreModule (config is optional)\nStoreDevtoolsModule.instrument({\nmaxAge: 25, // Retains last 25 states\nlogOnly: environment.production, // Restrict extension to log-only mode\n}),\n!environment.production ? StoreDevtoolsModule.instrument() : []\n],\nproviders: [],\nbootstrap: [AppComponent]\n})\nexport class AppModule { }\nCreate an entity model and initial state\nWe need a simple model for our list of movies. Create a file watchlist/models/movies.ts and insert the following code:\nexport interface Movie {\nid: number;\ntitle: string;\nreleaseYear: number;\nruntimeMinutes: number;\ngenre: Genre;\n}\nexport type Genre = &apos;action&apos; | &apos;fantasy&apos; | &apos;sci-fi&apos; | &apos;romantic&apos; | &apos;comedy&apos; | &apos;mystery&apos;;\nexport interface WatchlistItem {\nid: number;\nmovie: Movie;\nadded: Date;\nplaybackMinutes: number;\n}\nNote\nWe discourage putting several types into the same file and do this only for the sake of keeping this tutorial brief.\nLater we will learn how to retrieve data from the backend using effects. For now we will create an initial state for the user with a default movie.\nState is defined and transforms by a reducer function. Let&#x2019;s create a watchlist reducer:\ncd watchlist/reducers\nng g reducer WatchlistData --reducers index.ts\nOpen the generated file watchlist-data.reducer.ts. You see three exports: The State interface defines the shape of the state. There is only one instance of a feature state in the store at all times. The initialState constant is the state at application creation time. The reducer function will later be called by the store to produce the next state instance based on the current state and an action object.\nLet&#x2019;s put a movie into the user&#x2019;s watchlist:\nwatchlist-data.reducer.ts\nexport interface State {\nitems: WatchlistItem[];\n}\nexport const initialState: State = {\nitems: [\n{\nid: 42,\nmovie: {\nid: 1,\ntitle: &apos;Die Hard&apos;,\ngenre: &apos;action&apos;,\nreleaseYear: 1988,\nruntimeMinutes: 132\n},\nplaybackMinutes: 0,\nadded: new Date(),\n}\n]\n};\nSelect the current watchlist\nState slices can be retrieved from the store using selectors.\nCreate a watchlist component:\nng g c watchlist/Watchlist\nand add it to the exports of WatchlistModule. Also, replace app.component.html with\n&lt;app-watchlist&gt;&lt;/app-watchlist&gt;\nState observables are obtained using selectors. They are memoized by default, meaning that you don&#x2019;t have to worry about performance if you use complicated calculations when deriving state&#x2009;&#x2014;&#x2009;these are only performed once per state emission.\nAdd a selector to watchlist-data.reducer.ts:\nexport const getAllItems = (state: State) =&gt; state.items;\nNext, we have to re-export the selector for this substate in the feature reducer. Modify the watchlist/reducers/index.ts like this:\nwatchlist/reducers/index.ts\nimport {\nActionReducer,\nActionReducerMap,\ncreateFeatureSelector,\ncreateSelector,\nMetaReducer\n} from &apos;@ngrx/store&apos;;\nimport { environment } from &apos;src/environments/environment&apos;;\nimport * as fromWatchlistData from &apos;./watchlist-data.reducer&apos;;\nimport * as fromRoot from &apos;src/app/reducers/index&apos;;\nexport interface WatchlistState { (1)\nwatchlistData: fromWatchlistData.State;\n}\nexport interface State extends fromRoot.State { (2)\nwatchlist: WatchlistState;\n}\nexport const reducers: ActionReducerMap&lt;WatchlistState&gt; = { (3)\nwatchlistData: fromWatchlistData.reducer,\n};\nexport const metaReducers: MetaReducer&lt;WatchlistState&gt;[] = !environment.production ? [] : [];\nexport const getFeature = createFeatureSelector&lt;State, WatchlistState&gt;(&apos;watchlist&apos;); (4)\nexport const getWatchlistData = createSelector( (5)\ngetFeature,\nstate =&gt; state.watchlistData\n);\nexport const getAllItems = createSelector( (6)\ngetWatchlistData,\nfromWatchlistData.getAllItems\n);\nThe feature state, each member is managed by a different reducer\nFeature states are registered by the forFeature method. This interface provides a typesafe path from root to feature state.\nTie substates of a feature state to the corresponding reducers\nCreate a selector to access the &apos;watchlist&apos; feature state\nselect the watchlistData sub state\nre-export the selector\nNote how createSelector allows to chain selectors. This is a powerful tool that also allows for selecting from multiple states.\nYou can use selectors as pipeable operators:\nwatchlist.component.ts\nexport class WatchlistComponent {\nwatchlistItems$: Observable&lt;WatchlistItem[]&gt;;\nconstructor(\nprivate store: Store&lt;fromWatchlist.State&gt;\n) {\nthis.watchlistItems$ = this.store.pipe(select(fromWatchlist.getAllItems));\n}\n}\nwatchlist.component.html\n&lt;h1&gt;Watchlist&lt;/h1&gt;\n&lt;ul&gt;\n&lt;li *ngFor=&quot;let item of watchlistItems$ | async&quot;&gt;{{item.movie.title}} ({{item.movie.releaseYear}}): {{item.playbackMinutes}}/{{item.movie.runtimeMinutes}} min watched&lt;/li&gt;\n&lt;/ul&gt;\nDispatching an action to update watched minutes\nWe track the user&#x2019;s current progress at watching a movie as the playbackMinutes property. After closing a video, the watched minutes have to be updated. In NgRx, state is being updated by dispatching actions. An action is an option with a (globally unique) type discriminator and an optional payload.\nCreating the action\nCreate a file playback/actions/index.ts. In this example, we do not further separate the actions per sub state. Actions can be defined by using action creators:\nplayback/actions/index.ts\nimport { createAction, props, union } from &apos;@ngrx/store&apos;;\nexport const playbackFinished = createAction(&apos;[Playback] Playback finished&apos;, props&lt;{ movieId: number, stoppedAtMinute: number }&gt;());\nconst actions = union({\nplaybackFinished\n});\nexport type ActionsUnion = typeof actions;\nFirst we specify the type, followed by a call to the payload definition function. Next, we create a union of all possible actions for this file using union, which allows us a to access action payloads in the reducer in a typesafe way.\nTip\nAction types should follow the naming convention [Source] Event, e.g. [Recommended List] Hide Recommendation or [Auth API] Login Success. Think of actions rather as events than commands. You should never use the same action at two different places (you can still handle multiple actions the same way). This faciliates tracing the source of an action. For details see Good Action Hygiene with NgRx by Mike Ryan (video).\nDispatch\nWe skip the implementation of an actual video playback page and simulate wathcing a movie in 10 minute segments by adding a link in the template:\nwatchlist-component.html\n&lt;li *ngFor=&quot;let item of watchlistItems$ | async&quot;&gt;... &lt;button (click)=&quot;stoppedPlayback(item.movie.id, item.playbackMinutes + 10)&quot;&gt;Add 10 Minutes&lt;/button&gt;&lt;/li&gt;\nwatchlist-component.ts\nimport * as playbackActions from &apos;src/app/playback/actions&apos;;\n...\nstoppedPlayback(movieId: number, stoppedAtMinute: number) {\nthis.store.dispatch(playbackActions.playbackFinished({ movieId, stoppedAtMinute }));\n}\nState reduction\nNext, we handle the action inside the watchlistData reducer. Note that actions can be handled by multiple reducers and effects at the same time to update different states, for example if we&#x2019;d like to show a rating modal after playback has finished.\nwatchlist-data.reducer.ts\nexport function reducer(state = initialState, action: playbackActions.ActionsUnion): State {\nswitch (action.type) {\ncase playbackActions.playbackFinished.type:\nreturn {\n...state,\nitems: state.items.map(updatePlaybackMinutesMapper(action.movieId, action.stoppedAtMinute))\n};\ndefault:\nreturn state;\n}\n}\nexport function updatePlaybackMinutesMapper(movieId: number, stoppedAtMinute: number) {\nreturn (item: WatchlistItem) =&gt; {\nif (item.movie.id === movieId) {\nreturn {\n...item,\nplaybackMinutes: stoppedAtMinute\n};\n} else {\nreturn item;\n}\n};\n}\nNote how we changed the reducer&#x2019;s function signature to reference the actions union. The switch-case handles all incoming actions to produce the next state. The default case handles all actions a reducer is not interested in by returning the state unchanged. Then we find the watchlist item corresponding to the movie with the given id and update the playback minutes. Since state is immutable, we have to clone all objects down to the one we would like to change using the object spread operator (&#x2026;&#x200B;).\nCaution\nSelectors rely on object identity to decide whether the value has to be recalculated. Do not clone objects that are not on the path to the change you want to make. This is why updatePlaybackMinutesMapper returns the same item if the movie id does not match.\nAlternative state mapping with immer\nIt can be hard to think in immutable changes, especially if your team has a strong background in imperative programming. In this case, you may find the immer library convenient, which allows to produce immutable objects by manipulating a proxied draft. The same reducer can then be written as:\nwatchlist-data.reducer.ts with immer\nimport { produce } from &apos;immer&apos;;\n...\ncase playbackActions.playbackFinished.type:\nreturn produce(state, draft =&gt; {\nconst itemToUpdate = draft.items.find(item =&gt; item.movie.id === action.movieId);\nif (itemToUpdate) {\nitemToUpdate.playbackMinutes = action.stoppedAtMinute;\n}\n});\nImmer works out of the box with plain objects and arrays.\nRedux devtools\nIf the StoreDevToolsModule is instrumented as described above, you can use the browser extension Redux devtools to see all dispatched actions and the resulting state diff, as well as the current state, and even travel back in time by undoing actions.\nFigure 87. Redux devtools\nContinue with learning about effects\n19.3. Side effects with NgRx/Effects\nReducers are pure functions, meaning they are side-effect free and deterministic. Many actions however have side effects like sending messages or displaying a toast notification. NgRx encapsulates these actions in effects.\nLet&#x2019;s build a recommended movies list so the user can add movies to their watchlist.\n19.3.1. Obtaining the recommendation list from the server\nCreate a module for recommendations and add stores and states as in the previous chapter. Add EffectsModule.forRoot([]) to the imports in AppModule below StoreModule.forRoot(). Add effects to the feature module:\nng generate effect recommendation/Recommendation -m recommendation/recommendation.module.ts\nWe need actions for loading the movie list, success and failure cases:\nrecommendation/actions/index.ts\nimport { createAction, props, union } from &apos;@ngrx/store&apos;;\nimport { Movie } from &apos;src/app/watchlist/models/movies&apos;;\nexport const loadRecommendedMovies = createAction(&apos;[Recommendation List] Load movies&apos;);\nexport const loadRecommendedMoviesSuccess = createAction(&apos;[Recommendation API] Load movies success&apos;, props&lt;{movies: Movie[]}&gt;());\nexport const loadRecommendedMoviesFailure = createAction(&apos;[Recommendation API] Load movies failure&apos;, props&lt;{error: any}&gt;());\nconst actions = union({\nloadRecommendedMovies,\nloadRecommendedMoviesSuccess,\nloadRecommendedMoviesFailure\n});\nexport type ActionsUnion = typeof actions;\nIn the reducer, we use a loading flag so the UI can show a loading spinner. The store is updated with arriving data.\nrecommendation/actions/index.ts\nexport interface State {\nitems: Movie[];\nloading: boolean;\n}\nexport const initialState: State = {\nitems: [],\nloading: false\n};\nexport function reducer(state = initialState, action: recommendationActions.ActionsUnion): State {\nswitch (action.type) {\ncase &apos;[Recommendation List] Load movies&apos;:\nreturn {\n...state,\nitems: [],\nloading: true\n};\ncase &apos;[Recommendation API] Load movies failure&apos;:\nreturn {\n...state,\nloading: false\n};\ncase &apos;[Recommendation API] Load movies success&apos;:\nreturn {\n...state,\nitems: action.movies,\nloading: false\n};\ndefault:\nreturn state;\n}\n}\nexport const getAll = (state: State) =&gt; state.items;\nexport const isLoading = (state: State) =&gt; state.loading;\nWe need an API service to talk to the server. For demonstration purposes, we simulate an answer delayed by one second:\nrecommendation/services/recommendation-api.service.ts\n@Injectable({\nprovidedIn: &apos;root&apos;\n})\nexport class RecommendationApiService {\nprivate readonly recommendedMovies: Movie[] = [\n{\nid: 2,\ntitle: &apos;The Hunger Games&apos;,\ngenre: &apos;sci-fi&apos;,\nreleaseYear: 2012,\nruntimeMinutes: 144\n},\n{\nid: 4,\ntitle: &apos;Avengers: Endgame&apos;,\ngenre: &apos;fantasy&apos;,\nreleaseYear: 2019,\nruntimeMinutes: 181\n}\n];\nloadRecommendedMovies(): Observable&lt;Movie[]&gt; {\nreturn of(this.recommendedMovies).pipe(delay(1000));\n}\n}\nHere are the effects:\nrecommendation/services/recommendation-api.service.ts\n@Injectable()\nexport class RecommendationEffects {\nconstructor(\nprivate actions$: Actions,\nprivate recommendationApi: RecommendationApiService,\n) { }\n@Effect()\nloadBooks$ = this.actions$.pipe(\nofType(recommendationActions.loadRecommendedMovies.type),\nswitchMap(() =&gt; this.recommendationApi.loadRecommendedMovies().pipe(\nmap(movies =&gt; recommendationActions.loadRecommendedMoviesSuccess({ movies })),\ncatchError(error =&gt; of(recommendationActions.loadRecommendedMoviesFailure({ error })))\n))\n);\n}\nEffects are always observables and return actions. In this example, we consume the actions observable provided by NgRx and listen only for the loadRecommendedMovies actions by using the ofType operator. Using switchMap, we map to a new observable, one that loads movies and maps the successful result to a new loadRecommendedMoviesSuccess action or a failure to loadRecommendedMoviesFailure. In a real application we would show a notification in the error case.\nNote\nIf an effect should not dispatch another action, return an empty observable.\nContinue reading how to simplify CRUD (Create Read Update Delete) operations using @ngrx/entity.\n19.4. Simplifying CRUD with NgRx/Entity\nMost of the time when manipulating entries in the store, we like to create, add, update, or delete entries (CRUD). NgRx/Entity provides convenience functions if each item of a collection has an id property. Luckily all our entities already have this property.\nLet&#x2019;s add functionality to add a movie to the watchlist. First, create the required action:\nrecommendation/actions/index.ts\nexport const addToWatchlist = createAction(&apos;[Recommendation List] Add to watchlist&apos;,\nprops&lt;{ watchlistItemId: number, movie: Movie, addedAt: Date }&gt;());\nNote\nYou may wonder why the Date object is not created inside the reducer instead, since it should always be the current time. However, remember that reducers should be deterministic state machines&#x2009;&#x2014;&#x2009;State A + Action B should always result in the same State C. This makes reducers easily testable.\nThen, rewrite the watchlistData reducer to make use of NgRx/Entity:\nrecommendation/actions/index.ts\nexport interface State extends EntityState&lt;WatchlistItem&gt; { (1)\n}\nexport const entityAdapter = createEntityAdapter&lt;WatchlistItem&gt;(); (2)\nexport const initialState: State = entityAdapter.getInitialState(); (3)\nconst entitySelectors = entityAdapter.getSelectors();\nexport function reducer(state = initialState, action: playbackActions.ActionsUnion | recommendationActions.ActionsUnion): State {\nswitch (action.type) {\ncase playbackActions.playbackFinished.type:\nconst itemToUpdate = entitySelectors\n.selectAll(state) (4)\n.find(item =&gt; item.movie.id === action.movieId);\nif (itemToUpdate) {\nreturn entityAdapter.updateOne({ (5)\nid: itemToUpdate.id,\nchanges: { playbackMinutes: action.stoppedAtMinute } (6)\n}, state);\n} else {\nreturn state;\n}\ncase recommendationActions.addToWatchlist.type:\nreturn entityAdapter.addOne({id: action.watchlistItemId, movie: action.movie, added: action.addedAt, playbackMinutes: 0}, state);\ndefault:\nreturn state;\n}\n}\nexport const getAllItems = entitySelectors.selectAll;\nNgRx/Entity requires state to extend EntityState. It provides a list of ids and a dictionary of id &#x21D2; entity entries\nThe entity adapter provides data manipulation operations and selectors\nThe state can be initialized with getInitialState(), which accepts an optional object to define any additional state beyond EntityState\nselectAll returns an array of all entities\nAll adapter operations consume the state object as the last argument and produce a new state\nUpdate methods accept a partial change definition; you don&#x2019;t have to clone the object\nThis concludes the tutorial on NgRx. If you want to learn about advanced topics such as selectors with arguments, testing, or router state, head over to the official NgRx documentation.\n&#x2190;&#xA0;Previous:&#xA0;Layouts&#xA0;| &#x2191;&#xA0;Up:&#xA0;devon4ng&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Cookbook&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devon4node.asciidoc.html","title":"VI. devon4node","body":"\nVI. devon4node\ndevonfw is a platform which provides solutions to building business applications which combine best-in-class frameworks and libraries as well as industry proven practices and code conventions. devonfw is 100% Open Source (Apache License version 2.0) since the beginning of 2018.\ndevon4node is the NodeJS stack of devonfw. It allows you to build business applications (backends) using NodeJS technology in standardized way based on established best-practices.\ndevon4node is based on NestJS. Nest (NestJS) is a framework for building efficient, scalable Node.js server-side applications. It uses progressive TypeScript and combines elements of OOP (Object Oriented Programming), FP (Functional Programming), and FRP (Functional Reactive Programming).\ndevon4node Architechture\nLayers\nGuides\ndevon4node applications\n&#x2190;&#xA0;Previous:&#xA0;Samples&#xA0;| &#x2191;&#xA0;Up:&#xA0;devonfw guide&#xA0;| Next:&#xA0;devon4node Architechture&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devon4node.asciidoc_devon4node-applications.html","title":"31. devon4node applications","body":"\n31. devon4node applications\n31.1. devon4node Samples\nIn the folder /samples, you can find some devon4node examples that could be useful for you in order to understand better the framework.\nThe samples are:\nTodo\nEmployee\nComponents example\nAlso, we have another realistic example in the My Thai Star repository. This example is the implementation of My Thai Star backend, which is compatible with the frontend made with Angular. To do that, this node implementation exposes the same API as Java backend. Take care with this example, as we need to follow the Java API, some components do not follow the devon4node patterns and code conventions.\n31.1.1. Todo example\nThis example is the backend part of an TO-DO application. It exposes and API where you can create, read, update and delete a TO-DO list.\nIn order to start the application, run the following commands in the todo folder:\n$ yarn\n$ yarn build\n$ yarn start\nNow, you can acces to the application using the url http://localhost:3000/v1/todo/todos. If you want to now all endpoints exposed, you can see the swagger at: http://localhost:3000/v1/api.\nAlso, in this example we show you how to control the access to you application by implementing an authentication mechanism using JWT and rol based strategy. In order to access to the list of todos (http://localhost:3000/v1/todo/todos), first you need to call to POST http://localhost:3000/v1/auth/login and in the body you need to send the user information:\n{\n&quot;username&quot;: &quot;user&quot;,\n&quot;password&quot;: &quot;password&quot;\n}\nIt will return a JWT token for the user user. The rol of this user is USER, so you can only access to the methods GET, POST and DELETE of the endpoint http://localhost:3000/v1/todo/todos. If you login with the user admin/admin, you will be able to access to the methods UPDATE and PATCH.\n31.1.2. Employee example\nThis is an example of employee management application. With the application you can create, read, update and delete employees.\nIn order to start the application, run the following commands in the todo folder:\n$ yarn\n$ yarn build\n$ yarn start\nNow, you can acces to the application using the url http://localhost:8081/v1/employee/employees. If you want to now all endpoints exposed, you can see the swagger at: http://localhost:8081/v1/api.\nThis is a simple example without authentication. With this example you can learn how to work with database migrations. You can find them in the folder /src/migrations. The TypeORM is configured in order to execute the migrations every time that you start this application (&quot;migrationsRun&quot;: true at ormconfig.json). You can also execute the migration manually by typing the command devon4node db migration:run, or revert executing devon4node db migration:revert. Take into account that the database that this application is using is an in-memory sqlite, so every time that you stop the application all data is lost.\n31.1.3. Components example\nThis example allow you to understand better the execution order of the components of a devon4node application (guards, pipes, interceptors, filters, middlewares).\nIn order to start the application, run the following commands in the todo folder:\n$ yarn\n$ yarn build\n$ yarn start\nIn order to see the execution order, you can call to http://localhost:3000/v1. It will show you the execution order of all components except the filters. If you want to know the execution order while a filter is applied, call to the endpoint with the following queries: ?hello=error, ?hello=controller, ?hello=global.\n31.2. Create the employee sample step by step\n31.2.1. Application requisites\nThe employee application needs:\nA configuration module\nA SQLite in memory database\nSecurity: CORS\nSwagger support\nAuthentication using JWT\nCRUD for manage employees. The employees will have the following properties:\nname\nsurname\nemail\n31.2.2. Create the application\nInstall devon4node CLI\nExecute the command npm i -g @devon4node/cli\nCreate the new application\nExecute the command devon4node new employee\nThen, you need to select the components interactively. The result will be:\nWith this, you will generate the following files:\n/employee/.prettierrc\n/employee/nest-cli.json\n/employee/package.json\n/employee/README.md\n/employee/tsconfig.build.json\n/employee/tsconfig.json\n/employee/tslint.json\n/employee/src/main.ts\n/employee/test/app.e2e-spec.ts\n/employee/test/jest-e2e.json\n/employee/src/app/app.controller.spec.ts\n/employee/src/app/app.controller.ts\n/employee/src/app/app.module.ts\n/employee/src/app/app.service.ts\n/employee/src/app/core/core.module.ts\n/employee/src/app/shared/logger/winston.logger.ts\n/employee/src/app/core/configuration/configuration.module.ts\n/employee/src/app/core/configuration/model/index.ts\n/employee/src/app/core/configuration/model/types.ts\n/employee/src/app/core/configuration/services/configuration.service.spec.ts\n/employee/src/app/core/configuration/services/configuration.service.ts\n/employee/src/app/core/configuration/services/index.ts\n/employee/src/config/default.ts\n/employee/src/config/develop.ts\n/employee/src/config/production.ts\n/employee/src/config/test.ts\n/employee/src/config/uat.ts\n/employee/docker-compose.yml\n/employee/ormconfig.json\n/employee/src/app/shared/model/entities/base-entity.entity.ts\n/employee/src/app/core/auth/auth.module.ts\n/employee/src/app/core/auth/controllers/auth.controller.spec.ts\n/employee/src/app/core/auth/controllers/auth.controller.ts\n/employee/src/app/core/auth/controllers/index.ts\n/employee/src/app/core/auth/decorators/index.ts\n/employee/src/app/core/auth/decorators/roles.decorator.spec.ts\n/employee/src/app/core/auth/decorators/roles.decorator.ts\n/employee/src/app/core/auth/guards/index.ts\n/employee/src/app/core/auth/guards/roles.guard.spec.ts\n/employee/src/app/core/auth/guards/roles.guard.ts\n/employee/src/app/core/auth/model/index.ts\n/employee/src/app/core/auth/model/roles.enum.ts\n/employee/src/app/core/auth/model/user-request.interface.ts\n/employee/src/app/core/auth/services/auth.service.spec.ts\n/employee/src/app/core/auth/services/auth.service.ts\n/employee/src/app/core/auth/services/index.ts\n/employee/src/app/core/auth/strategies/index.ts\n/employee/src/app/core/auth/strategies/jwt.strategy.spec.ts\n/employee/src/app/core/auth/strategies/jwt.strategy.ts\n/employee/src/app/core/user/user.module.ts\n/employee/src/app/core/user/model/index.ts\n/employee/src/app/core/user/model/dto/user-payload.dto.ts\n/employee/src/app/core/user/model/entities/user.entity.ts\n/employee/src/app/core/user/services/index.ts\n/employee/src/app/core/user/services/user.service.spec.ts\n/employee/src/app/core/user/services/user.service.ts\n/employee/test/auth/auth.service.mock.ts\n/employee/test/user/user.repository.mock.ts\n/employee/src/app/employee/employee.module.ts\n/employee/src/app/employee/model/entities/employee.entity.ts\n/employee/src/app/employee/model/index.ts\n/employee/src/app/employee/controllers/employee.crud.controller.ts\n/employee/src/app/employee/services/employee.crud.service.ts\n/employee/src/app/employee/services/index.ts\n/employee/src/app/employee/controllers/index.ts\nOpen the VSCode\nExecute the commands:\ncd employee\ncode .\nFill in the entity: src/app/employee/model/entities/employee.entity.ts\nAdd the columns\n@Entity()\nexport class Employee extends BaseEntity {\n@Column(&apos;varchar&apos;, { length: 255, nullable: true })\nname?: string;\n@Column(&apos;varchar&apos;, { length: 255, nullable: true })\nsurname?: string;\n@Column(&apos;varchar&apos;, { length: 255, nullable: true })\nemail?: string;\n}\nAdd the validations\n@Entity()\nexport class Employee extends BaseEntity {\n@IsDefined({ groups: [CrudValidationGroups.CREATE] })\n@IsOptional({ groups: [CrudValidationGroups.UPDATE] })\n@MaxLength(255)\n@Column(&apos;varchar&apos;, { length: 255, nullable: true })\nname?: string;\n@IsDefined({ groups: [CrudValidationGroups.CREATE] })\n@IsOptional({ groups: [CrudValidationGroups.UPDATE] })\n@MaxLength(255)\n@Column(&apos;varchar&apos;, { length: 255, nullable: true })\nsurname?: string;\n@IsDefined({ groups: [CrudValidationGroups.CREATE] })\n@IsOptional({ groups: [CrudValidationGroups.UPDATE] })\n@MaxLength(255)\n@IsEmail()\n@Column(&apos;varchar&apos;, { length: 255, nullable: true })\nemail?: string;\n}\nAdd the transformations\nIn this specific case, we will not transform any property, but you can see an example in the src/app/shared/model/entities/base-entity.entity.ts file.\nexport abstract class BaseEntity {\n@PrimaryGeneratedColumn(&apos;increment&apos;)\nid!: number;\n@VersionColumn({ default: 1 })\n@Exclude({ toPlainOnly: true })\nversion!: number;\n@CreateDateColumn()\n@Exclude({ toPlainOnly: true })\ncreatedAt!: string;\n@UpdateDateColumn()\n@Exclude({ toPlainOnly: true })\nupdatedAt!: string;\n}\nAdd swagger metadata\n@Entity()\nexport class Employee extends BaseEntity {\n@ApiModelPropertyOptional()\n@IsDefined({ groups: [CrudValidationGroups.CREATE] })\n@IsOptional({ groups: [CrudValidationGroups.UPDATE] })\n@MaxLength(255)\n@Column(&apos;varchar&apos;, { length: 255, nullable: true })\nname?: string;\n@ApiModelPropertyOptional()\n@IsDefined({ groups: [CrudValidationGroups.CREATE] })\n@IsOptional({ groups: [CrudValidationGroups.UPDATE] })\n@MaxLength(255)\n@Column(&apos;varchar&apos;, { length: 255, nullable: true })\nsurname?: string;\n@ApiModelPropertyOptional()\n@IsDefined({ groups: [CrudValidationGroups.CREATE] })\n@IsOptional({ groups: [CrudValidationGroups.UPDATE] })\n@MaxLength(255)\n@IsEmail()\n@Column(&apos;varchar&apos;, { length: 255, nullable: true })\nemail?: string;\n}\nAdd swagger metadata to src/app/employee/controllers/employee.crud.controller.ts\n@ApiUseTags(&apos;employee&apos;)\nGenerate database migrations\nBuild the application: yarn build\nGenerate the tables creation migration: devon4node db migration:generate -n CreateTables\nThe optput will be something similar to:\nexport class CreateTables1572480273012 implements MigrationInterface {\nname = &apos;CreateTables1572480273012&apos;;\npublic async up(queryRunner: QueryRunner): Promise&lt;any&gt; {\nawait queryRunner.query(\n`CREATE TABLE &quot;user&quot; (&quot;id&quot; integer PRIMARY KEY AUTOINCREMENT NOT NULL, &quot;version&quot; integer NOT NULL DEFAULT (1), &quot;createdAt&quot; datetime NOT NULL DEFAULT (datetime(&apos;now&apos;)), &quot;updatedAt&quot; datetime NOT NULL DEFAULT (datetime(&apos;now&apos;)), &quot;username&quot; varchar(255) NOT NULL, &quot;password&quot; varchar(255) NOT NULL, &quot;role&quot; integer NOT NULL DEFAULT (0))`,\nundefined,\n);\nawait queryRunner.query(\n`CREATE TABLE &quot;employee&quot; (&quot;id&quot; integer PRIMARY KEY AUTOINCREMENT NOT NULL, &quot;version&quot; integer NOT NULL DEFAULT (1), &quot;createdAt&quot; datetime NOT NULL DEFAULT (datetime(&apos;now&apos;)), &quot;updatedAt&quot; datetime NOT NULL DEFAULT (datetime(&apos;now&apos;)), &quot;name&quot; varchar(255), &quot;surname&quot; varchar(255), &quot;email&quot; varchar(255))`,\nundefined,\n);\n}\npublic async down(queryRunner: QueryRunner): Promise&lt;any&gt; {\nawait queryRunner.query(`DROP TABLE &quot;employee&quot;`, undefined);\nawait queryRunner.query(`DROP TABLE &quot;user&quot;`, undefined);\n}\n}\nThe number in the name is a timestamp, so may change in your application.\nCreate a migration to insert data:`devon4node db migration:create -n InsertData`\nand fill in with the following code:\nexport class InsertData1572480830290 implements MigrationInterface {\npublic async up(queryRunner: QueryRunner): Promise&lt;any&gt; {\nawait queryRunner.query(\n`INSERT INTO EMPLOYEE(id, name, surname, email) VALUES(1, &apos;Stefano&apos;, &apos;Rossini&apos;, &apos;stefano.rossini@capgemini.com&apos;);`,\n);\nawait queryRunner.query(\n`INSERT INTO EMPLOYEE(id, name, surname, email) VALUES(2, &apos;Angelo&apos;, &apos;Muresu&apos;, &apos;angelo.muresu@capgemini.com&apos;);`,\n);\nawait queryRunner.query(\n`INSERT INTO EMPLOYEE(id, name, surname, email) VALUES(3, &apos;Jaime&apos;, &apos;Gonzalez&apos;, &apos;jaime.diaz-gonzalez@capgemini.com&apos;);`,\n);\nawait queryRunner.query(\n`INSERT INTO EMPLOYEE(id, name, surname, email) VALUES(4, &apos;Dario&apos;, &apos;Rodriguez&apos;, &apos;dario.rodriguez-gonzalez@capgemini.com&apos;);`,\n);\nawait queryRunner.query(`INSERT INTO USER(id, username, password, role) VALUES(?, ?, ?, ?);`, [\n1,\n&apos;user&apos;,\nawait hash(&apos;password&apos;, await genSalt(12)),\nroles.USER,\n]);\nawait queryRunner.query(`INSERT INTO USER(id, username, password, role) VALUES(?, ?, ?, ?);`, [\n2,\n&apos;admin&apos;,\nawait hash(&apos;admin&apos;, await genSalt(12)),\nroles.ADMIN,\n]);\n}\npublic async down(queryRunner: QueryRunner): Promise&lt;any&gt; {\nawait queryRunner.query(`DELETE FROM EMPLOYEE`);\nawait queryRunner.query(`DELETE FROM USER`);\n}\n}\nStart the application: yarn start:dev\nCheck the swagger endpoint: http://localhost:3000/v1/api\nMake petitions to the employee CRUD: http://localhost:3000/v1/employee/employees\nWrite the tests\nAs we do not create any method, only add some properties to the entity, all application must be tested by the autogenerated code. As we add some modules, you need to uncomment some lines in the src/app/core/configuration/services/configuration.service.spec.ts:\ndescribe(&apos;ConfigurationService&apos;, () =&gt; {\nconst configService: ConfigurationService = new ConfigurationService();\nit(&apos;should return the values of test config file&apos;, () =&gt; {\nexpect(configService.isDev).toStrictEqual(def.isDev);\nexpect(configService.host).toStrictEqual(def.host);\nexpect(configService.port).toStrictEqual(def.port);\nexpect(configService.clientUrl).toStrictEqual(def.clientUrl);\nexpect(configService.globalPrefix).toStrictEqual(def.globalPrefix);\n// Remove comments if you add those modules\nexpect(configService.database).toStrictEqual(def.database);\nexpect(configService.swaggerConfig).toStrictEqual(def.swaggerConfig);\nexpect(configService.jwtConfig).toStrictEqual(def.jwtConfig);\n// expect(configService.mailerConfig).toStrictEqual(def.mailerConfig);\n});\nit(&apos;should take the value of environment varible if defined&apos;, () =&gt; {\nprocess.env.isDev = &apos;true&apos;;\nprocess.env.host = &apos;notlocalhost&apos;;\nprocess.env.port = &apos;123456&apos;;\nprocess.env.clientUrl = &apos;http://theclienturl.net&apos;;\nprocess.env.globalPrefix = &apos;v2&apos;;\nprocess.env.swaggerConfig = JSON.stringify({\nswaggerTitle: &apos;Test Application&apos;,\n});\nprocess.env.database = JSON.stringify({\ntype: &apos;oracle&apos;,\ncli: { entitiesDir: &apos;src/notentitiesdir&apos; },\n});\nprocess.env.jwtConfig = JSON.stringify({ secret: &apos;NOTSECRET&apos; });\n// process.env.mailerConfig = JSON.stringify({ mailOptions: { host: &apos;notlocalhost&apos; }});\nexpect(configService.isDev).toBe(true);\nexpect(configService.host).toBe(&apos;notlocalhost&apos;);\nexpect(configService.port).toBe(123456);\nexpect(configService.clientUrl).toBe(&apos;http://theclienturl.net&apos;);\nexpect(configService.globalPrefix).toBe(&apos;v2&apos;);\nconst database: any = { ...def.database, type: &apos;oracle&apos; };\ndatabase.cli.entitiesDir = &apos;src/notentitiesdir&apos;;\nexpect(configService.database).toStrictEqual(database);\nexpect(configService.swaggerConfig).toStrictEqual({\n...def.swaggerConfig,\nswaggerTitle: &apos;Test Application&apos;,\n});\nexpect(configService.jwtConfig).toStrictEqual({\n...def.jwtConfig,\nsecret: &apos;NOTSECRET&apos;,\n});\n// const mail: any = { ...def.mailerConfig };\n// mail.mailOptions.host = &apos;notlocalhost&apos;;\n// expect(configService.mailerConfig).toStrictEqual(mail);\nprocess.env.isDev = undefined;\nprocess.env.host = undefined;\nprocess.env.port = undefined;\nprocess.env.clientUrl = undefined;\nprocess.env.globalPrefix = undefined;\nprocess.env.database = undefined;\nprocess.env.swaggerConfig = undefined;\nprocess.env.jwtConfig = undefined;\n// process.env.mailerConfig = undefined;\n});\n});\nAnd the output should be:\n&#x2190;&#xA0;Previous:&#xA0;Guides&#xA0;| &#x2191;&#xA0;Up:&#xA0;devon4node&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;devonfw shop floor&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devon4node.asciidoc_guides.html","title":"30. Guides","body":"\n30. Guides\n30.1. Key Principles\ndevon4node is built following some basic principles like:\nSOLID\nPatterns\nOpen\nBut key principles that best define devon4node (and are inherited from NestJS) are:\nSimplicity (aka KISS)\nReusability\nProductivity\n30.1.1. Simplicity\nIn devon4node we tryed to do everything as simple as possible. Following this princible we will be able to do easy to mantain applications.\nFor example, in order to expose all CRUD operations for an entity, you only need to create a controller like:\n@Crud({\nmodel: {\ntype: Employee,\n},\n})\n@CrudType(Employee)\n@Controller(&apos;employee/employees&apos;)\nexport class EmployeeCrudController {\nconstructor(public service: EmployeeCrudService) {}\n}\nYou can find this code in the employee example. Only with this code your exposing the full CRUD operations for the employee entity. As you can see, it&#x2019;s an empty class with some decorators and the EmployeeCrudService inyected as dependency. Simple, isn&#x2019;t it? The EmployeeCrudService is also simple:\n@Injectable()\nexport class EmployeeCrudService extends TypeOrmCrudService&lt;Employee&gt; {\nconstructor(@InjectRepository(Employee) repo: Repository&lt;Employee&gt;) {\nsuper(repo);\n}\n}\nAnother empty class which extends from TypeOrmCrudService&lt;Employee&gt; and injects the Employee Repository as dependency. Nothing else.\nWith these examples you can get an idea of how simple it can be to code a devon4node application .\n30.1.2. Reusability\nNestJS (and devon4node) applications are designed in a modular way. This allows you to isolate some functionality in a module, and then reuse it in every application that you need. This is the same behaviour that Angular has. You can see it in the NestJS modules like TypeORM, Swagger and others. Also, in devon4node we have the Mailer module.\nIn your applications, you only need to import those modules and then you will be able to use the functionality that they implement. Example\n@Module({\nimports: [ AuthModule, ConfigurationModule ],\n})\nexport class SomeModule {}\n30.1.3. Productivity\ndevon4node is designed to create secure enterprise applications. But also, it allow you to do it in a fast way. To increase the productivity devon4node, devon4node provide a CLI with some commands in order to generate some boilerplate code.\nFor example, to create a module you need to create a new file for a module (or copy it) and write the code, then you need to import it in the AppModule. This is a easy example, but you can introduce some errors: forget to import it in the AppModule, introduce errors with the copy/paste and so on. By using the command devon4node g module --name &lt;module-name&gt; it will do everything for you. Just a simple command. In this specific case probably you do not see any advantage, but there are other complex cases where you can generate more complex code with the devon4node command.\nSee CLI and code generation in order to know how to increase your productivity creating devon4node applications.\n30.2. devon4node CLI\ndevon4node CLI is a tool designed to manage devon4node applications in a easy way. Highly inspired by Nest CLI.\nIn this page we will explain all commands available and their arguments.\n30.2.1. Prerequisites\nNodeJS lts\nyarn\n30.2.2. devon4node\nAfter install the devon4node CLI package npm i -g @devon4node/cli, the command devon4node (or d4n) must be available in your system. This have new, generate and db subcommands and also accepts the following arguments:\nArguments\nDescription\n--help, -h\nShows help\n-v, --version\nShows version number\nExamples:\ndevon4node -h\ndevon4node new -h\nnew\ndevon4node new allows you to create new devon4node applications. It&#x2019;s an interactive command and it will ask you for everything that it need in order to create a new application.\nArguments\nDescription\n--help, -h\nShow help\n--no-interactive, -n\nExecute the command without ask anything to the user\n--dry-run\nAllow to test changes before execute command.\n--skip-git, -g\nAllow to skip git repository initialization.\n--skip-install, -s\nAllow to skip package installation.\n--version, -v\nShow version number\nExamples:\ndevon4node new my-app -sg\ndevon4node new my-app -n\ngenerate\nThis command allows you to generate code into your application. It receive he name of the schematic that will generate the code.\nArguments\nDescription\n--help, -h\nShow help\n--interactive, -i\nGenerate code using the interactive mode (same as new command).\n--skip-install, -s\nAllow to skip package installation.\n--dry-run, -d\nAllow to test changes before execute command.\n--path, -p\nPath to project.\nExamples:\ndevon4node generate -i\ndevon4node generate service --name my-service\napplication\nCreate a devon4node application. It is used by the new command.\nArguments\nDescription\n--dry-run, -d\nAllow to test changes before execute command.\n--path,\nPath to project.\n--name, -n\nThe name of the application.\nangular-app\nCreate a new Angular application. Inherit from Nest CLI\nArguments\nDescription\n--dry-run, -d\nAllow to test changes before execute command.\n--path,\nPath to project.\n--initApp\nFlag to skip the angular application generation.\n--name, -n\nThe name of the application.\nclass\nCreate a new class.Inherit from Nest CLI\nArguments\nDescription\n--dry-run, -d\nAllow to test changes before execute command.\n--path, -p\nThe path to create the class.\n--name, -n\nThe name of the class.\n--flat\nFlag to indicate if a directory is created.\n--spec\nSpecifies if a spec file is generated.\n--language\nNest class language (ts/js).\n--sourceRoot\nNest controller source root directory.\ncontroller\nCreate a Nest controller.\nArguments\nDescription\n--dry-run, -d\nAllow to test changes before execute command.\n--path, -p\nThe path to create the controller.\n--name, -n\nThe name of the controller. To create a controller with name Banana in the module fruits you need to introduce fruits/banana\n--spec\nSpecifies if a spec file is generated.\ndecorator\nCreate a Nest decorator. Inherit from Nest CLI\nArguments\nDescription\n--dry-run, -d\nAllow to test changes before execute command.\n--path, -p\nThe path to create the decorator.\n--name, -n\nThe name of the decorator.\n--language\nNest decorator language (ts/js).\n--sourceRoot\nNest decorator source root directory.\n--flat\nFlag to indicate if a directory is created.\nfilter\nCreate a Nest filter.\nArguments\nDescription\n--dry-run, -d\nAllow to test changes before execute command.\n--path, -p\nThe path to create the filter.\n--name, -n\nThe name of the filter. To create a filter with name Banana in the module fruits you need to introduce fruits/banana\n--language\nNest filter language (ts/js).\n--sourceRoot\nNest filter source root directory.\n--flat\nFlag to indicate if a directory is created.\n--spec\nSpecifies if a spec file is generated.\ngateway\nCreate a Nest gateway. Inherit from Nest CLI\nArguments\nDescription\n--dry-run, -d\nAllow to test changes before execute command.\n--path, -p\nThe path to create the gateway.\n--name, -n\nThe name of the gateway.\n--language\nNest gateway language (ts/js).\n--sourceRoot\nNest gateway source root directory.\n--flat\nFlag to indicate if a directory is created.\n--spec\nSpecifies if a spec file is generated.\nguard\nCreate a Nest guard.\nArguments\nDescription\n--dry-run, -d\nAllow to test changes before execute command.\n--path, -p\nThe path to create the guard.\n--name, -n\nThe name of the guard. To create a guard with name Banana in the module fruits you need to introduce fruits/banana\n--language\nNest guard language (ts/js).\n--sourceRoot\nNest guard source root directory.\n--flat\nFlag to indicate if a directory is created.\n--spec\nSpecifies if a spec file is generated.\ninterceptor\nCreate a Nest interceptor.\nArguments\nDescription\n--dry-run, -d\nAllow to test changes before execute command.\n--path, -p\nThe path to create the interceptor.\n--name, -n\nThe name of the interceptor. To create an interceptor with name Banana in the module fruits you need to introduce fruits/banana\n--language\nNest interceptor language (ts/js).\n--sourceRoot\nNest interceptor source root directory.\n--flat\nFlag to indicate if a directory is created.\n--spec\nSpecifies if a spec file is generated.\ninterface\nCreate a Nest interface. Inherit from Nest CLI\nArguments\nDescription\n--dry-run, -d\nAllow to test changes before execute command.\n--path, -p\nThe path to create the interface.\n--name, -n\nThe name of the interface.\n--sourceRoot\nNest interface source root directory\n--flat\nFlag to indicate if a directory is created.\nmiddleware\nCreate a Nest middleware.\nArguments\nDescription\n--dry-run, -d\nAllow to test changes before execute command.\n--path, -p\nThe path to create the middleware.\n--name, -n\nThe name of the middleware. To create a middleware with name Banana in the module fruits you need to introduce fruits/banana\n--language\nNest middleware language (ts/js).\n--sourceRoot\nNest middleware source root directory.\n--flat\nFlag to indicate if a directory is created.\n--spec\nSpecifies if a spec file is generated.\nmodule\nCreate a Nest module.\nArguments\nDescription\n--help, -h\nShows help\n--dry-run, -d\nAllow to test changes before execute command.\n--path, -p\nThe path to create the module.\n--name, -n\nThe name of the module. To create a module named module-b as a submodule of module-a, you need to introduce module-a/module-b\n--module\nThe path to import the module.\n--language\nNest module language (ts/js).\n--sourceRoot\nNest module source root directory.\n--skipImport\nFlag to skip the module import.\npipe\nCreate a Nest pipe.\nArguments\nDescription\n--help, -h\nShows help\n--dry-run, -d\nAllow to test changes before execute command.\n--path, -p\nThe path to create the pipe.\n--name, -n\nThe name of the pipe. To create a pipe with name Banana in the module fruits you need to introduce fruits/banana\n--language\nNest pipe language (ts/js).\n--sourceRoot\nNest pipe source root directory.\n--flat\nFlag to indicate if a directory is created.\n--spec\nSpecifies if a spec file is generated.\nprovider\nCreate a Nest provider. Inherit from Nest CLI\nArguments\nDescription\n--help, -h\nShows help\n--dry-run, -d\nAllow to test changes before execute command.\n--path, -p\nThe path to create the provider.\n--name, -n\nThe name of the provider.\n--language\nNest provider language (ts/js).\n--sourceRoot\nNest provider source root directory.\n--flat\nFlag to indicate if a directory is created.\n--spec\nSpecifies if a spec file is generated.\nservice\nCreate a Nest service.\nArguments\nDescription\n--help, -h\nShows help\n--dry-run, -d\nAllow to test changes before execute command.\n--path, -p\nThe path to create the service.\n--name, -n\nThe name of the service.\n--spec\nSpecifies if a spec file is generated. To create a service with name Banana in the module fruits you need to introduce fruits/banana\nresolver\nCreate a Nest resolver. Inherit from Nest CLI\nArguments\nDescription\n--help, -h\nShows help\n--dry-run, -d\nAllow to test changes before execute command.\n--path, -p\nThe path to create the resolver.\n--name, -n\nThe name of the resolver.\n--language\nNest resolver language (ts/js).\n--sourceRoot\nNest resolver source root directory.\n--flat\nFlag to indicate if a directory is created.\n--spec\nSpecifies if a spec file is generated.\nconfiguration\nCreate a Nest CLI configuration. Inherit from Nest CLI\nArguments\nDescription\n--help, -h\nShows help\n--dry-run, -d\nAllow to test changes before execute command.\n--path,\nPath to project.\nlibrary\nCreate a Nest library (mono-repo). Inherit from Nest CLI\nArguments\nDescription\n--help, -h\nShows help\n--dry-run, -d\nAllow to test changes before execute command.\n--path, -p\nThe path to create the library.\n--name, -n\nThe name of the library.\n--prefix\nThe prefix of the library.\n--language\nNest library language.\n--rootDir\nThe libraries root directory.\nsub-app\nCreate a Nest application (mono-repo). Inherit from Nest CLI\nArguments\nDescription\n--help, -h\nShows help\n--dry-run, -d\nAllow to test changes before execute command.\n--path, -p\nThe path to create the application.\n--name, -n\nThe name of the application.\n--language\nNest application language.\n--rootDir\nApplications root directory.\ntypeorm\nInitialice typeorm into your current project in a correct way.\nArguments\nDescription\n--help, -h\nShows help\n--dry-run, -d\nAllow to test changes before execute command.\n--path, -p\nPath to project.\n--db\nDatabase type.\nentity\nAdd a TypeOrm entity to your project. Requires TypeORM installed in the project.\nArguments\nDescription\n--help, -h\nShows help\n--dry-run, -d\nAllow to test changes before execute command.\n--path, -p\nChange the application folder where you will create the entity\n--name, -n\nThe entity name. To create a entity with name Banana in the module fruits you need to introduce fruits/banana\nconfig-module\nAdd the config module to the project.\nIt will add the @devon4node/common module as a project dependency. Then, it will generate the configuration module into your project and add it in the core module. Also, it generates the config files for the most common environments.\nArguments\nDescription\n--help, -h\nShows help\n--dry-run, -d\nAllow to test changes before execute command.\n--path, -p\nPath to project.\ncrud\nGenerate CRUD methods for a entity. Requires TypeORM installed in the project.\nIt will add the @nestjsx/crud module as a project dependency. Then, generates an entity, a CRUD controller and a CRUD service. It also register the entity, controller and service in the module.\nArguments\nDescription\n--help, -h\nShows help\n--dry-run, -d\nAllow to test changes before execute command.\n--path, -p\nChange the application folder where you will create the crud\n--name, -n\nThe crud name. To create crud with name Banana in the module fruits you need to introduce fruits/banana\nmailer\nAdd @devon4node/mailer module to project.\nIt will add the @devon4node/mailer module as a project dependency. Also, it will add it to the core module and it will generate some email template examples.\nArguments\nDescription\n--help, -h\nShows help\n--dry-run, -d\nAllow to test changes before execute command.\n--path, -p\nPath to project.\nswagger\nAdd swagger module to project.\nIt will add the @nestjs/swagger module as a project dependency. Also, it will update the main.ts file in order to expose the endpoint for swagger. The default endpoint is: /v1/api\nArguments\nDescription\n--help, -h\nShows help\n--dry-run, -d\nAllow to test changes before execute command.\n--path, -p\nPath to project.\nauth-jwt\nAdd the auth JWT module to the project.\nIt will add to your project the auth-jwt and user module. Also, it will import those modules into the core module.\nArguments\nDescription\n--help, -h\nShows help\n--dry-run, -d\nAllow to test changes before execute command.\n--path, -p\nPath to project.\nall-in-one\nExecute multiple schematics at the same time.\nThis schematic is used by the interactive mode.\nArguments\nDescription\n--help, -h\nShows help\n--dry-run, -d\nAllow to test changes before execute command.\n--path, -p\nPath to config file\nsecurity\nAdd cors and helmet to your project.\nIt will add helmet package as project dependency and update the main.ts file in order to enable the cors and helmet in your application.\nArguments\nDescription\n--help, -h\nShows help\n--dry-run, -d\nAllow to test changes before execute command.\n--path, -p\nPath to project.\ndb\nExecute a database command. Same as typeorm CLI.\nschema:sync\nSynchronizes your entities with database schema. It runs schema update queries on all connections you have. To run update queries on a concrete connection use -c option.\nArguments\nDescription\n--help, -h\nShows help\n--connection, -c\nName of the connection on which schema synchronization needs to to run.\n--config, -f\nName of the file with connection configuration.\n--version, -v\nShows number version\nschema:log\nShows sql to be executed by schema:sync command. It shows sql log only for your default connection. To run update queries on a concrete connection use -c option.\nArguments\nDescription\n--help, -h\nShows help\n--connection, -c\nName of the connection on which schema synchronization needs to to run.\n--config, -f\nName of the file with connection configuration.\n--version, -v\nShows number version\nschema:drop\nDrops all tables in the database on your default connection. To drop table of a concrete connection&#x2019;s database use -c option.\nArguments\nDescription\n--help, -h\nShows help\n--connection, -c\nName of the connection on which schema synchronization needs to to run.\n--config, -f\nName of the file with connection configuration.\n--version, -v\nShows number version\nquery\nExecutes given SQL query on a default connection. Specify connection name to run query on a specific connection.\nArguments\nDescription\n--help, -h\nShows help\n--connection, -c\nName of the connection on which schema synchronization needs to to run.\n--config, -f\nName of the file with connection configuration.\n--version, -v\nShows number version\nentity:create\nGenerates a new entity.\nArguments\nDescription\n--help, -h\nShows help\n--connection, -c\nName of the connection on which to run a query\n--name, -n\nName of the entity class.\n--dir\nDirectory where entity should be created.\n--config, -f\nName of the file with connection configuration.\n--version, -v\nShows number version\nsubscriber:create\nGenerates a new subscriber.\nArguments\nDescription\n--help, -h\nShows help\n--connection, -c\nName of the connection on which to run a query\n--name, -n\nName of the entity class.\n--dir\nDirectory where entity should be created.\n--config, -f\nName of the file with connection configuration.\n--version, -v\nShows number version\nmigration:create\nCreates a new migration file.\nArguments\nDescription\n--help, -h\nShows help\n--connection, -c\nName of the connection on which to run a query\n--name, -n\nName of the entity class.\n--dir\nDirectory where entity should be created.\n--config, -f\nName of the file with connection configuration.\n--version, -v\nShows number version\nmigration:generate\nGenerates a new migration file with sql needs to be executed to update schema.\nArguments\nDescription\n--help, -h\nShows help\n--connection, -c\nName of the connection on which to run a query\n--name, -n\nName of the entity class.\n--dir\nDirectory where entity should be created.\n--config, -f\nName of the file with connection configuration.\n--version, -v\nShows number version\nmigration:run\nRuns all pending migrations.\nArguments\nDescription\n--help, -h\nShows help\n--connection, -c\nName of the connection on which run a query.\n--transaction, -t\nIndicates if transaction should be used or not for migration run. Enabled by default.\n--config, -f\nName of the file with connection configuration.\n--version, -v\nShows number version\nmigration:show\nShow all migrations and whether they have been run or not\nArguments\nDescription\n--help, -h\nShows help\n--connection, -c\nName of the connection on which run a query.\n--config, -f\nName of the file with connection configuration.\n--version, -v\nShows number version\nmigration:revert\nReverts last executed migration.\nArguments\nDescription\n--help, -h\nShows help\n--connection, -c\nName of the connection on which run a query.\n--transaction, -t\nIndicates if transaction should be used or not for migration revert. Enabled by default.\n--config, -f\nName of the file with connection configuration.\n--version, -v\nShows number version\nversion\nPrints TypeORM version this project uses.\nArguments\nDescription\n--help, -h\nShows help\n--version, -v\nShows number version\ncache:clear\nClears all data stored in query runner cache.\nArguments\nDescription\n--help, -h\nShows help\n--connection, -c\nName of the connection on which run a query.\n--config, -f\nName of the file with connection configuration.\n--version, -v\nShows number version\n30.2.3. Command autocompletion\nIn order to enable the command autocompletion, you need to add the following code to your .bashrc or .zshrc:\n_yargs_completions()\n{\nlocal cur_word args type_list\ncur_word=&quot;${COMP_WORDS[COMP_CWORD]}&quot;\nargs=(&quot;${COMP_WORDS[@]}&quot;)\n# ask yargs to generate completions.\ntype_list=$(devon4node --get-yargs-completions &quot;${args[@]}&quot;)\nCOMPREPLY=( $(compgen -W &quot;${type_list}&quot; -- ${cur_word}) )\n# if no match was found, fall back to filename completion\nif [ ${#COMPREPLY[@]} -eq 0 ]; then\nCOMPREPLY=()\nfi\nreturn 0\n}\ncomplete -o default -F _yargs_completions devon4node\n30.3. Code Generation\nAs we mention in the page key principles, one of our key principles is Productivity. In order to provide that productivity, we have some tools to generate code. These tools will help you generate the common parts of the application so that you can focus only on the specific functionality.\nThose tools are:\ndevon4node CLI\nCobiGen\n30.3.1. Differences between tools\nAs you may have noticed, we have two tools to generate code, instead of having just one. What is the reason? The reason is that each tool starts from a different input. devon4node CLI receives user input, while cobigen receives code already exists and generates the rest accordingly. So, in order to increase the productivity, we can combine the use of both.\n30.3.2. devon4node CLI\nThe description of devon4node CLI is discussed on his page. Anyway, in this page we will explain how to use it with the interactive mode.\nNew application\nTo create a new devon4node application, you just need to type the command devon4node new\nThe interactive mode will request you some input. First, you need to choose the general components of the application. Those are described in the devon4node CLI page.\nIf you choose database, then you can select the database that you want:\nThen, you can choose to create application modules or not.\nIf yes, you need introduce the module name:\nAnd then, select the components\nand names\nIt will generate some files:\nCREATE /test-app/.prettierrc (51 bytes)\nCREATE /test-app/nest-cli.json (67 bytes)\nCREATE /test-app/package.json (2358 bytes)\nCREATE /test-app/README.md (3370 bytes)\nCREATE /test-app/tsconfig.build.json (97 bytes)\nCREATE /test-app/tsconfig.json (568 bytes)\nCREATE /test-app/tslint.json (426 bytes)\nCREATE /test-app/src/main.ts (1033 bytes)\nCREATE /test-app/test/app.e2e-spec.ts (561 bytes)\nCREATE /test-app/test/jest-e2e.json (183 bytes)\nCREATE /test-app/src/app/app.controller.spec.ts (617 bytes)\nCREATE /test-app/src/app/app.controller.ts (274 bytes)\nCREATE /test-app/src/app/app.module.ts (371 bytes)\nCREATE /test-app/src/app/app.service.ts (142 bytes)\nCREATE /test-app/src/app/core/core.module.ts (1047 bytes)\nCREATE /test-app/src/app/shared/logger/winston.logger.ts (1081 bytes)\nCREATE /test-app/src/app/core/configuration/configuration.module.ts (214 bytes)\nCREATE /test-app/src/app/core/configuration/model/index.ts (24 bytes)\nCREATE /test-app/src/app/core/configuration/model/types.ts (280 bytes)\nCREATE /test-app/src/app/core/configuration/services/configuration.service.spec.ts (2950 bytes)\nCREATE /test-app/src/app/core/configuration/services/configuration.service.ts (1078 bytes)\nCREATE /test-app/src/app/core/configuration/services/index.ts (42 bytes)\nCREATE /test-app/src/config/default.ts (322 bytes)\nCREATE /test-app/src/config/develop.ts (817 bytes)\nCREATE /test-app/src/config/production.ts (817 bytes)\nCREATE /test-app/src/config/test.ts (816 bytes)\nCREATE /test-app/src/config/uat.ts (817 bytes)\nCREATE /test-app/docker-compose.yml (25 bytes)\nCREATE /test-app/ormconfig.json (467 bytes)\nCREATE /test-app/src/app/shared/model/entities/base-entity.entity.ts (517 bytes)\nCREATE /test-app/src/app/core/auth/auth.module.ts (897 bytes)\nCREATE /test-app/src/app/core/auth/controllers/auth.controller.spec.ts (3086 bytes)\nCREATE /test-app/src/app/core/auth/controllers/auth.controller.ts (1087 bytes)\nCREATE /test-app/src/app/core/auth/controllers/index.ts (36 bytes)\nCREATE /test-app/src/app/core/auth/decorators/index.ts (36 bytes)\nCREATE /test-app/src/app/core/auth/decorators/roles.decorator.spec.ts (870 bytes)\nCREATE /test-app/src/app/core/auth/decorators/roles.decorator.ts (124 bytes)\nCREATE /test-app/src/app/core/auth/guards/index.ts (32 bytes)\nCREATE /test-app/src/app/core/auth/guards/roles.guard.spec.ts (2354 bytes)\nCREATE /test-app/src/app/core/auth/guards/roles.guard.ts (712 bytes)\nCREATE /test-app/src/app/core/auth/model/index.ts (74 bytes)\nCREATE /test-app/src/app/core/auth/model/roles.enum.ts (39 bytes)\nCREATE /test-app/src/app/core/auth/model/user-request.interface.ts (140 bytes)\nCREATE /test-app/src/app/core/auth/services/auth.service.spec.ts (3777 bytes)\nCREATE /test-app/src/app/core/auth/services/auth.service.ts (1076 bytes)\nCREATE /test-app/src/app/core/auth/services/index.ts (33 bytes)\nCREATE /test-app/src/app/core/auth/strategies/index.ts (33 bytes)\nCREATE /test-app/src/app/core/auth/strategies/jwt.strategy.spec.ts (713 bytes)\nCREATE /test-app/src/app/core/auth/strategies/jwt.strategy.ts (618 bytes)\nCREATE /test-app/src/app/core/user/user.module.ts (338 bytes)\nCREATE /test-app/src/app/core/user/model/index.ts (82 bytes)\nCREATE /test-app/src/app/core/user/model/dto/user-payload.dto.ts (188 bytes)\nCREATE /test-app/src/app/core/user/model/entities/user.entity.ts (542 bytes)\nCREATE /test-app/src/app/core/user/services/index.ts (33 bytes)\nCREATE /test-app/src/app/core/user/services/user.service.spec.ts (2127 bytes)\nCREATE /test-app/src/app/core/user/services/user.service.ts (1065 bytes)\nCREATE /test-app/test/auth/auth.service.mock.ts (948 bytes)\nCREATE /test-app/test/user/user.repository.mock.ts (1095 bytes)\nCREATE /test-app/src/app/test/test.module.ts (469 bytes)\nCREATE /test-app/src/app/test/model/entities/test.entity.ts (174 bytes)\nCREATE /test-app/src/app/test/model/index.ts (88 bytes)\nCREATE /test-app/src/app/test/controllers/test.crud.controller.ts (393 bytes)\nCREATE /test-app/src/app/test/services/test.crud.service.ts (394 bytes)\nCREATE /test-app/src/app/test/services/index.ts (69 bytes)\nCREATE /test-app/src/app/test/controllers/index.ts (75 bytes)\nCREATE /test-app/src/app/test/controllers/test.controller.spec.ts (496 bytes)\nCREATE /test-app/src/app/test/controllers/test.controller.ts (107 bytes)\nCREATE /test-app/src/app/test/services/test.service.spec.ts (464 bytes)\nCREATE /test-app/src/app/test/services/test.service.ts (92 bytes)\nCREATE /test-app/src/app/test/model/entities/another-test.entity.ts (181 bytes)\nThen, install all dependencies, format the code and initialize git.\nGenerate more components\nThe generate command has also an interactive mode. To execute generate command in interactive mode, you only need to change to an existing application and then execute the command devon4node generate -i\nThe functionality is the same as new command, but the difference is generate command do not generate the base application.\nGenerate database migrations\nThe last command that we have in order to generate command is devon4node db migration:generate -n MigrationName. It will connect to the database, read all entities and then it will generate a migration file with all sql queries need to transform the current status of the database to the status defined by the entities. If the database is empty, it will generate all sql queries need to create all tables defined in the entities. You can find a exmple in the todo example\n30.3.3. CobiGen\nCurrently, we do not have templates to generate devon4node code (we have planned to do that in the future). Instead, we have templates that read the code of a devon4node application and generate a devon4ng application. Visit the CobiGen page for more information.\n30.4. Coding Conventions\ndevon4node defines some coding conventions in order to improve the readability, reduce the merge conflicts and be able to develop applications in an industrialized way.\nIn order to ensure that you are following the devon4node coding conventions, you can use the following tools:\nTSLint: TSLint is an extensible static analysis tool that checks TypeScript code for readability, maintainability, and functionality errors. We recommend to use the TSLint VSCode extension (included in the devonfw Platform Extension Pack) in order to be able to see the linting error while you are developing.\nPrettier: Prettier is a code formatter. We recommend to use the Prettier VSCode extension (included in the devonfw Platform Extension Pack) and enable the editor.formatOnSave option.\ndevon4node CLI: this tool will generate code follwing the devon4node coding conventions. Also, when you generate a new project using the devon4node CLI, it generates the configuration files for TSLint and Prettier that satisfy the devon4node coding conventions.\nWhen you combine all tools, you can be sure that you follow the devon4node coding conventions.\n30.4.1. Detailed devon4node Coding Conventions\nHere we will detail some of most important devon4node coding conventions. To be sure that you follows all devon4node coding conventions use the tools described before.\nIndentation\nAll devon4node code files must be indented using spaces. The indentation with must be 2 spaces.\nWhite space\nIn order to improve the readability of your code, you must introduce whitespaces. Example:\nif(condition){\nmust be\nif (condition) {\nNaming conventions\nFile naming\nThe file name must follow the pattern: (name in kebap case).(kind of component).(extension)\nThe test file name must follow the pattern: (name in kebap case).(kind of component).spec.(extension)\nExample:\nauth-jwt.service.ts\nauth-jwt.service.spec.ts\nInterface naming\nThe interface names must be in pascal case, and must start with I. There is some controversy in starting the interface names with an I, but we decided to do it because is most of cases you will have an interface and a class with the same name, so, to differentiate them, we decided to start the interfaces with I. Other devonfw stacks solves it by adding the suffix Impl in the class implementations.\nExample:\ninterface ICoffee {}\nClass naming\nThe class names must be in pascal case.\nExample:\nclass Coffee {}\nVariable naming\nAll variable names must be in camel case.\nconst coffeeList: Coffe[];\nDeclarations\nFor all variable declarations we must use const or let. var is forbidden. We preffer to use const when possible.\nProgramming practices\nTrailing comma\nAll statements must end with a trailing comma. Example:\n{\none: &apos;one&apos;,\ntwo: &apos;two&apos; // bad\n}\n{\none: &apos;one&apos;,\ntwo: &apos;two&apos;, // good\n}\nArrow functions\nAll anonymous functions must be definned with the arrow function notation. In most of cases it&#x2019;s not a problem, but sometimes, when you do not want to bind this when you define the function, you can use the other function definition. In this special cases you must disable the linter for those sentence.\nComments\nComments must start with a whitespace. Example:\n//This is a bad comment\n// This is OK\nQuotemarks\nFor string definitions, we must use single quotes.\nif statements\nIn all if statements you always must use brackets. Example:\n// Bad if statement\nif (condition)\nreturn true;\n// Good if statement\nif (condition) {\nreturn true;\n}\n30.5. Dependency Injection\nThe dependency injection is a well-known common design pattern applied by frameworks in all languages, like Spring in Java, Angular and others. The intention of this page is not to explain how dependency injection works, but instead how it is addressed by NestJS.\nNestJS resolve the dependency injection in their modules. When you define a provider in a module, it can be injected in all components of the module. By default, those providers are only available in the module where it is defined. The only way to export a module provider to other modules which import it is adding those provider to the export array. You can also reexport modules.\n30.5.1. Inject dependencies in NestJS\nIn oder to inject a dependency in a NestJS component, you need to declare it in the component constructor. Example:\nexport class CoffeeController {\nconstructor(public readonly conffeeService: CoffeeService) {}\n}\nNestJS can resolve all dependencies that are defined in the module as provider, and also the dependencies exported by the modules imported. Example:\n@Module({\ncontrollers: [CoffeeController],\nproviders: [CoffeeService],\n})\nexport class CoffeeModule {}\nInject dependencies in the constructor is the is the preferred choice, but, sometimes it is not possible. For example, when you are extending another class and want to keep the constructor definition. In this specific cases we can inject dependencies in the class properties. Example:\nexport class CoffeeController {\n@Inject(CoffeeService)\nprivate readonly conffeeService: CoffeeService;\n}\n30.5.2. Dependency Graph\nIn the previous image, the Module A can inject dependencies exported by Module B, Module E and Module F. If module B reexport Module C and Module D, they are also accesibles by Module A.\nIf there is a conflict with the injection token, it resolves the provider with less distance with the module. For example: if the modules C and F exports a UserService provider, the Module A will resolve the UserService exported by the Module F, because the distance from Module A to Module F is 1, and the distance from Module A to Module C is 2.\nWhen you define a module as global, the dependency injection system is the same. The only difference is now all modules as a link to the global module. For example, if we make the Module C as global the dependency graph will be:\n30.5.3. Custom providers\nWhen you want to change the provider name, you can use a NestJS feature called custom providers. For example, if you want to define a provider called MockUserService with the provider token UserService you can define it like:\n@Module({\nproviders: [{\nprovide: UserService,\nuseValue: MockUserService,\n}],\n})\nWith this, when you inject want to inject UserService as dependency, the MockUserService will be injected.\nCustom provider token can be also a string:\n@Module({\nproviders: [{\nprovide: &apos;USER_SERVICE&apos;,\nuseValue: MockUserService,\n}],\n})\nbut now, when you want to inject it as dependency you need to use the @Inject decorator.\nconstructor(@Inject(&apos;USER_SERVICE&apos;) userService: any) {}\n30.6. Configuration Module\ndevon4node provides a way to generate a configuration module inside your appication. To generate it you only need to execute the command devon4node generate config-module. This command will generate inside your application:\nConfiguration module inside the core module.\nconfig folder where all environment configuration are stored.\ndefault configuration: configuration for your local development environment.\ndevelop environment configuration for the develop environment.\nuat environment configuration for the uat environment.\nproduction environment configuration for the production environment.\nproduction environment configuration for the production environment.\ntest environment configuration used by test.\nNote\nsome code generators will add some properties to this module, so, be sure that the config module is the first module that you generate in your application.\n30.6.1. Use the configuration service\nTo use the configuration service, you only need to inject it as dependency. As configuration module is defined in the core module, it will be available everywhere in your application. Example:\nexport class MyProvider {\nconstructor(public readonly configService: ConfigurationService) {}\nmyMethod() {\nreturn this.confiService.isDev;\n}\n}\n30.6.2. Choose an environment file\nBy default, when you use the configuration service it will take the properties defined in the default.ts file. If you want to change the configuration file, you only need to set the NODE_ENV environment property with the name of the desired environment. Examples: in windows execute set NODE_ENV=develop before executing the application, in linux execute NODE_ENV=develop before executing the application or NODE_ENV=develop yarn start.\n30.6.3. Override configuration properties\nSometimes, you want to keep some configuration property secure, and you do not want to publish it to the repository, or you want to reuse some configuration file but you need to change some properties. For those scenarios, you can override configuration properties by defining a environment variable with the same name. For example, if you want to override the property host, you can do: set host=&quot;newhost&quot;. It also works with objects. For example, if you want to change the value of secret in the property jwtConfig for this example, you can set a environment variable like this: set jwtConfig=&quot;{&quot;secret&quot;: &quot;newsecret&quot;}&quot;. As you can see, this environment variable has a JSON value. It will take object and merge the jwtConfig property with the properties defined inside the environment variable. It other properties maintain their value. The behaviour is the same for the nested objects.\n30.6.4. Add a configuration property\nIn order to add a new property to the configuration module, you need to follow some steps:\nAdd the property to IConfig interface in src/app/core/configuration/types.ts file. With this, we can ensure that the ConfigurationService and the environment files has those property at compiling time.\nAdd the new property getter to ConfigurationService. You must use the get method of ConfigurationService to ensure that the property will be loaded from the desired config file. You can also add extra logic if needed.\nAdd the property to all config files inside the src/config folder.\nExample:\nWe want to add the property devonfwUrl to our ConfigurationService, so:\nWe add the following code in IConfig interface:\ndevonfwUrl: string;\nThen, we add the getter in the ConfigurationService:\nget devonfwUrl(): string {\nreturn this.get(&apos;devonfwUrl&apos;)!;\n}\nFinally, we add the definition in all config files:\ndevonfwUrl: &apos;https://devonfw.com&apos;,\n30.7. Auth JWT module\ndevon4node provides a way to generate a default authentication module using JWT (JSON Web Token). It uses the @nestjs/passport library descrbe here.\nTo generate the devon4node auth-jwt module you only need to execute the command: devon4node generate auth-jwt. We generate this module inside the applications instead of distributing a npm package because this module is prone to be modified depending on the requirements. It alse generate a basic user module.\nIn this page we will explain the default implementation provided by devon4node. For more information about authentication, JWT, passport and other you can see:\nJWT\nNestJS authentication\nPassport\nPassport JWT\n30.7.1. Auth JWT endpoints\nIn order to execute authentication operations, the auth-jwt module exposes the following endpoints:\nPOST /auth/login: receive an username and a password and return the token in the header if the combination of username and password is correct.\nPOST /auth/register: register a new user.\nGET /auth/currentuser: return the user data if he is authenticated.\n30.7.2. Protect endpoints with auth-jwt\nIn order to protect your endpoints with auth-jwt module you only need to add the AuthGuard() in the UseGuards decorator. Example:\n@Get(&apos;currentuser&apos;)\n@UseGuards(AuthGuard())\ncurrentUser(@Request() req: UserRequest) {\nreturn req.user;\n}\nNow, all request to currentuser are protected by the AuthGuard.\n30.7.3. Role based Access Control\nThe auth-jwt module provides also a way to control the access to some endpoints by using roles. For example, if you want to grant access to a endpoint only to admins, you only need to add the Roles decorator to those endpoints with the roles allowed. Example:\n@Get(&apos;currentuser&apos;)\n@UseGuards(AuthGuard())\n@Roles(roles.ADMIN)\ncurrentUser(@Request() req: UserRequest) {\nreturn req.user;\n}\n30.8. Swagger\nWe can use swagger (OpenAPI) in order to describe the endpoints that our application exposes.\nNestJS provides a module which will read the code of our application and will expose one endpoint where we can see the swagger.\nAdd swagger to a devon4node application is simple, you only need to execute the command devon4node generate swagger and it will do everything for you. The next time that you start your application, you will be able to see the swagger at /v1/api endpoint.\nThe swagger module can read your code in order to create the swagger definition, but sometimes you need to help him by decorating your handlers.\nFor more information about decorators and other behaviours about swagger module, you can see the NestJS swagger documentation page\nNote\nthe OpenAPI specification that this module will emmit is v2.0. The OpenAPI v3.0 is not available yet by using this module.\n30.9. TypeORM\nTypeORM is the default ORM provided by devon4node. It supports MySQL, MariaDB, Postgres, CockroachDB, SQLite, Microsoft SQL Server, Oracle, sql.js relational dabases and also supports MongoDB NoSQL database.\nAdd TypeORM support to a devon4node application is very easy: you only need to execute the command devon4node generate typeorm and it will add all required dependencies to the project and also imports the @nestjs/typeorm module.\nFor more information about TypeORM and the integration with NestJS you can visit TypeORM webpage, TypeORM GitHub repository and NestJS TypeORM documentation page\n30.9.1. Configuration\nWhen you have the configuration module, the typeorm generator will add one property in order to be able to configure the database depending on the environment. Example:\ndatabase: {\ntype: &apos;sqlite&apos;,\ndatabase: &apos;:memory:&apos;,\nsynchronize: false,\nmigrationsRun: true,\nlogging: true,\nentities: [&apos;dist/**/*.entity.js&apos;],\nmigrations: [&apos;dist/migration/**/*.js&apos;],\nsubscribers: [&apos;dist/subscriber/**/*.js&apos;],\ncli: {\nentitiesDir: &apos;src/entity&apos;,\nmigrationsDir: &apos;src/migration&apos;,\nsubscribersDir: &apos;src/subscriber&apos;,\n},\n},\nThis object is a TypeORM ConnectionOptions. For fore information about it visit the TypeORM Connection Options page.\nThere is also a special case: the default configuration. As the devon4node CLI need the database configuration when you use the devon4node db command, we also provide the ormconfig.json file. In this file you must put the configuration for you local environment. In order to do not have duplicated the configuration for local environment, in the default config file the database property is setted like:\ndatabase: require(&apos;../../ormconfig.json&apos;),\nSo, you only need to maintain the ormconfig.json file for the local environment.\n30.9.2. Entity\nEntity is a class that maps to a database table. The devon4node CLI has a generator to create new entities. You only need to execute the command devon4node generate entity --name &lt;module-name&gt;/&lt;entity-name&gt; and it generate the entity.\nIn the entity, you must define all columns, relations, primary keys of your database table. By default, devon4node provides a class named BaseEntity. All entities created with the devon4node CLI will extends the BaseEntity. This entity provides you some common columns:\nid: the primary key of you table\nversion: the version of the entry (used for auditing purposes)\ncreatedAt: creation date of the entry (used for auditing purposes)\nupdatedAt: last update date of the entry (used for auditing purposes)\nFor more information about Entities, please visit the TypeORM entities page\n30.9.3. Repository\nWith repositories, you can manage (insert, update, delete, load, etc.) a concrete entity. Using this pattern, we have separated the data (Entities) from the methods to manage it (Repositories).\nTo use a repository you only need to:\nImport it in the module as follows:\n@Module({\nimports: [TypeOrmModule.forFeature([Employee])],\n})\nNote\nif you generate the entities with the devon4node CLI, this step is not neccesary, devon4node CLI do it for you.\nInject the repository as dependency in your service:\nconstructor(@InjectRepository(Employee) employeeRepository: Repository&lt;Employee&gt;) {}\nYou can see more details in the NestJS database and NestJS TypeORM documentation pages.\n30.10. Serializer\nSerialization is the process of translating data structures or object state into a format that can be transmitted across network and reconstructed later.\nNestJS by default serialize all data to JSON (JSON.stringify). Sometimes it is not enought. In some situations you need to exlude some property (e.g password). Instead doing it manually, devon4node provides an interceptor (ClassSerializerInterceptor) that will do it for you. You only need to return a class instance as always and the interceptor will transform those class to the expected data.\nThe ClassSerializerInterceptor takes the class-transformer decorators in order to know how to transform the class and then send the result to the client.\nSome of class-transformer decorators are:\nExpose\nExclude\nType\nTransform\nAnd methods to transform data:\nplainToClass\nplainToClassFromExist\nclassToPlain\nclassToClass\nserialize\ndeserialize\ndeserializeArray\nSee the class-transformer page for more information.\nSee NestJS serialization page for more information about ClassSerializerInterceptor.\n30.11. Validation\nTo be sure that your application will works well, you must validate any input data. devon4node by default provides a ValidationPipe. This ValidationPipe is the responsible of validate the request input and, if the input do not pass the validation process, it returns a 400 Bad Request error.\n30.11.1. Defining Validators\nThe ValidationPipe needs to know how to validate the input. For that purpose we use the class-validator package. This package allows you to define the validation of a class by using decorators.\nFor example:\nexport class Coffee {\n@IsDefined()\n@IsString()\n@MaxLength(255)\nname: string;\n@IsDefined()\n@IsString()\n@MaxLength(25)\ntype: string;\n@IsDefined()\n@IsNumber()\nquantity: number;\n}\nAs you can see in the previous example, we used some decorators in order to define the validators for every property of the Coffee class. You can find all decorators in the class-validator github repository.\nNow, when you want to receive a Coffee as input in some endpoint, it will execute the validations before executing the handler function.\nNote\nIn order to be able to use the class-validator package, you must use classes instead of interfaces. As you know interfaces disappear at compiling time, and class-validator need to know the metadata of the properties in order to be able to validate.\nNote\nThe ValidationPipe only works if you put a specific type in the handler definition. For example, if you define a handler like getCoffee(@Body() coffee: any): Coffee {} the ValidationPipe will not do anything. You must specify the type of the input: getCoffee(@Body() coffee: Coffee): Coffee {}\n30.12. Logger\nWhen you create a new devon4node application, it already has a logger: src/app/shared/logger/winston.logger.ts. This logger provide the methods log, error and warn. All of those methods will write a log message, but with a different log level.\nThe winston logger has two transports: one to log everything inside the file logs/general.log and the other to log only the error logs inside the file logs/error.log. In addition, it uses the default NestJS logger in order to show the logs in the console.\nAs you can see it is a simple example about how to use logger in a devon4node application. It will be update to a complex one in the next versions.\n30.12.1. How to use logger\nIn order to use the logger you only need to inject the logger as a dependecy:\nconstructor(logger: WinstonLogger){}\nand then use it\nasync getAll() {\nthis.service.getAll();\nthis.logger.log(&apos;Returing all data&apos;);\n}\n30.13. Mailer Module\nThis module enables you to send emails in devon4node. It also provides a template engine using Handlebars.\nIt is a NestJS module that inject into your application a MailerService, which is the responsible to send the emails using the nodemailer library.\n30.13.1. Installing\nExecute the following command in a devon4node project:\nyarn add @devon4node/mailer\n30.13.2. Configuring\nTo configure the mailer module, you only need to import it in your application into another module. Example:\n@Module({\n...\nimports: [\nMailerModule.forRoot(),\n],\n...\n})\nYour must pass the configuration using the forRoot or forRootAsync methods.\nforRoot()\nThe forRoot method recives an MailerModuleOptions object as parameter. It configures the MailerModule using the input MailerModuleOptions object.\nThe structure of MailerModuleOptions is:\n{\nhbsOptions?: {\ntemplatesDir: string;\nextension?: string;\npartialsDir?: string;\nhelpers?: IHelperFunction[];\ncompilerOptions?: ICompileOptions;\n},\nmailOptions?: nodemailerSmtpTransportOptions;\nemailFrom: string;\n}\nHere, you need to specify the Handlebars compile options, the nodemailer transport options and the email address which will send the emails.\nThen, you need to call to forRoot function in the module imports. Example:\n@Module({\n...\nimports: [\nMailerModule.forRoot({\nmailOptions: {\nhost: &apos;localhost&apos;,\nport: 1025,\nsecure: false,\ntls: {\nrejectUnauthorized: false,\n},\n},\nemailFrom: &apos;noreply@capgemini.com&apos;,\nhbsOptions: {\ntemplatesDir: join(__dirname, &apos;../..&apos;, &apos;templates/views&apos;),\npartialsDir: join(__dirname, &apos;../..&apos;, &apos;templates/partials&apos;),\nhelpers: [{\nname: &apos;fullname&apos;,\nfunc: person =&gt; `${person.name} ${person.surname}`,s\n}],\n},\n}),\n...\n})\nforRootAsync()\nThe method forRootAsync enables you to get the mailer configuration in a asynchronous way. It is usefull when you need to get the configuration using, for example, a service (e.g. ConfigurationService).\nExample:\n@Module({\n...\nimports: [\nMailerModule.forRootAsync({\nimports: [ConfigurationModule],\nuseFactory: (config: ConfigurationService) =&gt; {\nreturn config.mailerConfig;\n},\ninject: [ConfigurationService],\n}),\n...\n})\nIn this example, we use the ConfigurationService in order to get the MailerModuleOptions (the same as forRoot)\n30.13.3. Usage\nIn order to use, you only need to inject using the dependency injection the MailerService.\nExample:\n@Injectable()\nexport class CatsService {\nconstructor(private readonly mailer: MailerService) {}\n}\nThen, you only need to use the methods provided by the MailerService in your service. Take into account that you can inject it in every place that support NestJS dependency injection.\nMailerService methods\nsendPlainMail\nThe method sendPlainMail recive a string sends a email.\nThe method signatures are:\nsendPlainMail(emailOptions: SendMailOptions): Promise&lt;SentMessageInfo&gt;;\nsendPlainMail(to: string, subject: string, mail: string): Promise&lt;SentMessageInfo&gt;;\nExamples:\nthis.mailer.sendPlainMail({\nto: &apos;example@example.com&apos;,\nsubject: &apos;This is a subject&apos;,\nhtml: &apos;&lt;h1&gt;Hello world&lt;/h1&gt;&apos;\n});\nthis.mailer.sendPlainMail(&apos;example@example.com&apos;, &apos;This is a subject&apos;, &apos;&lt;h1&gt;Hello world&lt;/h1&gt;&apos;);\nsendTemplateMail\nThe method sendTemplateMail sends a email based on a Handlebars template. The templates are registered using the templatesDir option or using the addTemplate method.\nThe template name is the name of the template (without extension) or the first parameter of the method addTemplate.\nThe method signatures are:\nsendTemplateMail(emailOptions: SendMailOptions, templateName: string, emailData: any, hbsOptions?: RuntimeOptions): Promise&lt;SentMessageInfo&gt;;\nsendTemplateMail(to: string, subject: string, templateName: string, emailData: any, hbsOptions?: RuntimeOptions): Promise&lt;SentMessageInfo&gt;;\nExamples:\nthis.mailer.sendTemplateMail({\nto: &apos;example@example.com&apos;,\nsubject: &apos;This is a subject&apos;,\nhtml: &apos;&lt;h1&gt;Hello world&lt;/h1&gt;&apos;\n}, &apos;template1&apos;, { person: {name: &apos;Dario&apos;, surname: &apos;Rodriguez&apos;}});\nthis.mailer.sendTemplateMail(&apos;example@example.com&apos;, &apos;This is a subject&apos;, &apos;template1&apos;, { person: {name: &apos;Dario&apos;, surname: &apos;Rodriguez&apos;}});\naddTemplate\nAdds a new template to the MailerService.\nMehotd signature:\naddTemplate(name: string, template: string, options?: CompileOptions): void;\nExample:\nthis.mailer.addTemplate(&apos;newTemplate&apos;, &apos;&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;{{&gt;partial1}}&lt;/body&gt;&lt;/html&gt;&apos;)\nregisterPartial\nRegister a new partial in Handlebars.\nMehotd signature:\nregisterPartial(name: string, partial: Handlebars.Template&lt;any&gt;): void;\nExample:\nthis.mailer.registerPartial(&apos;partial&apos;, &apos;&lt;h1&gt;Hello World&lt;/h1&gt;&apos;)\nregisterHelper\nRegister a new helper in Handlebars.\nMehotd signature:\nregisterHelper(name: string, helper: Handlebars.HelperDelegate): void;\nExample:\nthis.mailer.registerHelper(&apos;fullname&apos;, person =&gt; `${person.name} ${person.surname}`)\n30.13.4. Handlebars templates\nAs mentioned above, this module allow you to use Handlebars as template engine, but it is optional. If you do not need the Handlebars, you just need to keep the hbsOptions undefined.\nIn order to get the templates form the file system, you can specify the template folder, the partials folder and the helpers.\nAt the moment of module initialization, it will read the content of the template folder, and will register every file with the name (without extension) and the content as Handlebars template. It will do the same for the partials.\nYou can specify the extension of template files using the extension parameter. The default value is .handlebars\n30.13.5. Local development\nIf you want to work with this module but you dont have a SMTP server, you can use the streamTransport. Example:\n{\nmailOptions: {\nstreamTransport: true,\nnewline: &apos;windows&apos;,\n},\nemailFrom: ...\nhbsOptions: ...\n}\nThen, you need to get the sendPlainMail or sendTemplateMail result, and print the email to the extandart output (STDOUT). Example:\nconst mail = await this.mailer.sendTemplateMail(...);\nmail.message.pipe(process.stdout);\n&#x2190;&#xA0;Previous:&#xA0;Layers&#xA0;| &#x2191;&#xA0;Up:&#xA0;devon4node&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;devon4node applications&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devon4node.asciidoc_layers.html","title":"29. Layers","body":"\n29. Layers\n29.1. Controller Layer\nThe controller layer is responsible for handling the requests/responses to the client. This layer knows everything about the endpoints exposed, the expected input (and also validate it), the response schema, the HTTP codes for the reponse and the HTTP errors that every endpoint can send.\n29.1.1. How to implement the controller layer\nThis layer is implemented by the NestJS controllers. Let&#x2019;s see how it works with an example:\n@Controller(&apos;coffee/coffees&apos;)\nexport class CoffeeController {\nconstructor(private readonly coffeeService: CoffeeService) {}\n@Post(&apos;search&apos;)\n@HttpCode(200)\nasync searchCoffees(@Body() search: CoffeeSearch): Promise&lt;Array&lt;Coffe&gt;&gt; {\ntry {\nreturn await this.coffeeService.searchCoffees(search);\n} catch (error) {\nthrow new BadRequestException(error.message, error);\n}\n}\n}\nAs you can see in the example, to create a controller you only need to decorate a class with the Controller decorator. This example is handling all request to coffee/coffees.\nAlso, you have defined one handler. This handler is listening to POST request for the route coffee/coffees/search. In addition, this handler is waiting for a CoffeeSearch object and returns an array of Coffee. In order to keep it simple, that&#x2019;s all that you need in order to define one route.\nOne important thing that can be observed in this example is that there is no business logic. It delegates to the service layer and return the response to the client. At this point, transformations from the value that you receive from the service layer to the desired return type are also allowed.\nBy default, every POST handler return an HTTP 204 response with the returned value as body, but you can change it in a easy way by using decorators. As you can see in the example, the handler will return a HTTP 200 response (@HttpCode(200)).\nFinally, if the service layer throws an error, this handler will catch it and return a HTTP 400 Bad Request response. The controller layer is the only one that knows about the answers to the client, therefore it is the only one that knows which error codes should be sent.\n29.1.2. Validation\nIn order to do not propagate errors in the incoming payload, we need to validate all data in the controller layer. See the validation guide for more information.\n29.1.3. Error handling\nIn the previous example, we catch all errors using the try/catch statement. This is not the usual implementation. In order to catch properly the errors you must use the exception filters. Example:\n@Controller(&apos;coffee/coffees&apos;)\nexport class CoffeeController {\nconstructor(private readonly coffeeService: CoffeeService) {}\n@Post(&apos;search&apos;)\n@HttpCode(200)\n@UseFilters(CaffeExceptionFilter)\nasync searchCoffees(@Body() search: CoffeeSearch): Promise&lt;Array&lt;Coffe&gt;&gt; {\nreturn await this.coffeeService.searchCoffees(search);\n}\n}\n29.2. Service Layer\nThe logic layer is the heart of the application and contains the main business logic. It knows everything about the business logic, but it does not know about the response to the client and the HTTP errors. That&#x2019;s why this layer is separated from the controller layer.\n29.2.1. How to implement the service layer\nThis layer is implemented by services, a specific kind of providers. Let&#x2019;s see one example:\n@Injectable()\nexport class CoffeeService {\nconstructor(private readonly coffeeService: CoffeeService) {}\nasync searchCoffees(@InjectRepository(Coffee) coffeRepository: Repository&lt;Coffee&gt;): Promise&lt;Array&lt;Coffe&gt;&gt; {\nconst coffees = this.coffeRepository.find();\nreturn doSomeBusinessLogic(coffees);\n}\n}\nThis is the CoffeeService that we inject in the example of controller layer. As you can see, a service is a regular class with the Injectable decorator. Also, it inject as dependency the data access layer (in this specific case, the Repository&lt;Coffee&gt;).\nThe services, exposes methods in order to transform the input from the controllers by appling some business logic. They can also request data from the data access layer. And that&#x2019;s all.\n29.3. Data Access Layer\nThe data access layer is responsible for all outgoing connections to access and process data. This is mainly about accessing data from a persistent data-store but also about invoking external services.\nThis layer is implemented using providers. Those providers could be: services, repositories and others. Although services can be used for this layer, they should not be confused with the service layer. Services in this layer are responsible for data access, while services in the service layer are responsible for business logic.\n29.3.1. Database\nWe strongly recommend link:Although services can be used for this layer, they should not be confused with the service layer. Services in this layer are responsible for data access, while services in the service layer are responsible for business logic. TypeORM for database management in devon4node applications. TypeORM supports the most commonly used relational databases, link Oracle, MySQL, MariaDB, Postgres, SQLite, MSSQL and others. Also, it supports no-relational databases like MongoDB.\nTypeORM supports Active Record and Repository patterns. We recommend to use the Repository pattern. This pattern allows you to separate the data objects from the methods to manipulate the database.\n29.3.2. External APIs\nIn order to manage the data in a external API, you need to create a service for that purpose. In order to manage the connections with the external API, we strongly recommend the NestJS HTTP module\n&#x2190;&#xA0;Previous:&#xA0;devon4node Architechture&#xA0;| &#x2191;&#xA0;Up:&#xA0;devon4node&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Guides&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devonfw-shop-floor.asciidoc.html","title":"VII. devonfw shop floor","body":"\nVII. devonfw shop floor\nWhat is devonfw shop floor?\nHow to use it\nProvisioning environments\nConfiguration and services integration\nCreate project\nDeployment environments\nMonitoring\nAnnexes\n&#x2190;&#xA0;Previous:&#xA0;devon4node applications&#xA0;| &#x2191;&#xA0;Up:&#xA0;devonfw guide&#xA0;| Next:&#xA0;What is devonfw shop floor?&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devonfw-shop-floor.asciidoc_annexes.html","title":"39. Annexes","body":"\n39. Annexes\n39.1. BitBucket\n[Under construction]\nThe purpose of the present document is to provide the basic steps carried out to setup a BitBucket server in OpenShift.\nIntroduction\nBitBucket is the Atlassian tool that extends the Git functionality, by adding integration with JIRA, Confluence, or Trello, as well as incorporates extra features for security or management of user accounts (See BitBucket).\nBitBucket server is the Atlassian tool that runs the BitBucket services (See BitBucket server).\nThe followed approach has been not using command line, but OpenShift Web Console, by deploying the Docker image atlassian/bitbucket-server (available in Docker Hub) in the existing project Deployment.\nThe procedure below exposed consists basically in three main steps:\nDeploy the BitBucket server image (from OpenShift web console)\nAdd a route for the external traffic (from OpenShift web console)\nConfigure the BitBucket server (from BitBucket server web console)\nPrerequisites\nOpenShift up &amp; running\nAtlassian account (with personal account key). Not required for OpenShift, but for the initial BitBucket server configuration.\nProcedure\n]\n=== Step 0: Log into our OpenShift Web console\nStep 1: Get into Development project\nStep 2.1: Deploy a new image to the project\nStep 2.2: Introduce the image name (available in Docker Hub) and search\nImage name: atlassian/bitbucket-server\nStep 2.3: Leave by the moment the default config. since it is enough for the basic setup. Press Create\nStep 2.4: Copy the oc commands in case it is required to work via command line, and Go to overview\nStep 2.5: Wait until OpenShift deploys and starts up the image. All the info will be available.\nPlease notice that there are no pre-configured routes, hence the application is not accessible from outside the cluster.\nStep 3: Create a route in order for the application to be accessible from outside the cluster (external traffic). Press Create\nPlease notice that there are different fields that can be specified (hostname, port). If required, the value of those fields can be modified later.\nLeave by the moment the default config. as it is enough for the basic setup.\nThe route for external traffic is now available.\nNow the BitBucker server container is up &amp; running in our cluster.\nThe below steps correspond to the basic configuration of our BitBucket server.\nStep 4.1: Click on the link of the external traffic route. This will open our BitBucket server setup wizard\nStep 4.2: Leave by the moment the Internal database since it is enough for the basic setup (and it can be modified later), and click Next\nStep 4.3: Select the evaluation license, and click I have an account\nStep 4.4: Select the option Bitbucker (Server)\nStep 4.5: Introduce your organization (Capgemini), and click Generate License\nStep 4.6: Confirm that you want to install the license on the BitBucket server\nThe license key will be automatically generated. Click Next\nStep 4.7: Introduce the details of the Administration account.\nSince our BitBucket server is not going to be integrated with JIRA, click on Go to Bitbucket. The integration with JIRA can be configured later.\nStep 4.8: Log in with the admin account that has been just created\nDONE !!\n[Under construction]\nThe purpose of the present document is to provide the basic steps carried out to improve the configuration of BitBucket server in OpenShift.\nThe improved configuration consists on:\nPersistent Volume Claims\nHealth Checks (pending to be completed)\nPersistent Volume Claims.\nPlease notice that the BitBucket server container does not use persistent volume claims by default, which means that the data (e.g.: BitBucket server config.) will be lost from one deployment to another.\nIt is very important to create a persistent volume claim in order to prevent the mentioned loss of data.\nStep 1: Add storage\nStep 2: Select the appropriate storage, or create it from scratch if necessary\nStep 3: Introduce the required information\nPath as it is specified in the BitBucket server Docker image (/var/atlassian/application-data/bitbucket)\nVolume name with a unique name to clearly identify the volume\nThe change will be inmediately applied\n39.2. Basic Selenium Grid setup in OpenShift\n[Under construction]\nThe purpose of the present document is to provide the basic steps carried out to setup a Selenium Grid (Hub + Nodes) in OpenShift.\n39.2.1. Introduction\nSelenium is a tool to automate web browser across many platforms. It allows the automation of the testing in many different browsers, operating systems, programing laguages, or testing frameworks. (for further information pelase see Selenium)\nSelenium Grid is the platform provided by Selenium in order to perform the execution of tests in parallel and in a distributed way.\nIt basically consists on a Selenium Server (also known as hub or simply server) which redirects the requests it receives to the appropriate node (Firefox node, Chrome node, &#x2026;&#x200B;) depending on how the Selenium WebDriver is configured or implemented (See Selenium Doc.)\nAdditional documentacion:\nhttps://www.tutorialspoint.com/selenium/selenium_grids.htm\nhttp://www.softwaretestinghelp.com/selenium-ide-download-and-installation-selenium-tutorial-2\nhttps://examples.javacodegeeks.com/enterprise-java/selenium/selenium-standalone-server-example\nhttps://tripleqa.com/2016/09/26/hello-world-selenium\nhttp://queirozf.com/entries/selenium-hello-world-style-tutorial\n39.2.2. Prerequisites\nOpenShift up &amp; running\n39.2.3. Procedure\nThe present procedure is divided into two different main parts:\n* First part: Selenium Hub (server) installation\n* Second part: Selenium node installation (Firefox &amp; Chrome)\n* Create persistent volumes for the hub and the node(s)\nSelenium Hub installation\nThe followed approach consists on deploying new image from the OpenShift WenConsole.\nThe image as well as its documentation and details can be found at Selenium Hub Docker Image\nStep 1: Deploy Image\nStep 2: Image Name\nAs it is specified in the documentation (selenium/hub)\n(Please notice that, as it is described in the additional documentation of the above links, the server will run by default on 4444 port)\nStep 3: Introduce the appropriate resource name\n(selenium-hub in this case)\n(No additional config. is required by the moment)\nOnce the image is deployed, you will be able to check &amp; review the config. of the container. Please notice by, by default, no route is created for external traffic, hence the application (the selenium server or hub) is not reachable from outside the cluster\nStep 4: Create a route for external traffic\nStep 5: Change the default config. if necessary\nDONE !!\nThe Selenium Server is now accesible from outside the cluster. Click on the link of the route and you will be able to see the server home page.\nconsole/view config to see the default server config.\nPlease notice that the server is not detecting any node up &amp; running, since we have not yet installed none of them.\nSelenium Node Firefox installation\n(Same steps apply for Selenium Node Chrome with the selenium/node-chrome Docker image)\nThe key point of the nodes installation is to specify the host name and port of the hub. If this step is not correctly done, the container will be setup but the application will not run.\nThe followed approach consists on deploying new image from the OpenShift WenConsole.\nThe image as well as its documentation and details can be found at Selenium Hub Docker Image (firefox node in this case)\nStep 1: Deploy Image\nIntroduce the appropriate Docker Image name as it is specified in the documentation (selenium/node-firefox)\nStep 2: Introduce the appropriate resource name\n(selenium-node-firefox in this case)\nStep 3: Introduce, as environment variables, the host name and port of the selenium hub previously created\nEnv. var. for selenium hub host name\nName: HUB_PORT_4444_TCP_ADDR\nValue: The Selenium hub host name. It&#x2019;s recommended to use the service name of the internal OpenShift service.\nEnv. var. for host selenium hub host port\nName: HUB_PORT_4444_TCP_PORT\nValue: 4444 (by default), or the appropriate one if it was changed during the installation.\nDONE !!\nIf the creation of the container was correct, we will be able to see our new selenium-node-firefox application up &amp; running, as well as we will be able to see that the firefox node has correctly detected the selenium hub (in the log of the POD)\nIf we go back to the configuration of the SeleniumHub through the WebConsole, we also will be able to see the our new firefox node\nPersistent Volumes\nLast part of the installation of the Selenium Grid consists on creating persistent volumes for both, the hub container and the node container.\nPersistent Volumes can be easely created folling the the BitBucket Extra server configuration\n39.3. Mirabaud CICD Environment Setup\nInitial requirements:\nOS: RHEL 6.5\nRemote setup in CI machine (located in the Netherlands)\n- Jenkins\n- Nexus\n- GitLab\n- Mattermost\n- Atlassian Crucible\n- SonarQube\n39.3.1. 1. Install Docker and Docker Compose in RHEL 6.5\nDocker\nDue to that OS version, the only way to have Docker running in the CI machine is by installing it from the EPEL repository (Extra Packages for Enterprise Linux).\nAdd EPEL\n# rpm -iUvh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm\nInstall docker.io from that repository\n# yum -y install docker-io\nStart Docker daemon\n# service docker start\nCheck the installation\n# docker -v\nDocker version 1.7.1, build 786b29d/1.7.1\nDocker Compose\nDownload and install it via curl. It will use this site.\n# curl -L https://github.com/docker/compose/releases/download/1.5.0/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose\n# chmod +x /usr/local/bin/docker-compose\nAdd it to your sudo path:\nFind out where it is:\n# echo $PATH\nCopy the docker-compose file from /usr/local/bin/ to your sudo PATH.\n# docker-compose -v\ndocker-compose version 1.5.2, build 7240ff3\n39.3.2. 2. Directories structure\nSeveral directories had been added to organize some files related to docker (like docker-compose.yml) and docker volumes for each service. Here&#x2019;s how it looks:\n/home\n/[username]\n/jenkins\n/volumes\n/jenkins_home\n/sonarqube\n/volumes\n/conf\n/data\n/extensions\n/lib\n/bundled-plugins\n/nexus\n/volumes\n/nexus-data\n/crucible\n/volumes\n/\n/gitlab\ndocker-compose.yml\n/volumes\n/etc\n/gitlab\n/var\n/log\n/opt\n/mattermost\ndocker-compose.yml\n/volumes\n/db\n/var\n/lib\n/postgresql\n/data\n/app\n/mattermost\n/config\n/data\n/logs\n/web\n/cert\n39.3.3. 3. CICD Services with Docker\nSome naming conventions had been followed as naming containers as mirabaud_[service].\nSeveral folders have been created to store each service&#x2019;s volumes, docker-compose.yml(s), extra configuration settings and so on:\nJenkins\nCommand\n# docker run -d -p 8080:8080 -p 50000:50000 --name=mirabaud_jenkins \\\n-v /home/[username]/jenkins/volumes/jenkins_home:/var/jenkins_home \\\njenkins\nGenerate keystore\nkeytool -importkeystore -srckeystore server.p12 -srcstoretype pkcs12 -srcalias 1 -destkeystore newserver.jks -deststoretype jks -destalias server\nStart jekins with SSL (TODO: make a docker-compose.yml for this):\nsudo docker run -d --name mirabaud_jenkins -v /jenkins:/var/jenkins_home -p 8080:8443 jenkins --httpPort=-1 --httpsPort=8443 --httpsKeyStore=/var/jenkins_home/certs/keystore.jks --httpsKeyStorePassword=Mirabaud2017\nVolumes\nvolumes/jenkins_home:/var/jenkins_home\nSonarQube\nCommand\n# docker run -d -p 9000:9000 -p 9092:9092 --name=mirabaud_sonarqube \\\n-v /home/[username]/sonarqube/volumes/conf:/opt/sonarqube/conf \\\n-v /home/[username]/sonarqube/volumes/data:/opt/sonarqube/data \\\n-v /home/[username]/sonarqube/volumes/extensions:/opt/sonarqube/extensions \\\n-v /home/[username]/sonarqube/volumes/lib/bundled-plugins:/opt/sonarqube//lib/bundled-plugins \\\nsonarqube\nVolumes\nvolumes/conf:/opt/sonarqube/conf\nvolumes/data:/opt/sonarqube/data\nvolumes/extensions:/opt/sonarqube/extensions\nvolumes/lib/bundled-plugins:/opt/sonarqube/lib/bundled-plugins\nNexus\nCommand\n# docker run -d -p 8081:8081 --name=mirabaud_nexus\\\n-v /home/[username]/nexus/nexus-data:/sonatype-work\nsonatype/nexus\nVolumes\nvolumes/nexus-data/:/sonatype-work\nAtlassian Crucible\nCommand\n# docker run -d -p 8084:8080 --name=mirabaud_crucible \\\n-v /home/[username]/crucible/volumes/data:/atlassian/data/crucible\nmswinarski/atlassian-crucible:latest\nVolumes\nvolumes/data:/atlassian/data/crucible\n39.3.4. 4. CICD Services with Docker Compose\nBoth Services had been deploying by using the # docker-compose up -d command from their root directories (/gitlab and /mattermost). The syntax of the two docker-compose.yml files is the one corresponding with the 1st version (due to the docker-compose v1.5).\nGitLab\ndocker-compose.yml\nmirabaud:\nimage: &apos;gitlab/gitlab-ce:latest&apos;\nrestart: always\nports:\n- &apos;8888:80&apos;\nvolumes:\n- &apos;/home/[username]/gitlab/volumes/etc/gilab:/etc/gitlab&apos;\n- &apos;/home/[username]/gitlab/volumes/var/log:/var/log/gitlab&apos;\n- &apos;/home/[username]/gitlab/volumes/var/opt:/var/opt/gitlab&apos;\nCommand (docker)\ndocker run -d -p 8888:80 --name=mirabaud_gitlab \\\n-v /home/[username]/gitlab/volumes/etc/gitlab/:/etc/gitlab \\\n-v /home/[username]/gitlab/volumes/var/log:/var/log/gitlab \\\n-v /home/[username]/gitlab/volumes/var/opt:/var/opt/gitlab \\\ngitlab/gitlab-ce\nVolumes\nvolumes/etc/gitlab:/etc/gitlab\nvolumes/var/opt:/var/log/gitlab\nvolumes/var/log:/var/log/gitlab\nMattermost\ndocker-compose.yml:\ndb:\nimage: mattermost/mattermost-prod-db\nrestart: unless-stopped\nvolumes:\n- ./volumes/db/var/lib/postgresql/data:/var/lib/postgresql/data\n- /etc/localtime:/etc/localtime:ro\nenvironment:\n- POSTGRES_USER=mmuser\n- POSTGRES_PASSWORD=mmuser_password\n- POSTGRES_DB=mattermost\napp:\nimage: mattermost/mattermost-prod-app\nlinks:\n- db:db\nrestart: unless-stopped\nvolumes:\n- ./volumes/app/mattermost/config:/mattermost/config:rw\n- ./volumes/app/mattermost/data:/mattermost/data:rw\n- ./volumes/app/mattermost/logs:/mattermost/logs:rw\n- /etc/localtime:/etc/localtime:ro\nenvironment:\n- MM_USERNAME=mmuser\n- MM_PASSWORD=mmuser_password\n- MM_DBNAME=mattermost\nweb:\nimage: mattermost/mattermost-prod-web\nports:\n- &quot;8088:80&quot;\n- &quot;8089:443&quot;\nlinks:\n- app:app\nrestart: unless-stopped\nvolumes:\n- ./volumes/web/cert:/cert:ro\n- /etc/localtime:/etc/localtime:ro\nSSL Certificate\nHow to generate the certificates:\nGet the crt and key from CA or generate a new one self-signed. Then:\n// 1. create the p12 keystore\n# openssl pkcs12 -export -in cert.crt -inkey mycert.key -out certkeystore.p12\n// 2. export the pem certificate with password\n# openssl pkcs12 -in certkeystore.p12 -out cert.pem\n// 3. export the pem certificate without password\n# openssl rsa -in cert.pem -out key-no-password.pem\nSSL:\nCopy the cert and the key without password at:\n./volumes/web/cert/cert.pem\nand\n./volumes/web/cert/key-no-password.pem\nRestart the server and the SSL should be enabled at port 8089 using HTTPS.\nVolumes\n-- db --\nvolumes/db/var/lib/postgresql/data:/var/lib/postgresql/data\n/etc/localtime:/etc/localtime:ro # absolute path\n-- app --\nvolumes/app/mattermost/config:/mattermost/config:rw\nvolumes/app/mattermost/data:/mattermost/data:rw\nvolumes/app/mattermost/logs:/mattermost/logs:rw\n/etc/localtime:/etc/localtime:ro # absolute path\n-- web --\nvolumes/web/cert:/cert:ro\n/etc/localtime:/etc/localtime:ro # absolute path\n39.3.5. 5. Service Integration\nAll integrations had been done following CICD Services Integration guides:\nJenkins - Nexus integration\nJenkins - GitLab integration\nJenkins - SonarQube integration\nNote\nThese guides may be obsolete. You can find here the official configuration guides,\n39.3.6. Jenkins - GitLab integration\nThe first step to have a Continuous Integration system for your development is to make sure that all your changes to your team&#x2019;s remote repository are evaluated by the time they are pushed. That usually implies the usage of so-called webhooks. You&#x2019;ll find a fancy explanation about what Webhooks are in here.\nTo resume what we&#x2019;re doing here, we are going to prepare our Jenkins and our GitLab so when a developer pushes some changes to the GitLab repository, a pipeline in Jenkins gets triggered. Just like that, in an automatic way.\n1. Jenkins GitLab plugin\nAs it usually happens, some Jenkins plug-in(s) must be installed. In this case, let&#x2019;s install those related with GitLab:\n2. GitLab API Token\nTo communicate with GitLab from Jenkins, we will need to create an authentication token from your GitLab user settings. A good practice for this would be to create it from a machine user. Something like (i.e.) devonfw-ci/******.\nSimply by adding a name to it and a date for it expire is enough:\nAs GitLab said, you should make sure you don&#x2019;t lose your token. Otherwise you would need to create a new one.\nThis will allow Jenkins to connect with right permissions to our GitLab server.\n3. Create &quot;GitLab API&quot; Token credentials\nThose credentials will use that token already generated in GitLab to connect once we declare the GitLab server in the Global Jenkins configuration. Obviously, those credentials must be GitLab API token-like.\nThen, we add the generated token in the API token field:\nLook in your Global credentials if they had been correctly created:\n4. Create GitLab connection in Jenkins\nSpecify a GitLab connection in your Jenkins&#x2019;s Manage Jenkins &gt; Configure System configuration. This will tell Jenkins where is our GitLab server, a user to access it from and so on.\nYou&#x2019;ll need to give it a name, for example, related with what this GitLab is dedicated for (specific clients, internal projects&#x2026;&#x200B;). Then, the Gitlab host URL is just where your GitLab server is. If you have it locally, that field should look similar to:\nConnection name: my-local-gitlab\nGitlab host URL: http://localhost:${PORT_NUMBER}\nFinally, we select our recently GitLab API token as credentials.\n5. Jenkins Pipeline changes\n5.1 Choose GitLab connection in Pipeline&#x2019;s General configuration\nFirst, our pipeline should allow us to add a GitLab connection to connect to (the already created one).\nIn the case of the local example, could be like this:\nGitLab connection: my-local-gitlab\nGitLab Repository Name: myusername/webhook-test (for example)\n5.2 Create a Build Trigger\nYou should already see your GitLab project&#x2019;s URL (as you stated in the General settings of the Pipeline).\nWrite .*build.* in the comment for triggering a build\nSpecify or filter the branch of your repo you want use as target. That means, whenever a git action is done to that branch (for example, master), this Pipeline is going to be built.\nGenerate a Secret token (to be added in the yet-to-be-created GitLab webhook).\n6. GitLab Webhook\nGo to you GitLab project&#x2019;s Settings &gt; Integration section.\nAdd the path to your Jenkins Pipeline. Make sure you add project instead of job in the path.\nPaste the generated Secret token of your Jenkins pipeline\nSelect your git action that will trigger the build.\n7. Results\nAfter all those steps you should have a result similar to this in your Pipeline:\nEnjoy the Continuous Integration! :)\n39.3.7. Jenkins - Nexus integration\nNexus is used to both host dependencies for devonfw projects to download (common Maven ones, custom ones such as ojdb and even devonfw so-far-IP modules). Moreover, it will host our projects&apos; build artifacts (.jar, .war, &#x2026;&#x200B;) and expose them for us to download, wget and so on. A team should have a bidirectional relation with its Nexus repository.\n1. Jenkins credentials to access Nexus\nBy default, when Nexus is installed, it contains 3 user credentials for different purposes. The admin ones look like this: admin/admin123. There are also other 2: deployment/deployment123 and TODO.\n// ADD USER TABLE IMAGE FROM NEXUS\nIn this case, let&#x2019;s use the ones with the greater permissions: admin/admin123.\nGo to Credentials &gt; System (left sidebar of Jenkins) then to Global credentials (unrestricted) on the page table and on the left sidebar again click on Add Credentials.\nThis should be shown in your Jenkins:\nFill the form like this:\nAnd click in OK to create them. Check if the whole thing went as expected:\n2. Jenkins Maven Settings\nThose settings are also configured (or maybe not-yet-configured) in our devonfw distributions in:\n/${devonfw-dist-path}\n/software\n/maven\n/conf\nsettings.xml\nGo to Manage Jenkins &gt; Managed files and select Add a new Config in the left sidebar.\nThe ID field will get automatically filled with a unique value if you don&#x2019;t set it up. No problems about that. Click on Submit and let&#x2019;s create some Servers Credentials:\nThose Server Credentials will allow Jenkins to access to the different repositories/servers that are going to be declared afterwards.\nLet&#x2019;s create 4 server credentials.\nmy.nexus: Will serve as general profile for Maven.\nmynexus.releases: When a mvn deploy process is executed, this will tell Maven where to push releases to.\nmynexus.snapshots: The same as before, but with snapshots instead.\nmynexus.central: Just in case we want to install an specific dependency that is not by default in the Maven Central repository (such as ojdbc), Maven will point to it instead.\nA more or less complete Jenkins Maven settings would look look like this:\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;\n&lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot;\nxmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;\nxsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt;\n&lt;mirrors&gt;\n&lt;mirror&gt;\n&lt;id&gt;mynexus.central&lt;/id&gt;\n&lt;mirrorOf&gt;central&lt;/mirrorOf&gt;\n&lt;name&gt;central&lt;/name&gt;\n&lt;url&gt;http://${URL-TO-YOUR-NEXUS-REPOS}/central&lt;/url&gt;\n&lt;/mirror&gt;\n&lt;/mirrors&gt;\n&lt;profiles&gt;\n&lt;profile&gt;\n&lt;id&gt;my.nexus&lt;/id&gt;\n&lt;!-- 3 REPOS ARE DECLARED --&gt;\n&lt;repositories&gt;\n&lt;repository&gt;\n&lt;id&gt;mynexus.releases&lt;/id&gt;\n&lt;name&gt;mynexus Releases&lt;/name&gt;\n&lt;url&gt;http://${URL-TO-YOUR-NEXUS-REPOS}/releases&lt;/url&gt;\n&lt;releases&gt;\n&lt;enabled&gt;true&lt;/enabled&gt;\n&lt;updatePolicy&gt;always&lt;/updatePolicy&gt;\n&lt;/releases&gt;\n&lt;snapshots&gt;\n&lt;enabled&gt;false&lt;/enabled&gt;\n&lt;updatePolicy&gt;always&lt;/updatePolicy&gt;\n&lt;/snapshots&gt;\n&lt;/repository&gt;\n&lt;repository&gt;\n&lt;id&gt;mynexus.snapshots&lt;/id&gt;\n&lt;name&gt;mynexus Snapshots&lt;/name&gt;\n&lt;url&gt;http://${URL-TO-YOUR-NEXUS-REPOS}/snapshots&lt;/url&gt;\n&lt;releases&gt;\n&lt;enabled&gt;false&lt;/enabled&gt;\n&lt;updatePolicy&gt;always&lt;/updatePolicy&gt;\n&lt;/releases&gt;\n&lt;snapshots&gt;\n&lt;enabled&gt;true&lt;/enabled&gt;\n&lt;updatePolicy&gt;always&lt;/updatePolicy&gt;\n&lt;/snapshots&gt;\n&lt;/repository&gt;\n&lt;/repositories&gt;\n&lt;pluginRepositories&gt;\n&lt;pluginRepository&gt;\n&lt;id&gt;public&lt;/id&gt;\n&lt;name&gt;Public Repositories&lt;/name&gt;\n&lt;url&gt;http://${URL-TO-YOUR-NEXUS}/nexus/content/groups/public/&lt;/url&gt;\n&lt;releases&gt;\n&lt;enabled&gt;true&lt;/enabled&gt;\n&lt;updatePolicy&gt;always&lt;/updatePolicy&gt;\n&lt;/releases&gt;\n&lt;snapshots&gt;\n&lt;enabled&gt;true&lt;/enabled&gt;\n&lt;updatePolicy&gt;always&lt;/updatePolicy&gt;\n&lt;/snapshots&gt;\n&lt;/pluginRepository&gt;\n&lt;/pluginRepositories&gt;\n&lt;/profile&gt;\n&lt;/profiles&gt;\n&lt;!-- HERE IS WHERE WE TELL MAVEN TO CHOOSE THE my.nexus PROFILE --&gt;\n&lt;activeProfiles&gt;\n&lt;activeProfile&gt;my.nexus&lt;/activeProfile&gt;\n&lt;/activeProfiles&gt;\n&lt;/settings&gt;\n3. Use it in Jenkins Pipelines\n39.3.8. Jenkins - SonarQube integration\nFirst thing is installing both tools by, for example, Docker or Docker Compose. Then, we have to think about how they should collaborate to create a more efficient Continuous Integration process.\nOnce our project&#x2019;s pipeline is triggered (it could also be triggered in a fancy way, such as when a merge to the develop branch is done).\n1. Jenkins SonarQube plugin\nTypically in those integration cases, Jenkins plug-in installations become a must. Let&#x2019;s look for some available SonarQube plug-in(s) for Jenkins:\n2. SonarQube token\nOnce installed let&#x2019;s create a token in SonarQube so that Jenkins can communicate with it to trigger their Jobs. Once we install SonarQube in our CI/CD machine (ideally a remote machine) let&#x2019;s login with admin/admin credentials:\nAfterwards, SonarQube itself asks you to create this token we talked about (the name is up to you):\nThen a token is generated:\nYou click in &quot;continue&quot; and the token&#x2019;s generation is completed:\n3. Jenkins SonarQube Server setup\nNow we need to tell Jenkins where is SonarQube and how to communicate with it. In Manage Jenkins &gt; Configure Settings. We add a name for the server (up to you), where it is located (URL), version and the Server authentication token created in point 2.\n4. Jenkins SonarQube Scanner\nInstall a SonarQube Scanner as a Global tool in Jenkins to be used in the project&#x2019;s pipeline.\n5. Pipeline code\nLast step is to add the SonarQube process in our project&#x2019;s Jenkins pipeline. The following code will trigger a SonarQube process that will evaluate our code&#x2019;s quality looking for bugs, duplications, and so on.\nstage &apos;SonarQube Analysis&apos;\ndef scannerHome = tool &apos;SonarQube scanner&apos;;\nsh &quot;${scannerHome}/bin/sonar-scanner \\\n-Dsonar.host.url=http://url-to-your-sq-server:9000/ \\\n-Dsonar.login=[SONAR_USER] -Dsonar.password=[SONAR_PASS] \\\n-Dsonar.projectKey=[PROJECT_KEY] \\\n-Dsonar.projectName=[PROJECT_NAME] -Dsonar.projectVersion=[PROJECT_VERSION] \\\n-Dsonar.sources=. -Dsonar.java.binaries=. \\\n-Dsonar.java.source=1.8 -Dsonar.language=java&quot;\n6. Results\nAfter all this, you should end up having something like this in Jenkins:\nAnd in SonarQube:\n7. Changes in a devonfw project to execute SonarQube tests with Coverage\nThe plugin used to have Coverage reports in the SonarQube for devonfw projects is Jacoco. There are some changes in the project&#x2019;s parent pom.xml that are mandatory to use it.\nInside of the &lt;properties&gt; tag:\n&lt;properties&gt;\n(...)\n&lt;sonar.jacoco.version&gt;3.8&lt;/sonar.jacoco.version&gt;\n&lt;sonar.java.coveragePlugin&gt;jacoco&lt;/sonar.java.coveragePlugin&gt;\n&lt;sonar.core.codeCoveragePlugin&gt;jacoco&lt;/sonar.core.codeCoveragePlugin&gt;\n&lt;sonar.dynamicAnalysis&gt;reuseReports&lt;/sonar.dynamicAnalysis&gt;\n&lt;sonar.language&gt;java&lt;/sonar.language&gt;\n&lt;sonar.java.source&gt;1.7&lt;/sonar.java.source&gt;\n&lt;sonar.junit.reportPaths&gt;target/surefire-reports&lt;/sonar.junit.reportPaths&gt;\n&lt;sonar.jacoco.reportPaths&gt;target/jacoco.exec&lt;/sonar.jacoco.reportPaths&gt;\n&lt;sonar.sourceEncoding&gt;UTF-8&lt;/sonar.sourceEncoding&gt;\n&lt;sonar.exclusions&gt;\n**/generated-sources/**/*,\n**io/oasp/mirabaud/general/**/*,\n**/*Dao.java,\n**/*Entity.java,\n**/*Cto.java,\n**/*Eto.java,\n**/*SearchCriteriaTo.java,\n**/*management.java,\n**/*SpringBootApp.java,\n**/*SpringBootBatchApp.java,\n**/*.xml,\n**/*.jsp\n&lt;/sonar.exclusions&gt;\n&lt;sonar.coverage.exclusions&gt;\n**io/oasp/mirabaud/general/**/*,\n**/*Dao.java,\n**/*Entity.java,\n**/*Cto.java,\n**/*Eto.java,\n**/*SearchCriteriaTo.java,\n**/*management.java,\n**/*SpringBootApp.java,\n**/*SpringBootBatchApp.java,\n**/*.xml,\n**/*.jsp\n&lt;/sonar.coverage.exclusions&gt;\n&lt;sonar.host.url&gt;http://${YOUR_SONAR_SERVER_URL}/&lt;/sonar.host.url&gt;\n&lt;jacoco.version&gt;0.7.9&lt;/jacoco.version&gt;\n&lt;war.plugin.version&gt;3.2.0&lt;/war.plugin.version&gt;\n&lt;assembly.plugin.version&gt;3.1.0&lt;/assembly.plugin.version&gt;\n&lt;/properties&gt;\nOf course, those sonar amd sonar.coverage can/must be changed to fit with other projects.\nNow add the Jacoco Listener as a dependency:\n&lt;dependencies&gt;\n&lt;dependency&gt;\n&lt;groupId&gt;org.sonarsource.java&lt;/groupId&gt;\n&lt;artifactId&gt;sonar-jacoco-listeners&lt;/artifactId&gt;\n&lt;scope&gt;test&lt;/scope&gt;\n&lt;/dependency&gt;\n&lt;/dependencies&gt;\nPlugin Management declarations:\n&lt;pluginManagement&gt;\n&lt;plugins&gt;\n&lt;plugin&gt;\n&lt;groupId&gt;org.sonarsource.scanner.maven&lt;/groupId&gt;\n&lt;artifactId&gt;sonar-maven-plugin&lt;/artifactId&gt;\n&lt;version&gt;3.2&lt;/version&gt;\n&lt;/plugin&gt;\n&lt;plugin&gt;\n&lt;groupId&gt;org.jacoco&lt;/groupId&gt;\n&lt;artifactId&gt;jacoco-maven-plugin&lt;/artifactId&gt;\n&lt;version&gt;${jacoco.version}&lt;/version&gt;\n&lt;/plugin&gt;\n&lt;/plugins&gt;\n&lt;pluginManagement&gt;\nPlugins:\n&lt;plugins&gt;\n(...)\n&lt;plugin&gt;\n&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\n&lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;\n&lt;version&gt;2.20.1&lt;/version&gt;\n&lt;configuration&gt;\n&lt;argLine&gt;-XX:-UseSplitVerifier -Xmx2048m ${surefireArgLine}&lt;/argLine&gt;\n&lt;testFailureIgnore&gt;false&lt;/testFailureIgnore&gt;\n&lt;useFile&gt;false&lt;/useFile&gt;\n&lt;reportsDirectory&gt;${project.basedir}/${sonar.junit.reportPaths}&lt;/reportsDirectory&gt;\n&lt;argLine&gt;${jacoco.agent.argLine}&lt;/argLine&gt;\n&lt;excludedGroups&gt;${oasp.test.excluded.groups}&lt;/excludedGroups&gt;\n&lt;alwaysGenerateSurefireReport&gt;true&lt;/alwaysGenerateSurefireReport&gt;\n&lt;aggregate&gt;true&lt;/aggregate&gt;\n&lt;properties&gt;\n&lt;property&gt;\n&lt;name&gt;listener&lt;/name&gt;\n&lt;value&gt;org.sonar.java.jacoco.JUnitListener&lt;/value&gt;\n&lt;/property&gt;\n&lt;/properties&gt;\n&lt;/configuration&gt;\n&lt;/plugin&gt;\n&lt;plugin&gt;\n&lt;groupId&gt;org.jacoco&lt;/groupId&gt;\n&lt;artifactId&gt;jacoco-maven-plugin&lt;/artifactId&gt;\n&lt;configuration&gt;\n&lt;argLine&gt;-Xmx128m&lt;/argLine&gt;\n&lt;append&gt;true&lt;/append&gt;\n&lt;propertyName&gt;jacoco.agent.argLine&lt;/propertyName&gt;\n&lt;destFile&gt;${sonar.jacoco.reportPath}&lt;/destFile&gt;\n&lt;excludes&gt;\n&lt;exclude&gt;**/generated-sources/**/*,&lt;/exclude&gt;\n&lt;exclude&gt;**io/oasp/${PROJECT_NAME}/general/**/*&lt;/exclude&gt;\n&lt;exclude&gt;**/*Dao.java&lt;/exclude&gt;\n&lt;exclude&gt;**/*Entity.java&lt;/exclude&gt;\n&lt;exclude&gt;**/*Cto.java&lt;/exclude&gt;\n&lt;exclude&gt;**/*Eto.java&lt;/exclude&gt;\n&lt;exclude&gt;**/*SearchCriteriaTo.java&lt;/exclude&gt;\n&lt;exclude&gt;**/*management.java&lt;/exclude&gt;\n&lt;exclude&gt;**/*SpringBootApp.java&lt;/exclude&gt;\n&lt;exclude&gt;**/*SpringBootBatchApp.java&lt;/exclude&gt;\n&lt;exclude&gt;**/*.class&lt;/exclude&gt;\n&lt;/excludes&gt;\n&lt;/configuration&gt;\n&lt;executions&gt;\n&lt;execution&gt;\n&lt;id&gt;prepare-agent&lt;/id&gt;\n&lt;phase&gt;initialize&lt;/phase&gt;\n&lt;goals&gt;\n&lt;goal&gt;prepare-agent&lt;/goal&gt;\n&lt;/goals&gt;\n&lt;configuration&gt;\n&lt;destFile&gt;${sonar.jacoco.reportPath}&lt;/destFile&gt;\n&lt;append&gt;true&lt;/append&gt;\n&lt;/configuration&gt;\n&lt;/execution&gt;\n&lt;execution&gt;\n&lt;id&gt;report-aggregate&lt;/id&gt;\n&lt;phase&gt;verify&lt;/phase&gt;\n&lt;goals&gt;\n&lt;goal&gt;report-aggregate&lt;/goal&gt;\n&lt;/goals&gt;\n&lt;/execution&gt;\n&lt;execution&gt;\n&lt;id&gt;jacoco-site&lt;/id&gt;\n&lt;phase&gt;verify&lt;/phase&gt;\n&lt;goals&gt;\n&lt;goal&gt;report&lt;/goal&gt;\n&lt;/goals&gt;\n&lt;/execution&gt;\n&lt;/executions&gt;\n&lt;/plugin&gt;\n&lt;/plugins&gt;\nJenkins SonarQube execution\nIf the previous configuration is already setup, once Jenkins execute the sonar maven plugin, it will automatically execute coverage as well.\nThis is an example of a block of code from a devonfw project&#x2019;s Jenkinsfile:\nwithMaven(globalMavenSettingsConfig: &apos;YOUR_GLOBAL_MAVEN_SETTINGS&apos;, jdk: &apos;OpenJDK 1.8&apos;, maven: &apos;Maven_3.3.9&apos;) {\nsh &quot;mvn sonar:sonar -Dsonar.login=[USERNAME] -Dsonar.password=[PASSWORD]&quot;\n}\n39.4. OKD (OpenShift Origin)\n39.4.1. What is OKD\nOKD is a distribution of Kubernetes optimized for continuous application development and multi-tenant deployment. OKD is the upstream Kubernetes distribution embedded in Red Hat OpenShift.\nOKD embeds Kubernetes and extends it with security and other integrated concepts. OKD is also referred to as Origin in github and in the documentation.\nOKD provides a complete open source container application platform. If you are looking for enterprise-level support, or information on partner certification, Red Hat also offers Red Hat OpenShift Container Platform.\nContinue reading&#x2026;&#x200B;\nHow to install Openshift Origin\nInitial setup\ns2i\ntemplates\nCustomize Openshift\nCustomize icons\nCustomize catalog\n39.4.2. Install OKD (Openshift Origin)\nPre-requisites\nInstall docker\nhttps://docs.docker.com/engine/installation/linux/docker-ce/debian/#set-up-the-repository\n$ sudo groupadd docker\n$ sudo usermod -aG docker $USER\nDownload Openshift Origin Client\nDownload Openshift Origin Client from here\nWhen the download it&#x2019;s complete, only extract it on the directory that you want, for example /home/administrador/oc\nAdd oc to path\n$ export PATH=$PATH:/home/administrador/oc\nInstall Openshift Cluster\nAdd the insecure registry\nCreate file /etc/docker/daemon.json with the next content:\n{\n&quot;insecure-registries&quot; : [ &quot;172.30.0.0/16&quot; ]\n}\nDownload docker images for openshift\n$ oc cluster up\nInstall Oc Cluster Wrapper\nTo manage easier the cluster persistent, we are going to use oc cluster wrapper.\ncd /home/administrador/oc\nwget https://raw.githubusercontent.com/openshift-evangelists/oc-cluster-wrapper/master/oc-cluster\noc-cluster up devonfw-shop-floor --public-hostname X.X.X.X\nConfigure iptables\nWe must create iptables rules to allow traffic from other machines.\n- The next commands it&apos;s to let all traffic, don&apos;t do it on a real server.\n- $ iptables -F\n- $ iptables -X\n- $ iptables -t nat -F\n- $ iptables -t nat -X\n- $ iptables -t mangle -F\n- $ iptables -t mangle -X\n- $ iptables -P INPUT ACCEPT\n- $ iptables -P OUTPUT ACCEPT\n- $ iptables -P FORWARD ACCEPT\n39.4.3. How to use Oc Cluster Wrapper\nWith oc cluster wrapper we could have different clusters with different context.\nCluster up\n$ oc-cluster up devonfw-shop-floor --public-hostname X.X.X.X\nCluster down\n$ oc-cluster down\nUse non-persistent cluster\noc cluster up --image openshift/origin --public-hostname X.X.X.X --routing-suffix apps.X.X.X.X.nip.io\n39.4.4. devonfw Openshift Origin Initial Setup\nThese are scripts to customize an Openshift cluster to be a devonfw Openshift.\nHow to use\nPrerequisite: Customize Openshift\ndevonfw Openshift Origin use custom icons, and we need to add it to openshift. More information:\nCustomize Openshift\nScript initial-setup\nDownload this script and execute it.\nMore information about what this script does here.\nKnown issues\nFailed to push image\nIf you receive an error like this:\nerror: build error: Failed to push image: After retrying 6 times, Push image still failed due to error: Get http://172.30.1.1:5000/v2/: dial tcp 172.30.1.1:5000: getsockopt: connection refused\nIt&#x2019;s because the registry isn&#x2019;t working, go to openshift console and enter into the default project https://x.x.x.x:8443/console/project/default/overview and you must see two resources, docker-registry and router they must be running. If they don&#x2019;t work, try to deploy them and look at the logs what is happen.\n39.4.5. s2i devonfw\nThis are the s2i source and templates to build an s2i images. It provides OpenShift builder images for components of the devonfw (at this moment only for angular and java).\nThis work is totally based on the implementation of Michael Kuehl from RedHat for Oasp s2i.\nAll this information is used as a part of the initial setup for openshift.\nPrevious setup\nIn order to build all of this, it will be necessary, first, to have a running OpenShift cluster. How to install it here.\nUsage\nBefore using the builder images, add them to the OpenShift cluster.\nDeploy the Source-2-Image builder images\nFirst, create a dedicated devonfw project as admin.\n$ oc new-project devonfw --display-name=&apos;devonfw&apos; --description=&apos;devonfw Application Standard Platform&apos;\nNow add the builder image configuration and start their build.\noc create -f https://raw.githubusercontent.com/devonfw/devonfw-shop-floor/master/dsf4openshift/openshift-devonfw-deployment/s2i/java/s2i-devonfw-java-imagestream.json --namespace=devonfw\noc create -f https://raw.githubusercontent.com/devonfw/devonfw-shop-floor/master/dsf4openshift/openshift-devonfw-deployment/s2i/angular/s2i-devonfw-angular-imagestream.json --namespace=devonfw\noc start-build s2i-devonfw-java --namespace=devonfw\noc start-build s2i-devonfw-angular --namespace=devonfw\nMake sure other projects can access the builder images:\noc policy add-role-to-group system:image-puller system:authenticated --namespace=devonfw\nThat&#x2019;s all!\nDeploy devonfw templates\nNow, it&#x2019;s time to create devonfw templates to use this s2i and add it to the browse catalog. More information here.\nBuild All\nUse this script to automatically install and build all image streams. The script also creates templates devonfw-angular and devonfw-java inside the project &apos;openshift&apos; to be used by everyone.\nOpen a bash shell as Administrator\nExecute shell file:\n$ /PATH/TO/BUILD/FILE/initial-setup.sh\nMore information about what this script does here.\nLinks &amp; References\nThis is a list of useful articles, etc, that I found while creating the templates.\nTemplate Icons\nRed Hat Cool Store Microservice Demo\nOpenshift Web Console Customization\n39.4.6. devonfw templates\nThis are the devonfw templates to build devonfw apps for Openshift using the s2i images. They are based on the work of Mickuehl in Oasp templates/mythaistar for deploy My Thai Star.\nInside the example-mythaistar we have an example to deploy My Thai Star application using devonfw templates.\nAll this information is used as a part of the initial setup for openshift.\nHow to use\nPrevious requirements\nDeploy the Source-2-Image builder images\nRemember that this templates need a build image from s2i-devonfw-angular and s2i-devonfw-java. More information:\nDeploy the Source-2-Image builder images.\nCustomize Openshift\nRemember that this templates also have custom icons, and to use it, we must modify the master-config.yml inside openshift. More information:\nCustomize Openshift.\nDeploy devonfw templates\nNow, it&#x2019;s time to create devonfw templates to use this s2i and add it to the browse catalog.\nTo let all user to use these templates in all openshift projects, we should create it in an openshift namespace. To do that, we must log in as an admin.\noc create -f https://raw.githubusercontent.com/devonfw/devonfw-shop-floor/master/dsf4openshift/openshift-devonfw-deployment/templates/devonfw-java-template.json --namespace=openshift\noc create -f https://raw.githubusercontent.com/devonfw/devonfw-shop-floor/master/dsf4openshift/openshift-devonfw-deployment/templates/devonfw-angular-template.json --namespace=openshift\nWhen it finishes, remember to logout as an admin and enter with our normal user.\n$ oc login\nHow to use devonfw templates in openshift\nTo use these templates with openshift, we can override any parameter values defined in the file by adding the --param-file=paramfile option.\nThis file must be a list of &lt;name&gt;=&lt;value&gt; pairs. A parameter reference may appear in any text field inside the template items.\nThe parameters that we must override are the following\n$ cat paramfile\nAPPLICATION_NAME=app-Name\nAPPLICATION_GROUP_NAME=group-Name\nGIT_URI=Git uri\nGIT_REF=master\nCONTEXT_DIR=/context\nThe following parameters are optional\n$ cat paramfile\nAPPLICATION_HOSTNAME=Custom hostname for service routes. Leave blank for default hostname, e.g.: &lt;application-name&gt;.&lt;project&gt;.&lt;default-domain-suffix&gt;,\n# Only for angular\nREST_ENDPOINT_URL=The URL of the backend&apos;s REST API endpoint. This can be declared after,\nREST_ENDPOINT_PATTERN=The pattern URL of the backend&apos;s REST API endpoint that must be modify by the REST_ENDPOINT_URL variable,\nFor example, to deploy My Thai Star Java\n$ cat paramfile\nAPPLICATION_NAME=&quot;mythaistar-java&quot;\nAPPLICATION_GROUP_NAME=&quot;My-Thai-Star&quot;\nGIT_URI=&quot;https://github.com/oasp/my-thai-star.git&quot;\nGIT_REF=&quot;develop&quot;\nCONTEXT_DIR=&quot;/java/mtsj&quot;\n$ oc new-app --template=devonfw-java --namespace=mythaistar --param-file=paramfile\n39.4.7. Customize Openshift Origin for devonfw\nThis is a guide to customize Openshift cluster.\nImages Styles\nThe icons for templates must measure the same as below or the images don&#x2019;t show right:\nOpenshift logo: 230px x 40px.\nTemplate logo: 50px x 50px.\nCategory logo: 110px x 36px.\nHow to use\nTo use it, we need to enter in openshift as an admin and use the next command:\n$ oc login\n$ oc edit configmap/webconsole-config -n openshift-web-console\nAfter this, we can see in our shell the webconsole-config.yaml, we only need to navigate until extensions and add the url for our own css in the stylesheetURLs and javascript in the scriptURLs section.\nIMPORTANT: Scripts and stylesheets must be served with the correct content type or they will not be run by the browser. Scripts must be served with Content-Type: application/javascript and stylesheets with Content-Type: text/css.\nIn git repositories, the content type of raw is text/plain. You can use rawgit to convert a raw from a git repository to the correct content type.\nExample:\nwebconsole-config.yaml: |\n[...]\nextensions:\nscriptURLs:\n- https://cdn.rawgit.com/devonfw/devonfw-shop-floor/master/dsf4openshift/openshift-cluster-setup/initial-setup/customizeOpenshift/scripts/catalog-categories.js\nstylesheetURLs:\n- https://cdn.rawgit.com/devonfw/devonfw-shop-floor/master/dsf4openshift/openshift-cluster-setup/initial-setup/customizeOpenshift/stylesheet/icons.css\n[...]\nMore information\nCustomize icons for Openshift.\nCustomize catalog for Openshift.\nOpenshift docs about customization.\nOld versions\nCustomize Openshift for version 3.7.\nHow to add Custom Icons inside openshift\nThis is a guide to add custom icons into an Openshift cluster.\nHere we can find an icons.css example to use the devonfw icons.\nImages Styles\nThe icons for templates must measure the same as below or the images don&#x2019;t show right:\nOpenshift logo: 230px x 40px.\nTemplate logo: 50px x 50px.\nCategory logo: 110px x 36px.\nCreate a css\nCustom logo for openshift cluster\nFor this example, we are going to call the css icons.css but you can call as you wish.\nOpenshift cluster draw their icon by the id header-logo, then we only need to add to our icons.css the next Style Attribute ID\n#header-logo {\nbackground-image: url(&quot;https://raw.githubusercontent.com/devonfw/devonfw-shop-floor/master/dsf4openshift/openshift-cluster-setup/initial-setup/customizeOpenshift/images/devonfw-openshift.png);\nwidth: 230px;\nheight: 40px;\n}\nCustom icons for templates\nTo use a custom icon to a template openshift use a class name. Then, we need to insert inside our icons.css the next Style Class\n.devonfw-logo {\nbackground-image: url(&quot;https://raw.githubusercontent.com/devonfw/devonfw-shop-floor/master/dsf4openshift/openshift-cluster-setup/initial-setup/customizeOpenshift/images/devonfw.png&quot;);\nwidth: 50px;\nheight: 50px;\n}\nTo show that custom icon on a template, we only need to write the name of our class in the tag &quot;iconClass&quot; of our template.\n{\n...\n&quot;items&quot;: [\n{\n...\n&quot;metadata&quot;: {\n...\n&quot;annotations&quot;: {\n...\n&quot;iconClass&quot;: &quot;devonfw-logo&quot;,\n...\n}\n},\n...\n}\n]\n}\nUse our own css inside openshift\nTo do that, we need to enter in openshift as an admin and use the next command:\n$ oc login\n$ oc edit configmap/webconsole-config -n openshift-web-console\nAfter this, we can see in our shell the webconsole-config.yaml, we only need to navigate until extensions and add the url for our own css in the stylesheetURLs section.\nIMPORTANT: Scripts and stylesheets must be served with the correct content type or they will not be run by the browser. stylesheets must be served with Content-Type: text/css.\nIn git repositories, the content type of raw is text/plain. You can use rawgit to convert a raw from a git repository to the correct content type.\nExample:\nwebconsole-config.yaml: |\n[...]\nextensions:\nstylesheetURLs:\n- https://cdn.rawgit.com/devonfw/devonfw-shop-floor/master/dsf4openshift/openshift-cluster-setup/initial-setup/customizeOpenshift/stylesheet/icons.css\n[...]\nHow to add custom catalog categories inside openshift\nThis is a guide to add custom Catalog Categories into an Openshift cluster.\nHere we can find a catalog-categories.js example to use the devonfw catalog categories.\nCreate a scrip to add custom langauges and custom catalog categories\nCustom language\nFor this example, we are going add a new language into the languages category. To do that we must create a script and we named as catalog-categories.js\n// Find the Languages category.\nvar category = _.find(window.OPENSHIFT_CONSTANTS.SERVICE_CATALOG_CATEGORIES,\n{ id: &apos;languages&apos; });\n// Add Go as a new subcategory under Languages.\ncategory.subCategories.splice(2,0,{ // Insert at the third spot.\n// Required. Must be unique.\nid: &quot;devonfw-languages&quot;,\n// Required.\nlabel: &quot;devonfw&quot;,\n// Optional. If specified, defines a unique icon for this item.\nicon: &quot;devonfw-logo-language&quot;,\n// Required. Items matching any tag will appear in this subcategory.\ntags: [\n&quot;devonfw&quot;,\n&quot;devonfw-angular&quot;,\n&quot;devonfw-java&quot;\n]\n});\nCustom category\nFor this example, we are going add a new category into the category tab. To do that we must create a script and we named as catalog-categories.js\n// Add a Featured category as the first category tab.\nwindow.OPENSHIFT_CONSTANTS.SERVICE_CATALOG_CATEGORIES.unshift({\n// Required. Must be unique.\nid: &quot;devonfw-featured&quot;,\n// Required\nlabel: &quot;devonfw&quot;,\nsubCategories: [\n{\n// Required. Must be unique.\nid: &quot;devonfw-languages&quot;,\n// Required.\nlabel: &quot;devonfw&quot;,\n// Optional. If specified, defines a unique icon for this item.\nicon: &quot;devonfw-logo-language&quot;,\n// Required. Items matching any tag will appear in this subcategory.\ntags: [\n&quot;devonfw&quot;,\n&quot;devonfw-angular&quot;,\n&quot;devonfw-java&quot;\n]\n}\n]\n});\nUse our own javascript inside openshift\nTo do that, we need to enter in openshift as an admin and use the next command:\n$ oc login\n$ oc edit configmap/webconsole-config -n openshift-web-console\nAfter this, we can see in our shell the webconsole-config.yaml, we only need to navigate until extensions and add the url for our own javascript in the scriptURLs section.\nIMPORTANT: Scripts and stylesheets must be served with the correct content type or they will not be run by the browser. Scripts must be served with Content-Type: application/javascript.\nIn git repositories, the content type of raw is text/plain. You can use rawgit to convert a raw from a git repository to the correct content type.\nExample:\nwebconsole-config.yaml: |\n[...]\nextensions:\nscriptURLs:\n- https://cdn.rawgit.com/devonfw/devonfw-shop-floor/master/dsf4openshift/openshift-cluster-setup/initial-setup/customizeOpenshift/scripts/catalog-categories.js\n[...]\nCustomize Openshift Origin v3.7 for devonfw\nThis is a guide to customize Openshift cluster. For more information read the next:\nOpenshift docs customization for the version 3.7.\nImages Styles\nThe icons for templates must measure the same as below or the images don&#x2019;t show right:\nOpenshift logo: 230px x 40px.\nTemplate logo: 50px x 50px.\nCategory logo: 110px x 36px.\nQuick Use\nThis is a quick example to add custom icons and categories inside openshift.\nTo modify the icons inside openshift, we must to modify our master-config.yaml of our openshift cluster. This file is inside the openshift container and to obtain a copy of it, we must to know what&#x2019;s our openshift container name.\nObtain the master-config.yaml of our openshift cluster\nObtain the name of our openshift container\nTo obtain it, we can know it executing the next:\n$ docker container ls\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES\n83a4e3acda5b openshift/origin:v3.7.0 &quot;/usr/bin/openshift &#x2026;&quot; 6 days ago Up 6 days origin\nHere we can see that the name of the container is origin. Normaly the container it&#x2019;s called as origin.\nCopy the master-config.yaml of our openshift container to our directory\nThis file is inside the openshift container in the next directory: /var/lib/origin/openshift.local.config/master/master-config.yaml and we can copy it with the next command:\n$ docker cp origin:/var/lib/origin/openshift.local.config/master/master-config.yaml ./\nNow we have a file with the configuration of our openshift cluster.\nCopy all customize files inside the openshift container\nTo use our customization of devonfw Openshift, we need to copy our files inside the openshift container.\nTo do this we need to copy the images, scripts and stylesheets from here inside openshift\ncontainer, for example, we could put it all inside a folder called openshift.local.devonfw. On the step one we obtain the name of this container, for this example we assume that it&#x2019;s called origin. Then our images are located inside openshift container and we can see an access it in /var/lib/origin/openshift.local.devonfw/images.\n$ docker cp ./openshift.local.devonfw origin:/var/lib/origin/\nEdit and copy the master-config.yaml to use our customize files\nThe master-config.yaml have a sections to charge our custom files. All these sections are inside the assetConfig and their names are the next:\nThe custom stylessheets are into extensionStylesheets.\nThe custom scripts are into extensionScripts.\nThe custom images are into extensions.\nTo use all our custom elements only need to add the directory routes of each element in their appropriate section of the master-config.yaml\n...\nassetConfig:\n...\nextensionScripts:\n- /var/lib/origin/openshift.local.devonfw/scripts/catalog-categories.js\nextensionStylesheets:\n- /var/lib/origin/openshift.local.devonfw/stylesheet/icons.css\nextensions:\n- name: images\nsourceDirectory: /var/lib/origin/openshift.local.devonfw/images\n...\n...\nNow we only need to copy that master-config.yaml inside openshift, and restart it to load the new configuration. To do that execute the next:\n$ docker cp ./master-config.yaml origin:/var/lib/origin/openshift.local.config/master/master-config.yaml\nTo re-start openshift do oc cluster down and start again your persistent openshift cluster.\nMore information\nCustomize icons for Openshift.\nCustomize catalog for Openshift.\nOpenshift docs about customization.\n&#x2190;&#xA0;Previous:&#xA0;Monitoring&#xA0;| &#x2191;&#xA0;Up:&#xA0;devonfw shop floor&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;cicdgen&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devonfw-shop-floor.asciidoc_configuration-and-services-integration.html","title":"35. Configuration and services integration","body":"\n35. Configuration and services integration\n35.1. Nexus Configuration\nIn this document you will see how you can configure Nexus repository and how to integrate it with jenkins.\n35.1.1. Prerequisites\nRepositories\nYou need to have one repository for snapshots, another for releases and another one for release-candidates. Normally you use maven2 (hosted) repositories and if you are going to use a docker registry, you need docker (hosted) too.\nTo create a repository in Nexus go to the administration clicking on the gear icon at top menu bar. Then on the left menu click on Repositories and press the Create repository button.\nNow you must choose the type of the repository and configure it. This is an example for Snapshot:\n35.1.2. Create user to upload/download content\nOnce you have the repositories, you need a user to upload/download content. To do it go to the administration clicking on the gear icon at top menu bar. Then on the left menu click on Users and press the Create local user button.\nNow you need to fill a form like this:\n35.1.3. Jenkins integration\nTo use Nexus in our pipelines you need to configure Jenkins.\nAdd nexus user credentials\nFirst of all you need to add the user created in the step before to Jenkins. To do it (on the left menu) click on Credentials, then on System. Now you could access to Global credentials (unrestricted).\nEnter on it and you could see a button on the left to Add credentials. Click on it and fill a form like this:\nAdd the nexus user to maven global settings\nNow you need to go to Manage Jenkins clicking on left menu and enter in Managed files.\nEdit the Global Maven settings.xml to add your nexus repositories credentials as you could see in the next image:\nAnd you are done.\n35.2. SonarQube Configuration\nTo use SonarQube you need to use a token to connect, and to know the results of the analysis you need a webhook. Also, you need to install and configure SonarQube in Jenkins.\n35.2.1. Generate user token\nTo generate the user token, go to your account clicking in the left icon on the top menu bar.\nGo to security tab and generate the token.\n35.2.2. Webhook\nWhen you execute our SonarQube scanner in our pipeline job, you need to ask SonarQube if the quality gate has been passed. To do it you need to create a webhook.\nGo to administration clicking the option on the top bar menu and select the tab for Configuration.\nThen search in the left menu to go to webhook section and create your webhook.\nAn example for Production Line:\n35.2.3. Jenkins integration\nTo use SonarQube in our pipelines you need to configure Jenkins to integrate SonarQube.\nSonarQube Scanner\nFirst, you need to configure the scanner. Go to Manage Jenkins clicking on left menu and enter in Global Tool Configuration.\nGo to SonarQube Scanner section and add a new SonarQube scanner like this.\nSonarQube Server\nNow you need to configure where is our SonarQube server using the user token that you create before. Go to Manage Jenkins clicking on left menu and enter in Configure System.\nFor example, in Production Line the server is the next:\nNote\nRemember, the token was created at the beginning of this SonarQube configuration.\n35.2.4. SonarQube configuration\nNow is time to configure your sonar in order to check correctly the quality of the project. To do it, please follow the official documentation about our plugins and Quality Gates and Profiles here.\nHow to ignore files\nNormaly the developers needs to ignore some files from Sonar analysis. To do it, they must add the next line as a parameter of the sonar execution to their Jenkinsfile in the SonarQube code analysis step.\n-Dsonar.exclusions=&apos;**/*.spec.ts, **/*.model.ts, **/*mock.ts&apos;\n&#x2190;&#xA0;Previous:&#xA0;Provisioning environments&#xA0;| &#x2191;&#xA0;Up:&#xA0;devonfw shop floor&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Create project&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devonfw-shop-floor.asciidoc_create-project.html","title":"36. Create project","body":"\n36. Create project\n36.1. Create and integrate git repository\n36.1.1. GitLab Configuration\nCreate new repository\nTo create a new project in GitLab, go to your dashboard and click the green New project button or use the plus icon in the navigation bar.\nThis opens the New project page. Choose your group and fill the name of your project, the description and the visibility level in the next form:\nNote\nmore information about how to create projects in GitLab in the official documentation\nService integration\nTo learn how to configure the integration between GitLab and Jenkins see the next example\n36.2. start new devonfw project\nIt is time to create your devonfw project:\n36.2.1. How to create new devonfw project\nHere you can find the official guides to start new devonfw projects:\nvisit our devon4ng guide.\nvisit our devon4j guide.\n36.3. cicd configuration\n36.3.1. Manual configuration\nJenkinsfile\nIntroduction\nHere you are going to learn how you should configure the jenkinsfile of your project to apply CI/CD operations and enables automated application deployment.\nHere you can find examples of the Jenkinsfile generated by cicdgen:\ndevon4j\ndevon4ng\ndevon4node\nNext you could find an explanation about what is done in these Jenkinsfiles.\nEnvironment values\nAt the top of the pipeline you should add the environment variables. in this tutorial you need the next variables:\n// sonarQube\n// Name of the sonarQube tool\nsonarTool = &apos;SonarQube&apos;\n// Name of the sonarQube environment\nsonarEnv = &quot;SonarQube&quot;\n// Nexus\n// Artifact groupId\ngroupId = &apos;&lt;%= groupid %&gt;&apos;\n// Nexus repository ID\nrepositoryId = &apos;pl-nexus&apos;\n// Nexus internal URL\nrepositoryUrl = &apos;http://nexus3-core:8081/nexus3/repository/&apos;\n// Maven global settings configuration ID\nglobalSettingsId = &apos;MavenSettings&apos;\n// Maven tool id\nmavenInstallation = &apos;Maven3&apos;\n// Docker registry\ndockerRegistry = &apos;docker-registry-&lt;%= plurl %&gt;&apos;\ndockerRegistryCredentials = &apos;nexus-docker&apos;\ndockerTool = &apos;docker-global&apos;\n// OpenShift\nopenshiftUrl = &apos;&lt;%= ocurl %&gt;&apos;\nopenShiftCredentials = &apos;openshift&apos;\nopenShiftNamespace = &apos;&lt;%= ocn %&gt;&apos;\nStages\nThe pipeline consists of stages, and at the beginning of each stage it is declared for which branches the step will be executed.\nNow it is time to create the stages.\nSetup Jenkins tools\nThe first stage is one of the most dangerous, because in it on one hand the tools are added to the pipeline and to the path and on other hand the values are tagged depending on the branch that is being executed. If you are going to create a ci/cd for a new branch or you are going to modify something, be very careful with everything that this first step declares.\nThis is an example of this stage:\nscript {\ntool yarn\ntool Chrome-stable\ntool dockerTool\nif (env.BRANCH_NAME.startsWith(&apos;release&apos;)) {\ndockerTag = &quot;release&quot;\nrepositoryName = &apos;maven-releases&apos;\ndockerEnvironment = &quot;_uat&quot;\nopenShiftNamespace += &quot;-uat&quot;\nsonarProjectKey = &apos;-release&apos;\n}\nif (env.BRANCH_NAME == &apos;develop&apos;) {\ndockerTag = &quot;latest&quot;\nrepositoryName = &apos;maven-snapshots&apos;\ndockerEnvironment = &quot;_dev&quot;\nopenShiftNamespace += &quot;-dev&quot;\nsonarProjectKey = &apos;-develop&apos;\n}\nif (env.BRANCH_NAME == &apos;master&apos;) {\ndockerTag = &quot;production&quot;\nrepositoryName = &apos;maven-releases&apos;\ndockerEnvironment = &apos;_prod&apos;\nopenShiftNamespace += &quot;-prod&quot;\nsonarProjectKey = &apos;&apos;\n}\nsh &quot;yarn&quot;\n}\nCode lint analysis\nThe next stage is to analyze the code making a lint analysis. To do it your project should have a tslint file with the configuration (tslint.json).\nanalyzing the code in your pipeline is as simple as executing the following command:\nsh &quot;&quot;&quot;yarn lint&quot;&quot;&quot;\nNote\nYour project need to have an script with tslint configuration (tslint.json).\nExecute tests\nTo test you application first of all your application should have created the tests and you should use one of the next two options:\nExecute test with maven (It should be used by devon4j).\nwithMaven(globalMavenSettingsConfig: globalSettingsId, maven: mavenInstallation) {\nsh &quot;mvn clean test&quot;\n}\nExecute test with yarn (It should be used by devon4ng or devon4node).\nsh &quot;&quot;&quot;yarn test:ci&quot;&quot;&quot;\nNote\nRemember that your project should have the tests created and in case of do it with yarn or npm, you package.json should have the script declared. This is an example &quot;test:ci&quot;: &quot;ng test --browsers ChromeHeadless --watch=false&quot;.\nSonarQube Analisys\nIt is time to see if your application complies the requirements of the sonar analysis.\nTo do it you could use one of the next two options:\nExecute Sonar with sonarTool (It should be used by devon4ng or devon4node).\nscript {\ndef scannerHome = tool sonarTool\ndef props = readJSON file: &apos;package.json&apos;\nwithSonarQubeEnv(sonarEnv) {\nsh &quot;&quot;&quot;\n${scannerHome}/bin/sonar-scanner \\\n-Dsonar.projectKey=${props.name}${sonarProjectKey} \\\n-Dsonar.projectName=${props.name}${sonarProjectKey} \\\n-Dsonar.projectVersion=${props.version} \\\n-Dsonar.sources=${srcDir} \\\n-Dsonar.typescript.lcov.reportPaths=coverage/lcov.info\n&quot;&quot;&quot;\n}\ntimeout(time: 1, unit: &apos;HOURS&apos;) {\ndef qg = waitForQualityGate()\nif (qg.status != &apos;OK&apos;) {\nerror &quot;Pipeline aborted due to quality gate failure: ${qg.status}&quot;\n}\n}\n}\nExecute Sonar with maven (It should be used by devon4j).\nscript {\nwithMaven(globalMavenSettingsConfig: globalSettingsId, maven: mavenInstallation) {\nwithSonarQubeEnv(sonarEnv) {\n// Change the project name (in order to simulate branches with the free version)\nsh &quot;cp pom.xml pom.xml.bak&quot;\nsh &quot;cp api/pom.xml api/pom.xml.bak&quot;\nsh &quot;cp core/pom.xml core/pom.xml.bak&quot;\nsh &quot;cp server/pom.xml server/pom.xml.bak&quot;\ndef pom = readMavenPom file: &apos;./pom.xml&apos;;\npom.artifactId = &quot;${pom.artifactId}${sonarProjectKey}&quot;\nwriteMavenPom model: pom, file: &apos;pom.xml&apos;\ndef apiPom = readMavenPom file: &apos;api/pom.xml&apos;\napiPom.parent.artifactId = pom.artifactId\napiPom.artifactId = &quot;${pom.artifactId}-api&quot;\nwriteMavenPom model: apiPom, file: &apos;api/pom.xml&apos;\ndef corePom = readMavenPom file: &apos;core/pom.xml&apos;\ncorePom.parent.artifactId = pom.artifactId\ncorePom.artifactId = &quot;${pom.artifactId}-core&quot;\nwriteMavenPom model: corePom, file: &apos;core/pom.xml&apos;\ndef serverPom = readMavenPom file: &apos;server/pom.xml&apos;\nserverPom.parent.artifactId = pom.artifactId\nserverPom.artifactId = &quot;${pom.artifactId}-server&quot;\nwriteMavenPom model: serverPom, file: &apos;server/pom.xml&apos;\nsh &quot;mvn sonar:sonar&quot;\nsh &quot;mv pom.xml.bak pom.xml&quot;\nsh &quot;mv api/pom.xml.bak api/pom.xml&quot;\nsh &quot;mv core/pom.xml.bak core/pom.xml&quot;\nsh &quot;mv server/pom.xml.bak server/pom.xml&quot;\n}\n}\ntimeout(time: 1, unit: &apos;HOURS&apos;) {\ndef qg = waitForQualityGate()\nif (qg.status != &apos;OK&apos;) {\nerror &quot;Pipeline aborted due to quality gate failure: ${qg.status}&quot;\n}\n}\n}\nBuild\nIf SonarQube is passed, you could build your application. To do it, if you are using devon4ng or devon4node you only need to add the next command:\nsh &quot;&quot;&quot;yarn build&quot;&quot;&quot;\nNote\nIf you are using devon4j this and the next step Store in Nexus are making together using mvn deploy.\nStore in Nexus\nOne time the application has been built the code of the application you could find the artifacts stored in the dist folder. You should push these artifacts to store them in Nexus.\nYou can do it following one of the next options:\nUse maven deploy config of your project (It should be used by devon4j).\nwithMaven(globalMavenSettingsConfig: globalSettingsId, maven: mavenInstallation) {\nsh &quot;mvn deploy -Dmaven.test.skip=true&quot;\n}\nConfigure maven deploy in your pipeline (It should be used by devon4ng and devon4node).\nscript {\ndef props = readJSON file: &apos;package.json&apos;\nzip dir: &apos;dist/&apos;, zipFile: &quot;&quot;&quot;${props.name}.zip&quot;&quot;&quot;\nversion = props.version\nif (!version.endsWith(&quot;-SNAPSHOT&quot;) &amp;&amp; env.BRANCH_NAME == &apos;develop&apos;) {\nversion = &quot;${version}-SNAPSHOT&quot;\nversion = version.replace(&quot;-RC&quot;, &quot;&quot;)\n}\nif (!version.endsWith(&quot;-RC&quot;) &amp;&amp; env.BRANCH_NAME.startsWith(&apos;release&apos;)) {\nversion = &quot;${version}-RC&quot;\nversion = version.replace(&quot;-SNAPSHOT&quot;, &quot;&quot;)\n}\nif (env.BRANCH_NAME == &apos;master&apos; &amp;&amp; (version.endsWith(&quot;-RC&quot;) || version.endsWith(&quot;-SNAPSHOT&quot;))){\nversion = version.replace(&quot;-RC&quot;, &quot;&quot;)\nversion = version.replace(&quot;-SNAPSHOT&quot;, &quot;&quot;)\n}\nwithMaven(globalMavenSettingsConfig: globalSettingsId, maven: mavenInstallation) {\nsh &quot;&quot;&quot;\nmvn deploy:deploy-file \\\n-DgroupId=${groupId} \\\n-DartifactId=${props.name} \\\n-Dversion=${version} \\\n-Dpackaging=zip \\\n-Dfile=${props.name}.zip \\\n-DrepositoryId=${repositoryId} \\\n-Durl=${repositoryUrl}${repositoryName}\n&quot;&quot;&quot;\n}\n}\nCreate docker image\nNow we need to use this artifacts to create a Docker image. To create the docker image you need an external server to do it. You could do it using one of the next:\nCreate docker image using OpenShift cluster\nTo create the docker image with this option you need to configure your OpenShift. You could read how to configure it here.\nprops = readJSON file: &apos;package.json&apos;\nwithCredentials([usernamePassword(credentialsId: &quot;${openShiftCredentials}&quot;, passwordVariable: &apos;pass&apos;, usernameVariable: &apos;user&apos;)]) {\nsh &quot;oc login -u ${user} -p ${pass} ${openshiftUrl} --insecure-skip-tls-verify&quot;\ntry {\nsh &quot;oc start-build ${props.name} --namespace=${openShiftNamespace} --from-dir=dist --wait&quot;\nsh &quot;oc import-image ${props.name} --namespace=${openShiftNamespace} --from=${dockerRegistry}/${props.name}:${dockerTag} --confirm&quot;\n} catch (e) {\nsh &quot;&quot;&quot;\noc logs \\$(oc get builds -l build=${props.name} --namespace=${openShiftNamespace} --sort-by=.metadata.creationTimestamp -o name | tail -n 1) --namespace=${namespace}\nthrow e\n&quot;&quot;&quot;\n}\n}\nNote\nif your project is a maven project you should read the pom.xml file instead of the package.json, you could do it with the next command def pom = readMavenPom file: &apos;pom.xml&apos;. Due to the fact that there are different variable names between those two files, remember to modify ${props.name} for ${pom.artifactId} in the code.\nCreate docker image using docker server\nTo create the docker image with this option you need to install docker and configure where is the docker host in your jenkins.\ndocker.withRegistry(&quot;&quot;&quot;${dockerRegistryProtocol}${dockerRegistry}&quot;&quot;&quot;, dockerRegistryCredentials) {\ndef props = readJSON file: &apos;package.json&apos;\ndef customImage = docker.build(&quot;${props.name}:${props.version}&quot;, &quot;-f ${dockerFileName} .&quot;)\ncustomImage.push()\ncustomImage.push(dockerTag);\n}\nhere\nNote\nif your project is a maven project you should read the pom.xml file instead of the package.json, you could do it with the next command def pom = readMavenPom file: &apos;pom.xml&apos;. Due to the fact that there are different variable names between those two files, remember to modify ${props.name} for ${pom.artifactId} and ${props.version} for ${pom.version} in the code.\nDeploy docker image\nOnce you have the docker image in the registry we only need to import it into your deployment environment. We can do it executing one of the next commands:\nDeploy docker image in OpenShift cluster\nTo deploy the docker image with this option you need to configure your OpenShift. You could read how to configure it here.\nscript {\nprops = readJSON file: &apos;package.json&apos;\nwithCredentials([usernamePassword(credentialsId: &quot;${openShiftCredentials}&quot;, passwordVariable: &apos;pass&apos;, usernameVariable: &apos;user&apos;)]) {\nsh &quot;oc login -u ${user} -p ${pass} ${openshiftUrl} --insecure-skip-tls-verify&quot;\ntry {\nsh &quot;oc import-image ${props.name} --namespace=${openShiftNamespace} --from=${dockerRegistry}/${props.name}:${dockerTag} --confirm&quot;\n} catch (e) {\nsh &quot;&quot;&quot;\noc logs \\$(oc get builds -l build=${props.name} --namespace=${openShiftNamespace} --sort-by=.metadata.creationTimestamp -o name | tail -n 1) --namespace=${openShiftNamespace}\nthrow e\n&quot;&quot;&quot;\n}\n}\n}\nNote\nif your project is a maven project you should read the pom.xml file instead of the package.json, you could do it with the next command def pom = readMavenPom file: &apos;pom.xml&apos;. Due to the fact that there are different variable names between those two files, remember to modify ${props.name} for ${pom.artifactId} in the code.\nDeploy docker image using docker server\nTo deploy the docker image with this option you need to install docker and configure your docker server and also integrate it with Jenkins.\nscript {\ndocker.withRegistry(&quot;&quot;&quot;${dockerRegistryProtocol}${dockerRegistry}&quot;&quot;&quot;, dockerRegistryCredentials) {\ndef props = readJSON file: &apos;package.json&apos;\ndocker.image(&quot;${props.name}:${props.version}&quot;).pull()\ndef containerId = sh returnStdout: true, script: &quot;&quot;&quot;docker ps -aqf &quot;name=${containerName}${dockerEnvironment}&quot; &quot;&quot;&quot;\nif (containerId?.trim()) {\nsh &quot;docker rm -f ${containerId.trim()}&quot;\n}\nprintln &quot;&quot;&quot;docker run -d --name ${containerName}${dockerEnvironment} --network=${networkName} ${dockerRegistry}/${props.name}:${props.version}&quot;&quot;&quot;\nsh &quot;&quot;&quot;docker run -d --name ${containerName}${dockerEnvironment} --network=${networkName} ${dockerRegistry}/${props.name}:${props.version}&quot;&quot;&quot;\n}\n}\nNote\nif your project is a maven project you should read the pom.xml file instead of the package.json, you could do it with the next command def pom = readMavenPom file: &apos;pom.xml&apos;. Due to the fact that there are different variable names between those two files, remember to modify ${props.name} for ${pom.artifactId} and ${props.version} for ${pom.version} in the code.\nCheck status\nNow is time to check if your pods are running ok.\nTo check if your pods are ok in OpenShift you should add the next code to your pipeline:\nscript {\nprops = readJSON file: &apos;package.json&apos;\nsleep 30\nwithCredentials([usernamePassword(credentialsId: &quot;${openShiftCredentials}&quot;, passwordVariable: &apos;pass&apos;, usernameVariable: &apos;user&apos;)]) {\nsh &quot;oc login -u ${user} -p ${pass} ${openshiftUrl} --insecure-skip-tls-verify&quot;\nsh &quot;oc project ${openShiftNamespace}&quot;\ndef oldRetry = -1;\ndef oldState = &quot;&quot;;\nsh &quot;oc get pods -l app=${props.name} &gt; out&quot;\ndef status = sh (\nscript: &quot;sed &apos;s/[\\t ][\\t ]*/ /g&apos; &lt; out | sed &apos;2q;d&apos; | cut -d&apos; &apos; -f3&quot;,\nreturnStdout: true\n).trim()\ndef retry = sh (\nscript: &quot;sed &apos;s/[\\t ][\\t ]*/ /g&apos; &lt; out | sed &apos;2q;d&apos; | cut -d&apos; &apos; -f4&quot;,\nreturnStdout: true\n).trim().toInteger();\nwhile (retry &lt; 5 &amp;&amp; (oldRetry != retry || oldState != status)) {\nsleep 30\noldRetry = retry\noldState = status\nsh &quot;&quot;&quot;oc get pods -l app=${props.name} &gt; out&quot;&quot;&quot;\nstatus = sh (\nscript: &quot;sed &apos;s/[\\t ][\\t ]*/ /g&apos; &lt; out | sed &apos;2q;d&apos; | cut -d&apos; &apos; -f3&quot;,\nreturnStdout: true\n).trim()\nretry = sh (\nscript: &quot;sed &apos;s/[\\t ][\\t ]*/ /g&apos; &lt; out | sed &apos;2q;d&apos; | cut -d&apos; &apos; -f4&quot;,\nreturnStdout: true\n).trim().toInteger();\n}\nif(status != &quot;Running&quot;){\ntry {\nsh &quot;&quot;&quot;oc logs \\$(oc get pods -l app=${props.name} --sort-by=.metadata.creationTimestamp -o name | tail -n 1)&quot;&quot;&quot;\n} catch (e) {\nsh &quot;echo error reading logs&quot;\n}\nerror(&quot;The pod is not running, cause: &quot; + status)\n}\n}\n}\nPost operations\nWhen all its finish, remember to clean your workspace.\npost {\ncleanup {\ncleanWs()\n}\n}\nNote\nYou could also delete your dir adding the next command deleteDir().\nDockerfile\nYou have examples of dockerfiles in cicdgen repository.\ninside these folders you could find all the files that you need to use those dockerfiles. Two dockerfiles are provaided, Dockerfile and Dockerfile.ci, the first one is to compile the code and create the docker image used normally in local, and Dockerfile.ci is to use in Jenkins or similar, after building the application.\nvisit our devon4ng Dockerfiles.\nvisit our devon4j Dockerfiles.\nvisit our devon4node Dockerfiles.\nNote\nDockerfile.ci should be copied to de artifacts and renamed as Dockerfile to work. In the case or devon4ng and devon4node this is the dist folder, in case of devon4ng is on server/target folder.\n36.3.2. Automatic configuration\ncicdgen\nIf you are using production line for provisioning you could use cicdgen to configure automatically almost everything explained in the manual configuration. To do it see the cicdgen documentation.\n&#x2190;&#xA0;Previous:&#xA0;Configuration and services integration&#xA0;| &#x2191;&#xA0;Up:&#xA0;devonfw shop floor&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Deployment environments&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devonfw-shop-floor.asciidoc_deployment-environments.html","title":"37. Deployment environments","body":"\n37. Deployment environments\n37.1. OpenShift\n37.1.1. dsf4openshift deployment environment\nIn this section you will see how you can create a new environment instance in OpenShift and the things that you must add to the Jenkinsfiles of your repository to deploy a branch in this new environment. To conclude you are going to see how to add config files for environment in the source code of the applications.\nConfigure your OpenShift to deploy your devonfw projects\nPrerequisites\nOpenShift Cluster\nTo have your deployment environment with OpenShift you need to have an OpenShift Cluster.\nManual configuration\nHere you can find all that you need to know to configure OpenShift manually.\nAutomatic configuration\nHere you can find all that you need to know to configure OpenShift automatically.\nService integration with jenkins\nPrerequisites\nTo integrate it, you need to have installed the plugin OpenShift Client. To install it go to Manage Jenkins clicking on left menu and enter in Manage Plugins. Go to Available tab and search it using the filter textbox in the top right corner and install it.\nConfiguration\nSecond, you need to configure the OC Client. Go to Manage Jenkins clicking on left menu and enter in Global Tool Configuration.\nGo to OpenShift Client Tools section and add a new one like this.\nUpgrade your Jenkinsfile\nNow it is time to add/upgrade the next stages in to your Jenkinsfile:\nAdd create docker image stage.\nAdd deploy docker image stage.\nAdd check status stage.\nUpgrade Setup Jenkins tools stage.\nNote\nRemember to upgrade your parameters to difference which environment is used per branch.\n37.1.2. OpenShift deployment environment manual configuration\nIn this section you will see how you can create a new environment instance in your OpenShift cluster to deploy devonfw projects using docker images.\nPrerequisites\ndevonfw project\nYo need to have a devonfw project in a git repository or a docker image uploaded to a docker registry.\nComunication between components\nOpenshift must have access to docker registry.\nDownload OpenShift Client Tools\nFirst of all you need to download the OpenShift client, you can find it here.\nRemember that what you need to download oc Client Tools and not OKD Server.\nNote\nThis tutorial has been made with the version 3.10.0 of the client, it is recommended to use the most current client, but if it does not work, it is possible that the instructions have become obsolete or that the OpenShift used needs another older/newer version of the client. To download a specific version of the client you can find here the older versions and the version 3.10.0.\nAdd oc client to path\nOnce you have downloaded the client you have to add it to the PATH environment variable.\nLog into OpenShift with admin account\nYou can log using a terminal and executing the next instructions:\noc login $OpenShiftUrl\nNote\nYou need a valid user to log in.\nSelect the project where you are going to create the environment\noc project $projectName\nAdd all the secrets that you need\nFor example, to create a secret for a nexus repository you should execute the next commands:\noc create secret docker-registry $nameForSecret --docker-server=${dockerRegistry} --docker-username=${user} --docker-password=${pass} --docker-email=no-reply@email.com\nConfigure OpenShift\nConfigure builds to create docker image using OpenShift\nIf you need to create docker images of your projects you could use OpenShift to do it (Off course only if you have enough rights).\nTo do it, follow the next steps.\nCreate new builds configs\nThe first thing you need to do for create a new environment is prepare the buildconfigs for the front and for the middleware and rise default memory limits for the middleware. You can do it using a terminal and executing the next instructions:\nThese are a summary about the parameters used in our commands:\n${dockerRegistry}: The url of the docker repository.\n${props.name}: The name of the project (for example could be find on package.json)\n${dockerTag}: The tag of the image\nNote\nFrom now on you will refer to the name that you are going to give to the environment as $enviroment. Remember to modify it for the correct value in all instructions.\ndevon4ng build config\nYou need to create nginx build config with docker.\noc new-build --strategy docker --binary --docker-image nginx:alpine-perl --name=${props.name}-$environment --to=${dockerRegistry}/${props.name}:${dockerTag} --to-docker=true\nNote\nYou need nginx:alpine-perl to read the environment config file in openshift, if you are not going to use it, you could use nginx:latest instead.\ndevon4node build config\noc new-build --strategy docker --binary --docker-image node:lts --name=${props.name}-$environment --to=${dockerRegistry}/${props.name}:${dockerTag} --to-docker=true\ndevon4j build config\noc new-build --strategy docker --binary --docker-image openjdk:&lt;version&gt; --name=${props.name}-$environment --to=${dockerRegistry}/${props.name}:${dockerTag} --to-docker=true\nNote\nYou need to specify the &lt;version&gt; of java used for your project. Also you can use the -alpine image. This image is based on the popular Alpine Linux project. Alpine Linux is much smaller than most distribution base images (~5MB), and thus leads to much slimmer images in general. More information on docker hub.\nHow to use the build\nIn this step is where you will build a docker image from a compiled application.\nPrerequisite\nTo build the source in OpenShift, first of all you need to compile your source and obtain the artifacts &quot;dist folder&quot; or download it from a repository. Normally the artifacts have been built on Jenkins and have been stored in Nexus.\nTo download it, you can access to your registry, select the last version and download the &quot;.tar&quot;. The next image shows an example of where is the link to download it, marked in yellow:\nBuild in OpenShift\nWhen you have the artifacts, you can send them to your openshift and build them using your buildconfig that you created on the previous step. This is going to create a new docker image and push it to your registry.\nIf your docker registry need credentials you should use a secret. You could add it to your buildconfig using the next command:\noc set build-secret --push bc/${props.name}-$environment ${nameForSecret}\nNow you can use your build config and push the docker image to your registry. To do it you need to use a terminal and execute the following:\noc start-build ${props.name}-$environment --from-dir=${artifactsPath} --follow\nNote\n${artifactsPath} is the path where you have the artifacts of the prerequisite (On jenkins is the dist folder generated by the build).\nNote\nMaybe you need to raise your memory or CPU limits.\nConfigure new environment\nNow it is time to configure the environment.\nPrerequisite\nYou need a docker image of your application. You could create it using OpenShift as you see in the last step.\nCreate new app on OpenShift\nTo create new app you need to use the next command.\noc new-app --docker-image=${artifactsPath} --name=${props.name}-$environment --source-secret=${nameForSecret}\nNote\nYou could add environment variables using -e $name=$value\nNote\nIf you do not need to use a secret remove the end part of the command --source-secret=${nameForSecret}\nCreate routes\nFinally, you need add a route to access the service.\nAdd http route\nIf you want to create an http route execute the following command in a terminal:\noc expose svc/${props.name}-$environment\nAdd https route\nIf you want to create an https route you can do it executing the following command:\noc create route edge --service=${props.name}-$environment\nIf you want to change the default route path you can use the command --hostname=$url. For example:\noc expose svc/${props.name}-$environment --hostname=$url\noc create route edge --service=${props.name}-$environment --hostname=$url\nImport new images from registry\nWhen you have new images in the registry you must import them to OpenShift. You could do it executing the next commands:\noc import-image ${props.name}-$environment --from=${dockerRegistry}/${props.name}:${dockerTag} --confirm\nNote\nMaybe you need to raise your memory or CPU limits. It is explained below.\nRaise/decrease memory or CPU limits\nIf you need to raise (or decrease) the memory or CPU limits that you need you could do it for your deployments and builders configurations following the next steps.\nFor deployments\nYou could do it in OpenShift using the user interface. To do it you should enter in OpenShift and go to deployments.\nAt the right top, you could see a drop down actions, click on it and you could edit the resource limits of the container.\nMaybe you should modify the resource limits of the pod too. To do it you should click on drop down actions and go to edit YAML. Then you could see something like the next image.\nIn the image, you could see that appear resources two times. One at the bottom of the image, this are the container resources that you modified on the previous paragraph and another one at the top of the image. The resources of the top are for the pod, you should give to it at least the same of the sum for all containers that the pod use.\nAlso you could do it using command line interface and executing the next command:\nTo modify pod limits\noc patch dc/boat-frontend-test --patch &apos;{&quot;spec&quot;:{&quot;strategy&quot;:{&quot;resources&quot;:{&quot;limits&quot;:{&quot;cpu&quot;: &quot;100m&quot;, &quot;memory&quot;: &quot;100Mi&quot;}, &quot;requests&quot;:{&quot;cpu&quot;: &quot;100m&quot;, &quot;memory&quot;: &quot;100Mi&quot;}}}}}&apos;\nTo modify container limits\nWhen this guide was written Openshift have a bug and you cannot do it from command line interface.\nNote\nIf that command did not work and you received an error like this error: unable to parse &quot;&apos;{spec:&#x2026;&#x200B;&quot;: yaml: found unexpected end of stream, try to use the patch using &quot;&quot; instead of &apos;&apos;. It looks like this: --patch &quot;{\\&quot;spec\\&quot;:&#x2026;&#x200B;\\&quot;}}}}&quot;\nFor builders\nYou could do it using command line interface and executing the next command:\noc patch bc/${props.name}${APP_NAME_SUFFIX} --patch &apos;{&quot;spec&quot;:{&quot;resources&quot;:{&quot;limits&quot;:{&quot;cpu&quot;: &quot;125m&quot;, &quot;memory&quot;: &quot;400Mi&quot;},&quot;requests&quot;:{&quot;cpu&quot;: &quot;125m&quot;, &quot;memory&quot;: &quot;400Mi&quot;}}}}&apos;\nNote\nIf that command did not work and you received an error like this error: unable to parse &quot;&apos;{spec:&#x2026;&#x200B;&quot;: yaml: found unexpected end of stream, try to use the patch using &quot;&quot; instead of &apos;&apos;. It looks like this: --patch &quot;{\\&quot;spec\\&quot;:&#x2026;&#x200B;\\&quot;}}}}&quot;\n37.1.3. OpenShift deployment environment automatic configuration\nIn this section you will see how you can create a new environment instance in your OpenShift cluster to deploy devonfw projects using docker images.\nPrerequisites\nAdd OpenShift Client to Jenkins\nTo integrate it, you need to have installed the plugin OpenShift Client. To install it go to Manage Jenkins clicking on left menu and enter in Manage Plugins. Go to Available tab and search it using the filter textbox in the top right corner and install it.\nConfiguration OpenShift Client in Jenkins\nSecond, you need to configure the OC Client. Go to Manage Jenkins clicking on left menu and enter in Global Tool Configuration.\nGo to OpenShift Client Tools section and add a new one like this.\ndevonfw project\nYou need to have a devonfw project in a git repository or a docker image uploaded to a docker registry.\nComunication between components\nJenkins must have access to git, docker registry and OpenShift.\nOpenshift must have access to docker registry.\nJenkinsfiles to Configure OpenShift\nYou can find one Jenkinsfile per devonfw technology in devonfw shop floor repository to configure automatically your OpenShift cluster.\nHow to use it\nTo use it you need to follow the next steps\nCreate a new pipeline\nYou need to create a new pipeline in your repository and point it to Jenkinsfile in devonfw shop floor repository.\nNote: In the script path section you should use the Jenkinsfile of the technology that you need.\nBuild with parameters\nThe first time that you execute the pipeline is going to fail because Jenkins does not know that this pipeline needs parameters to execute. The better that you can do is stop it manually when Declarative: Checkout SCM is over.\nThen you could see a button to Build with Parameters, click on it and fill the next form, these are the parameters:\nDocker registry credentials for OpenShift\nCREATE_SECRET: This option allows you to add the credentials of your docker registry in your OpenShift and stored it as a secret called docker-registry + registry_secret_name_suffix value.\nRemember that you only need one secret to connect with your registry per namespace, if you are going to add more than one application in the same namespace that use the same registry, use the same name suffix and please do not create more than one secret in the same namespace. The namespace is the OpenShift project when you are going to deploy your application.\nYou can see your secrets stored in OpenShift going to OpenShift and click on the left menu:\nNote\nIf the secret exists, you should uncheck the checkbox and fill the name suffix to use it.\nREGISTRY_SECRET_NAME_SUFFIX: This is the suffix of the name for your docker registry credentials stored in OpenShift as a secret. The name is going to be docker-registry + this suffix, if you use more than one docker-registry in the same namespace you need to add a suffix. For example you could add the name of your project, then to have the name as docker-registry-myprojectname you should use -myprojectname value.\nBuild your docker image using OpenShift and store it in your docker registry\nCREATE_DOCKER_BUILDER: This option allows you to create a build configuration in your OpenShift to create the docker images of your project and store them in your docker registry. If you are going to create the builder, your application is needed, you need to specify where is your git repository and which is the branch and credentials to use it.\nThe following parameters of this section are only necessary if a builder is to be created.\nGIT_REPOSITORY: This is the url of your git repository.\nNote\nIf you are using production line, remember to use the internal rout of your repository, to use it you must change the base url of your production line for the internal route http://gitlab-core:80/gitlab. For example, if your production line repository is for example https://shared-services.pl.s2-eu.capgemini.com/gitlab/boat/boat-frontend.git use http://gitlab-core:80/gitlab/boat/boat-frontend.git)\nGIT_BRANCH: This is the branch that we are going to use for creating the first docker image. The next time that you are going to use the builder you could use another branches.\nGIT_CREDENTIALS: This is the credentials id stored in your jenkins to download the code from your git repository.\nBUILD_SCRIPT: In case of use devon4ng or devon4node you could specify which is the build script used to build and create the first docker image with this builder.\nJAVA_VERSION In case of use devon4j this is the java version used for your docker image.\nDocker registry information\nDOCKER_REGISTRY: This is the url of your docker registry.\nNote\nIf you are using production line, the url of your registry is docker-registry- + your production line url. For example, if your production line is shared-services.pl.s2-eu.capgemini.com your docker registry is docker-registry-shared-services.pl.s2-eu.capgemini.com.\nIf you cannot access to your docker registry, please open an incident in i4u.\nDOCKER_REGISTRY_CREDENTIALS: This is the credentials id stored in your jenkins to download or upload docker images in your docker registry.\nDOCKER_TAG: This is the tag that is going to be used for the builder to push the docker image and for the deployment config to pull and deploy it.\nOpenShift cluster information\nOPENSHIFT_URL: This is the url of your OpenShift cluster.\nOPENSHIFT_CREDENTIALS: This is the credentials id stored in your jenkins to use OpenShift.\nOPENSHIFT_NAMESPACE: This is the name of the project in your OpenShift where you are going to use. The name of the project in OpenShift is called namespace.\nTake care because although you see at the top of your OpenShift interface the name of the project that you are using, this name is the display-name and not the value that you need. To obtain the correct value you must check your OpenShift url like you see in the next image:\nAPP_NAME_SUFFIX: The name of all things created in your OpenShift project are going to be called as the configuration of your application says. Normaly, our projects use a suffix that depends on the environment. You can see the values in the next list:\nFor develop branch we use -dev\nFor release branch we use -uat\nFor master branch we use -prod\nHOSTNAME: If you do not specify nothing, OpenShift is going to autogenerate a valid url for your application. You could modify the value by default but be sure that you configure everything to server your application in the route that you specify.\nSECURED_PROTOCOL: If true, the protocol for the route will be https otherwise will be http.\nJenkins tools\nAll those parameters are the name of the tools in your Jenkinsfile.\nTo obtain it you need enter in your Jenkins and go to Manage Jenkins clicking on left menu and enter in Global Tool Configuration or in Managed files.\nOPENSHIFT_TOOL: Is located in Global tool configuration.\nNODEJS_TOOL: Is located in Global tool configuration.\nYARN_TOOL: Is located in Global tool configuration, inside the custom tools.\nGLOBAL_SETTINGS_ID Is located in Managed files. You need to click on edit button and take the id.\nMAVEN_INSTALLATION Is located in Global tool configuration.\n&#x2190;&#xA0;Previous:&#xA0;Create project&#xA0;| &#x2191;&#xA0;Up:&#xA0;devonfw shop floor&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Monitoring&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devonfw-shop-floor.asciidoc_monitoring.html","title":"38. Monitoring","body":"\n38. Monitoring\n38.1. Build monitor view\nThis tool you will be able to see in real time what is the state of your Jenkins pipelines.\n38.1.1. Prerequisites\nAdd build monitor view plugin\nTo integrate it, you need to have installed the build monitor view. To install it go to Manage Jenkins clicking on left menu and enter in Manage Plugins. Go to Available tab and search it using the filter textbox in the top right corner and install it.\n38.1.2. How to use it\nWhen you have build monitor view installed, you could add a new view clicking on the + tab in the top bar.\nNow you need to fill which is the name that you are goint to give to your view and select Build Monitor View option.\nThen you can see the configuration.\nIn Job Filters section you can specify which resources are going to be showed and whether subfolders should be included in the search.\nIn Build Monitor - View Settings you could specify which is the name at the top of the view and what is the ordering criterion.\nIn Build Monitor - Widget Settings you could specify if you want to show the committers and which is the field to display if it fails.\nAnd this is the output:\nYou could limit the colums and the text scale clicking on the gear button at the right top corner.\n&#x2190;&#xA0;Previous:&#xA0;Deployment environments&#xA0;| &#x2191;&#xA0;Up:&#xA0;devonfw shop floor&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Annexes&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devonfw-shop-floor.asciidoc_provisioning-environments.html","title":"34. Provisioning environments","body":"\n34. Provisioning environments\n34.1. Production Line provisioning environment\nThe Production Line Project is a set of server-side collaboration tools for Capgemini engagements. It has been developed for supporting project engagements with individual tools like issue tracking, continuous integration, continuous deployment, documentation, binary storage and much more!\nFor additional information use the official documentation.\n34.1.1. How to obtain your Production Line\nYou can order your Production Line environment instance following the official guide. Remember that you need to order at least the next tools:\n* Jenkins\n* GitLab\n* SonarQube\n* Nexus\nBack.\n34.2. dsf4docker provisioning environment\n34.2.1. Architecture overview\n34.2.2. Prerequisite\nTo use dsf4docker provisioning environment you need a remote server and you must clone or download devonfw shop floor.\n34.2.3. How to use it\nNavigate to ./devonfw-shop-floor/dsf4docker/environment and here you can find one scripts to install it, and another one to uninstall it.\nInstall devonfw shop floor 4 Docker\nThere is an installation script to do so, so the complete installation should be completed by running it. Make sure this script has execution permissions in the Docker Host:\n# chmod +x dsf4docker-install.sh\n# sudo ./dsf4docker-install.sh\nThis script, besides the container &quot;installation&quot; itself, will also adapt the docker-compose.yml file to your host (using sed to replace the IP_ADDRESS word of the file for your real Docker Host&#x2019;s IP address).\nUninstall devonfw shop floor 4 Docker\nAs well as for the installation, if we want to remove everything concerning devonfw shop floor 4 Docker from our Docker Host, we&#x2019;ll run this script:\n# chmod +x dsf4docker-uninstall.sh\n# sudo ./dsf4docker-uninstall.sh\n34.2.4. A little history\nThe Docker part of the shop floor is created based on the experience of the environment setup of the project Mirabaud Advisory, and intended to be updated to latest versions. Mirabaud Advisory is a web service developed with devonfw (Java) that, alongside its own implementation, it needed an environment both for the team to follow CICD rules through their 1-week-long sprints and for the client (Mirabaud) to check the already done work.\nThere is a practical experience about the Mirabaud Case.\nBack.\n&#x2190;&#xA0;Previous:&#xA0;How to use it&#xA0;| &#x2191;&#xA0;Up:&#xA0;devonfw shop floor&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Configuration and services integration&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devonfw-testing.asciidoc.html","title":"X. devonfw testing","body":"\nX. devonfw testing\nHome\nHow to install\nMr Checker Test Framework modules\n&#x2190;&#xA0;Previous:&#xA0;Template Development&#xA0;| &#x2191;&#xA0;Up:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Home&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-devonfw-testing.asciidoc_mr-checker-test-framework-modules.html","title":"50. Mr Checker Test Framework modules","body":"\n50. Mr Checker Test Framework modules\n50.1. Mr Checker Framework Modules\nThis is the structure of Mr Checker Framework Modules:\nIn this section, it is possible to find all information regarding the main modules of Mr Checker:\nCore Test Module\nSelenium Test Module\nWebAPI Test Module\nSecurity Test Module\nDatabase Test Module\nMobile Test Module\nStandalone Test Module\nDevOps Module\n50.2. Core Test Module\n50.2.1. What is Core Test Module\n50.2.2. Core Test Module Functions\nTest reports with logs and/or screenshots\nTest groups/tags\nData driven approach\nTest case parallel execution\nBDD - Gherkin - Cucumber approach\nRun on independent Operating Systems\nExternalize test environment (DEV, QA, SIT, PROD)\nEncrypting sensitive data\n50.2.3. How to start?\nRead: Framework Test Class\n50.2.4. Test Class\nOverview\nThe following image gives a general overview of the test class &quot;lifecycle&quot;.\nMore information on the methods and annotations used in this image can be found in the following chapter.\nMethods and annotations\nThe actual tests that will be executed are loacted in so called Test Classes. Starting a new project, a new package should be created.\nSource folder: mrchecker-app-under-test/src/test/java\nName: com.example.selenium.tests.tests.YOUR_PROJECT\nTest classes have to extend the BaseTest class.\npublic class DemoTest extends BaseTest {\n@Override\npublic void setUp() {\n}\n@Override\npublic void tearDown() {\n}\n}\nBasePage method: setUp\nThis method will be executed before the test. It allows objects to be instantiated, e.g. Page objects.\n@Override\npublic void setUp() {\nsomeTestPage = new SomeTestPage();\n}\nBasePage method: tearDown\nThe tearDown methods executes after the test. It allows the clean up of the testing environment.\nAnnotations\nThe @Test annotation indicates that the following method is a test method.\nAdditionally, there are two annotations that can help preparing and disassembling the test class: @BeforeClass and @AfterClass.\n@BeforeClass will execute the following method once at the beginning, before running any test method. Compared to the setUp() method provided by the BaseTest class, this annotation will only run once, instead of before every single test method. The advantage here: Things like login can be set up in @BeforeClass, as they can oftentimes be very time consuming. Loggin in on a webapplication once and afterwards running all the test methods is more efficient than loggin in before every test method, even though they are being executed on the same page.\n@AfterClass will execute after the last test method. Just like @BeforeClass this method will only run once, in contrary to the tearDown() method.\nInitialize a new test method by using the @Test annotation.\n@Test\npublic void willResultBeShown() {\n}\nThis method will interact with a page object in order to test it.\nSample setup\n@BeforeClass\npublic static void setUpBeforeClass() throws Exception {\nBFLogger.logInfo(&quot;[Step1] Login as Account Administrator&quot;);\n}\n@AfterClass\npublic static void tearDownAfterClass() throws Exception {\nBFLogger.logInfo(&quot;[Step4] Logout&quot;);\n}\n@Override\npublic void setUp() {\nBFLogger.logInfo(&quot;Open home page before each test&quot;);\n}\n@Override\npublic void tearDown() {\nBFLogger.logInfo(&quot;Clean all data updated while executing each test&quot;);\n}\n@Test\npublic void test1() {\nBFLogger.logInfo(&quot;[Step2] Filter by \\&quot;Creation Date\\&quot; - Descending&quot;);\nBFLogger.logInfo(&quot;[Step3] Set $1 for first 10 Users in column \\&quot;Invoice to pay\\&quot;&quot;);\n}\n@Test\npublic void test2() {\nBFLogger.logInfo(&quot;[Step2] Filter by \\&quot;Invoice to pay\\&quot; - Ascending&quot;);\nBFLogger.logInfo(&quot;[Step3] Set $100 for first 10 Users in column \\&quot;Invoice to pay\\&quot;&quot;);\n}\n50.3. Selenium Test Module\n50.3.1. What is Mr Checker E2E Selenium Test Module\n50.3.2. Selenium Structure\nWhat is Selenium\nWhat is WebDriver\nWhat is Page Object Model/Pattern\nList of web elements (Button, Dropdown, Checkbox, Alert Popup, etc.)\n50.3.3. Framework Features\nConstruction of Framework Page Class\nEvery Page class must extend BasePage\nWhat is isLoaded(), load() and pageTitle() for\nHow to create selector variable - &apos;private static final By ButtonOkSelector = By.Css(&#x2026;&#x200B;)&apos;\nHow to prepare everlasting selector - documentation\nMethod/action naming convention - documentation\nWhy we should use findElementDynamic() and findElementQuietly() instead of classic Selenium findElement\nList of well-rounded groups of user friendly actions (ElementButton, ElementCheckbox, ElementInput, etc.)\nVerification points of well-defined Page classes and Test classes - documentation\nRun on different browsers: Chrome, Firefox, IE, Safari, Edge\nRun with different browser options\nRun with full range of resolution (mobile and desktop): Testing Response Design Webpage\n50.3.4. How to start?\nRead: My first Selenium Test\n50.3.5. Selenium Best Practices\nTable of best practices\n50.3.6. Selenium UFT Comparison\nSelenium UFT Comparison\n50.3.7. Sample Walkthrough\nThis page will walk you through the process of creating a test case. We&#x2019;ll create a very simple test for the Google search engine.\nTest Procedure\nWe would like to open the Google search engine, enter some search query and afterwards submit the form. We hope to see some results being listed, otherwise the test will fail. Summarized, the testing process would look like this.\nOpen google.com\nEnter the string &quot;Test&quot; into the searchbox\nSubmit the form\nGet the results and check if the result list is empty\nCreating new packages\nWe will need two new packages, one for the new page classes, the other one for our test classes.\nCreating package for test classes\nOpen Eclipse, use the &quot;Project Explorer&quot; on the left to navigate to\nmrchecker-app-under-test &#x2192; src/test/java &#x2192; com.example &#x2192; selenium.tests &#x2192; tests\nRight click on &quot;tests&quot;, click on &quot;New&quot; &#x2192; New Package. We&#x2019;ll the new package &quot;com.example.selenium.tests.googleSearch&quot;.\nCreating package for page classes\nNavigate to\nmrchecker-app-under-test &#x2192; src/main/java &#x2192; com.example &#x2192; selenium &#x2192; pages\nRight click on &quot;pages&quot;, click on &quot;New&quot; &#x2192; New Package. The new package will be called &quot;com.example.selenium.pages.googleSearch&quot;.\nCreating the test class\nThe test class will contain the entire testing-routine. At first, we&#x2019;ll create a new class inside of our newly created &quot;googleSearch&quot; package (under src/test/java) and call it &quot;GoogleSearchTest&quot;.\nAs &quot;GoogleSearchTest&quot; is a test class, it has to extend the BaseTest class. You may have to import some required packages and afterwards include a few required methods.\npublic class GoogleSearchTest extends BaseTest {\n@Override\npublic void setUp() {\n}\n@Override\npublic void tearDown() {\n}\n}\nNow, we&#x2019;ll need a new Page object, which will represent the Google Search page. The page class will be named &quot;GoogleSearchPage&quot;.\nprivate GoogleSearchPage googleSearchPage;\n@Override\npublic void setUp() {\ngoogleSearchPage = new GoogleSearchPage();\n}\nCreating the GoogleSearchPage class\nWe created a new field for the GoogleSearchPage class and instantiated an object in the setUp() method. As this class doesn&#x2019;t exist yet, we&#x2019;ll have to create it inside of the googleSearch page class package.\nWe extend the GoogleSearchPage class with BasePage, import all necessary packages and include all required methods.\npublic class GoogleSearchPage extends BasePage {\n@Override\npublic boolean isLoaded() {\nreturn false;\n}\n@Override\npublic void load() {\n}\n@Override\npublic String pageTitle() {\nreturn &quot;&quot;;\n}\n}\nAs this page class represents the Google homepage, we have to set up selectors for web elements required in our test case.\nIn our example we have to create a selector for the search bar which we&#x2019;ll interact with. The selector will be implemented as a field.\nprivate static final By selectorGoogleSearchInput = By.css(#lst-ib);\nThe input field&#x2019;s id #lst-ib was found by using the developer console in Google Chrome.\nThis selector can be used to create a WebElement object of said search bar. Therefore, we&#x2019;ll create a new method and call it &quot;enterGoogleSearchInput&quot;.\npublic GoogleResultPage enterGoogleSearchInput(String searchText) {\nWebElement googleSearchInput = getDriver().findDynamicElement(selectorGoogleSearchInput);\ngoogleSearchInput.sendKeys(searchText);\ngoogleSearchInput.submit();\nreturn new GoogleResultPage();\n}\nAs you can see, we return another page object that wasn&#x2019;t yet created. This step is required, as the results that we would like to check are on another Google Page. This means we&#x2019;ll have to create another page class, which will be shown later.\nFinally, the empty methods inherited from the BasePage class have to be filled:\n@Override\npublic boolean isLoaded() {\nif(getDriver().getTitle().equals(pageTitle())) {\nreturn true;\n}\nreturn false;\n}\n@Override\npublic void load() {\ngetDriver().get(&quot;http://google.com&quot;);\n}\n@Override\npublic String pageTitle() {\nreturn &quot;Google&quot;;\n}\nThe method isLoaded() checks if the page was loaded by comparing the actual title with the expected title provided by the method pageTitle(). The load() method simply loads a given URL, in this case http://google.com.\nThe completion of these methods finalizes our GoogleSearchPage class. Now we still have to create the GoogleResultPage class mentioned before. This page will deal with the elements on the Google search result page.\nCreating the GoogleResultPage class\nBy right-clicking on the &quot;pages&quot; package, we&#x2019;ll navigate to &quot;new&quot; &#x2192; &quot;Class&quot; to create a new class.\nThe GoogleResultPage class also has to extend BasePage and include all required methods. Next, a new selector for the result list will be created. By using the result list, we can finally check if the result count is bigger than zero and the search request therefore was successful.\nprivate static final By selectorResultList = By.cssSelector(&quot;#res&quot;);\nWe&#x2019;ll use this selector inside a new getter-method, which will return all ListElements.\npublic ListElements getResultList() {\nreturn getDriver().elementList(selectorResultList);\n}\nThis method will allow the testcase to simply get the result list and afterwards check if the list is empty or not.\nFinally, we have to complete all inherited methods.\n@Override\npublic boolean isLoaded() {\ngetDriver().waitForPageLoaded();\nif(getDriver().getCurrentUrl().contains(&quot;search&quot;)) {\nreturn true;\n}\nreturn false;\n}\n@Override\npublic void load() {\nBFLogger.logError(&quot;Google result page was not loaded.&quot;);\n}\n@Override\npublic String getTitle() {\nreturn &quot;&quot;;\n}\nThe method isLoaded() differs from the same method in GoogleSearchPage, because this site is being loaded as a result from a previous action. That&#x2019;s why we&#x2019;ll have to use the method getDriver().waitForPageLoaded() to be certain, that the page was loaded completely. Afterwards we check if the current URL contains the term &quot;search&quot;, as it only occurs on the result page. This way we can check if we&#x2019;re on the right page.\nAnother result of this page being loaded by another object, we don&#x2019;t have to load any specific URL. We just add a BFLogger instance to print an error message if the page was not successfully loaded.\nAs we don&#x2019;t use the getTitle() method we simply return an empty String.\nFinally, all required page classes are complete and we can finalize the test class.\nFinalizing the test class\nAt this point, our GoogleSearchTest class looks like this:\npublic class GoogleSearchTest {\nprivate GoogleSearchPage googleSearchPage;\n@Override\npublic void setUp() {\ngoogleSearchPage = new GoogleSearchPage();\n}\n@Override\npublic void tearDown() {\n}\n}\nNext up, we&#x2019;ll create the test method, let&#x2019;s call it shouldResultReturn().\n@Test\npublic void shouldResultReturn() {\nGoogleResultPage googleResultPage = googleSearchPage.enterGoogleSearchInput(&quot;Test&quot;);\nListElements results = googleResultPage.getResultList();\nassertTrue(&quot;Number of results equals 0&quot;, results.getSize() &gt; 0);\n}\nCode explanation: At first, we will run the enterGoogleSearchInput() method on the GoogleSearchPage with the parameter &quot;Test&quot; to search for this exact string on Google. As this method returns a GoogleResultPage object, we will store this in the local variable googleResultPage.\nAfterwards, we get the result list by utilizing the getter method that we created before. Finally, we create an assertion: We expect the list size to be bigger than zero, meaning that the google search query was successful as we received results. If this assertion is wrong, a message will be printed out, stating that the number of results equals zero.\nWe can run the test by right clicking on the test method &#x2192; Run as &#x2192; JUnit test.\nAfter starting the test, you&#x2019;ll notice a browser window opening, resizing to given dimensions, opening Google, entering the query &quot;Test&quot; and submitting the form. After completing the test, you&#x2019;ll see the test results on the right side of Eclipse. A green color indicator means that the test was successful, red means the test failed.\nThis walkthrough should&#x2019;ve provided you with the basic understanding on how the framework can be used to create test cases.\n50.4. WebAPI Test Module\n50.5. Service Virtualization\nWhat is service virtualization\nHow plug in service virtualization into Application Under Test\nHow to make virtual asset\nSmoke Tests virtualization\n50.6. Security Test Module\n50.6.1. What is Security?\nApplication Security is concerned with Integrity, Availability and Confidentiality of data processed, stored and transferred by the application.\nApplication Security is a cross-cutting concern which touches every aspect of the Software Development Lifecycle. You can introduce some SQL injection flaws in your application and make it exploitable, but you can also expose your secrets due to poor secret management process (which will have nothing to do with code itself), and fail as well.\nBecause of this, and many other reasons, not every aspect of security can be automatically verified. Manual tests and audits will be still needed. Nevertheless, every security requirement which are automatically verified, will prevent code degeneration and misconfiguration in a continuous manner.\n50.6.2. How to test Security?\nSecurity tests can be performed in many different ways like:\nStatic Code Analysis - improves the security by (usually) automated code review. Good way to search after vulnerabilities, which are &apos;obvious&apos; on the code level (like e.g. SQL injection). The downside is that the professional tools to perform such scans are very expensive and still produce many false positives.\nDynamic Code Analysis - tests are run against a working environment. Good way to search after vulnerabilities, which require all client- and server-side components to be present and running (like e.g. Cross-Site Scripting). Tests are performed in a semi-automated manner and require a proxy tool (like e.g. OWASP ZAP)\nUnit tests - self written and maintained tests. They work usually on the HTTP/REST level (as this defines the trust boundary between the client and the server) and run against a working environment. Unit tests are best suited to verify requirements which involve business knowledge of the system or which assure secure configuration on the HTTP level.\nIn the current release of the Security Module the main focus will be Unit Tests.\nAlthough the most common choice of environment for security tests to run on will be integration (as the environment offers the right stability and should mirror the production closely), it is not uncommon for some security tests to run on production as well. This is done for e.g. TLS configuration testing to ensure proper configuration of the most relevant environment in a continuous manner.\n50.6.3. Scope definition\n50.7. Database Test Module\n50.7.1. What is Mr. Checker Database Test Module\nDatabase module is based on Object-Relational Mapping programming technique. All functionalities are build using Java Persistence API but examples use Hibernate as a main provider.\n50.7.2. JPA structure schema\nThis module was written to allow the use of any JPA provider. Structure is represented by schema below.\n50.7.3. ORM representation applied in Framework\n50.8. Mobile Test Module\n50.9. Standalone Test Module\n50.10. DevOps Module\n50.10.1. What is DevOps for us?\nDevOps consists of a mixture of three key components in a technical project:\nPeople skills and mindset\nProcesses\nTools\nBy using E2E Mr Checker Test Framework it is possible to cover the majority of these areas.\n50.10.2. QA Team Goal\nFor QA engineers, it is essential to take care of the product code quality.\nTherefore, we have to understand, that a test case is also a code which has to be validated against quality gates.\nAs a result, we must test our developed test case alike it is done during standard Software Delivery Life Cycle.\n50.10.3. Well rounded test case production process\nHow do we define top notch test cases development process in E2E Mr Checker Test Framework\n50.10.4. Continuous Integration (CI) and Continuous Delivery (CD)\nContinuous Integration (CI) - procedure where quality gates validate test case creation process\nContinuous Delivery (CD) - procedure where we include as smoke/regression/security created test cases, validated against CI\n50.10.5. What should you receive from this DevOps module\n50.10.6. What will you gain with our DevOps module\nThe CI procedure has been divided into transparent modules. This solution makes configuration and maintenance very easy because everyone is able to manage versions and customize the configuration independently for each module. A separate security module ensures the protection of your credentials and assigned access roles regardless of changes in other modules.\nYour CI process will be matched to the current project. You can easily go back to the previous configuration, test a new one or move a selected one to other projects.\nDevOps module supports a delivery model in which executors are made available to the user as needed. It has advantages such as:\nSaving computing resources\nEliminating guessing on your infrastructure capacity needs\nNot spending time on running and maintaining additional executors\n50.10.7. How to build this DevOps module\nIf you want to install the module, please click the link below.\nInstallation should not take more than a few minutes\nDevOps module installation\nOnce you have implemented the module, you can learn more about:\nBuilding jobs &amp; Running builds\nDocker commands\n50.10.8. Continuous Integration\nEmbrace quality with Continuous Integration while you produce test case/s.\nOverview\nThere are two ways to set up your Continuous Integration environment:\nCreate a Jenkins instance from scratch (e.g. by using the Jenkins Docker image)\nUsing a clean Jenkins instance requires the installation of additional plugins. The plugins required and their versions can be found on this page.\nUse thre pre-configured custom Docker image provided by us\nNo more additional configurations is required (but optional) using this custom Docker image. Additionally, this Jenkins setup allows to be dynamically scaled across multiple machines and even the cloud (AWS, Azure, Google Cloud etc.).\nJenkins Overview\nJenkins is an Open Source Continuous Integration Tool. It allows the user to create automated build jobs which will run remotely on so called Jenkins Slaves. A build job can be triggered by several events, for example on new pull request on specified repositories or timed (e.g. at midnight).\nJenkins Configuration\nTests created by using the testing framework can be easily implemented on a Jenkins instance. The following chapter will describe such a job configuration. If you&#x2019;re running your own Jenkins instance you may have to install additional plugins listed on the page Jenkins Plugins for a trouble-free integration of your tests.\nInitial Configuration\nThe test job is configured as a so-called parametrized job. This means, after starting the job, parameters can be specified, which will then be used in the build process. In this case, branch and testname will be expected when starting the job. These parameters specify which branch in the code repository should be checked out (possibly feature branch) and the name of the test, that should be executed.\nBuild Process Configuration\nThe first step inside the build process configuration is to get the author of the commit that was made. The mail will be extracted and gets stored in a file called build.properties. This way, the author can be notified if the build fails.\nNext up, Maven will be used to check if the code can be compiled, without running any tests.\nAfter making sure that the code can be compiled, the actual tests will be executed.\n+\nimage::images/jenkins-build-3.png[&quot;Starting the actual tests&quot;, width=&quot;450&quot;, link=&quot;images/jenkins-build-3.png&quot;]\nFinally, reports will be generated.\nPost Build Configuration\nAt first, the results will be imported to the Allure System\nJUnit test results will be reported as well. Using this step, the test result trend graph will be displayed on the Jenkins job overview.\nFinally, an E-Mail will be sent to the previously extracted author of the commit.\nUsing the Pre-Configured Custom Docker Image\nIf you are starting a new Jenkins instance for your tests, we&#x2019;d suggest to use the pre-configured Docker image. This image already contains all configurations and additional features.\nThe configurations that are made are e.g. Plugins and Pre-Installed job setup samples. This way, you don&#x2019;t have to set up the entire CI-Environment from ground up.\nThe additional features from this docker image allow the dynamic creation and deletion of Jenkins slaves, by creating Docker containers. Also, Cloud Solutions can be implemented to allow wide-spread load balancing.\n50.10.9. Continuous Delivery\nInclude quality with Continuous Delivery during product release.\nimage::images/devops/CD.png[&quot;CD&quot;, width=&quot;450&quot;, link=&quot;image/devops/CD.png&quot;]\nOverview\nCD from Jenkins point of view does not change a lot from Continuous Integration one.\nJenkins Overview\nFor Jenkins CD setup please use the same Jenkins settings as for CI link\nThe only difference is:\nWhat type of test you we will execute. Before we have been picking test case(s), however now we will choose test suite(s)\nWho will will trigger given Smoke/Integration/Performance job\nWhat is the name of official branch. This branch ought to use be used always in every CD execution. It will be either master, or develop.\nJenkins for Smoke Tests\nIn point where we input test name - $TESTNAME ( link ), please input test suite which merge by tags -( link ) how to all test cases need to run only smoke tests.\nJenkins for Performance Tests\nUnder construction - added when WebAPI module is included.\n50.10.10. Selenium Grid\nWhat is Selenium Grid\nSelenium Grid allows running web/mobile browsers test cases to fulfil bedrock factors, such as:\nIndependence infrastructure, similar to end-users\nScalable infrastructure (\\~50 simultaneous sessions at once)\nHuge variety of web browsers (from mobile to desktop)\nContinuous Integration and Continuous Delivery process\nSupport multi-type programming languages (java, javascript, python, &#x2026;&#x200B;).\nOn a daily basis, a test automation engineer uses his local environments for test case execution/development.\nHowever, created browser test case has to be able to run on any other infrastructure. Selenium Grid enables this portability for us.\nSelenium Grid Structure\nFull documentation for Selenium Grid can be found here: here and here.\n&apos;Vanilla flavour&apos; Selenium Grid is based on two, not too complicated, ingredients:\nSelenium Hub - as one machine, accepting connections to grid from test cases executors. It also plays a managerial role in connection to/from Selenium Nodes\nSelenium Node - from one to many machines, where on each machine a browser used during test case execution is installed.\nHow to setup\nThere are two options of Selenium Grid setup:\nClassic, static solution - link\nCloud, scalable solution - link\nAdvantages and disadvantages of both solutions:\nHow to use Selenium Grid with E2E Mr Checker Test Frameworks\nRun following command either in Eclipse or in Jenkins:\n&gt; mvn test -Dtest=com.capgemini.ntc.selenium.tests.samples.resolutions.ResolutionTest -DseleniumGrid=&quot;http://10.40.232.61:4444/wd/hub&quot; -Dos=LINUX -Dbrowser=chrome\nAs a result of this command:\n-Dtest=com.capgemini.ntc.selenium.features.samples.resolutions.ResolutionTest - name of test case to execute\n-DseleniumGrid=&quot;http://10.40.232.61:4444/wd/hub&quot; - IP address of Selenium Hub\n-Dos=LINUX - what operating system must be taken during test case execution\n-Dbrowser=chrome - what type of browser will be used during test case execution\n50.10.11. What is Docker\nDocker - open source software platform to create, deploy and manage virtualized application containers on a common operating system (OS), with an ecosystem of allied tools.\n50.10.12. Where do we use Docker\nDevOps module consists of Docker images\nJenkins image\nJenkins job image\nJenkins management image\nSecurity image\nin addition, each new node is also based on the Docker\n50.10.13. Exploring basic Docker options\nLet&#x2019;s expose some of the most important commands that are needed when working with our DevOps module based on the Docker platform. Each command given below should be preceded by a sudo call by default. If you don&#x2019;t want to use sudo command create a Unix group called docker and add user to it.\n$ sudo groupadd docker\n$ sudo usermod -aG docker $USER\nBuild an image from a Dockerfile\n# docker build [OPTIONS] PATH | URL | -\n#\n# Options:\n# --tag , -t : Name and optionally a tag in the &#x2018;name:tag&#x2019; format\n$ docker build -t vc_jenkins_jobs .\nContainer start\n# docker run [OPTIONS] IMAGE[:TAG|@DIGEST] [COMMAND] [ARG...]\n#\n# Options:\n# -d : To start a container in detached mode (background)\n# -it : interactive terminal\n# --name : assign a container name\n# --rm : clean up\n# --volumes-from=&quot;&quot;: Mount all volumes from the given container(s)\n# -p : explicitly map a single port or range of ports\n# --volume : storage associated with the image\n$ docker run -d --name vc_jenkins_jobs vc_jenkins_jobs\nRemove one or more containers\n# docker rm [OPTIONS] CONTAINER\n#\n# Options:\n# --force , -f : Force the removal of a running container\n$ docker rm -f jenkins\nList containers\n# docker ps [OPTIONS]\n# --all, -a : Show all containers (default shows just running)\n$ docker ps\nPull an image or a repository from a registry\n# docker pull [OPTIONS] NAME[:TAG|@DIGEST]\n$ docker pull jenkins/jenkins:2.73.1\nPush the image or a repository to a registry\nPushing new image takes place in two steps. First save image by adding container ID to commit command and next use push:\n# docker push [OPTIONS] NAME[:TAG]\n$ docker ps\n# copy container ID from the result\n$ docker commit b46778v943fh vc_jenkins_mng:project_x\n$ docker push vc_jenkins_mng:project_x\nReturn information on Docker object\n# docker inspect [OPTIONS] NAME|ID [NAME|ID...]\n#\n# Options:\n# --format , -f : output format\n$ docker inspect -f &apos;{{ .Mounts }}&apos; vc_jenkins_mng\nList images\n# docker images [OPTIONS] [REPOSITORY[:TAG]]\n#\n# Options:\n--all , -a : show all images with intermediate images\n$ docker images\n$ docker images jenkins\nRemove one or more images\n# docker rmi [OPTIONS] IMAGE [IMAGE...]\n#\n# Options:\n# --force , -f : Force removal of the image\n$ docker rmi jenkins/jenkins:latest\nRun a command in a running container\n# docker exec [OPTIONS] CONTAINER COMMAND [ARG...]\n# -d : run command in the background\n# -it : interactive terminal\n# -w : working directory inside the container\n# -e : Set environment variables\n$ docker exec vc_jenkins_jobs sh -c &quot;chmod 755 config.xml&quot;\n50.10.14. Advanced commands\nRemove dangling images\n$ docker rmi $(docker images -f dangling=true -q)\nRemove all images\n$ docker rmi $(docker images -a -q)\nRemoving images according to a pattern\n$ docker images | grep &quot;pattern&quot; | awk &apos;{print $2}&apos; | xargs docker rm\nRemove all exited containers\n$ docker rm $(docker ps -a -f status=exited -q)\nRemove all stopped containers\n$ docker rm $(docker ps --no-trunc -aq)\nRemove containers according to a pattern\n$ docker ps -a | grep &quot;pattern&quot; | awk &apos;{print $1}&apos; | xargs docker rmi\nRemove dangling volumes\n$ docker volume rm $(docker volume ls -f dangling=true -q)\n50.10.15. How to build DevOps module\nPrerequisites\n64-bit Linux operating server system (recommended: Ubuntu 16.04 LTS server - Download\nNon-root user with sudo privileges (the default user created during the operating system installation process)\nDocker - open source software platform to create, deploy and manage virtualized application containers on a common operating system (OS), with an ecosystem of allied tools.\ngit - version control system software to clone module code\nDocker service installation\n$ curl -fsSL get.docker.com -o get-docker.sh\n$ sh get-docker.sh\nGit installation\n$ sudo apt-get install git\nCreating special system users\n$ sudo useradd -M jenkins\n$ sudo usermod -L jenkins\n$ sudo usermod -a -G docker jenkins\nCloning the repository\nCheck Your current directory, create a new for DevOps module in /home/&lt;YourUserName&gt;. Open it and enter git clone command\n$ pwd\n/home/&lt;YourUserName&gt;/\n$ mkdir dev_ops_module\n$ cd dev_ops_module\n$ pwd\n/home/&lt;YourUserName&gt;/dev_ops_module\n$ git clone https://bitbucket.org/lukasz_stefaniszyn/jenkinsdockercompose.git\nEnabling Docker remote API\nEnable Docker remote API - By default, due to security reasons, Docker runs via a non-networked Unix socket. This solution allows only local communication. The Docker daemon in Ubuntu 16.04 is configured by the system, so You need to modify file /lib/systemd/system/docker.service. You&#x2019;ll enable the access to the Docker daemon from specific IP address.\nAllow port communication:\n$ sudo ufw allow 4243/tcp\n$ sudo ufw allow 32000:33000/tcp\nOpen file docker service and replace variable ExecStart value:\n$ sudo nano /lib/systemd/system/docker.service\n#\n# Find variable ExecStart and change value to: /usr/bin/dockerd -H tcp://0.0.0.0:4243 -H unix:///var/run/docker.sock\n#\n$ ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:4243 -H unix:///var/run/docker.sock\nFinish editing this file: Nano: Ctrl + X and Y Vi: Esc + wq\nRestart deamon and docker service:\n$ sudo systemctl daemon-reload\n$ sudo service docker restart\nPlease, test the configuration:\n$ curl --noproxy GET http://127.0.0.1:4243/version\nThe answer should be similar to the one shown in the picture:\nJenkins with Docker\nExecute commands:\n$ sudo apt-get upgrade -y\n$ sudo apt-get install -y sudo libltdl-dev\n$ GID=$(cut -d: -f3 &lt; &lt;(getent group docker))\n50.10.16. Running module\nOpen JenkinsDockerCompose directory and print the path by pwd command:\n$ pwd\n/home/&lt;YourUserName&gt;/dev_ops_module\n$ cd jenkinsdockercompose\n$ pwd\n/home/&lt;YourUserName&gt;/dev_ops_module/jenkinsdockercompose/\nLet&#x2019;s edit a configuration file create_and_run.sh. You can use default system text editors such as &quot;nano&quot; or &quot;vi&quot;:\n$ nano create_and_run.sh\nReplace variable value in the second line with the path previously displayed: /home/&lt;YourUserName&gt;/dev_ops_module/jenkinsdockercompose/\n1 echo &quot;Set global variables&quot;\n2 REPO_HOME=/home/&lt;DefaultUserName&gt;/dev_ops_module/jenkinsdockercompose/\nFinish editing this file: Nano: Ctrl + X and Y Vi: Esc + wq and run the script:\n$ sudo ./create_and_run.sh\nWait until the end of the building process. It can take a few minutes. What happened there?\nSetting global variables\nRemoving older docker images (if they exist)\nBuilding jenkins_home_security image\nBuilding jenkins_home_jobs image\nBuilding jenkins_home_mng image\nStart Jenkins\nRun web browser with address http://&lt;server-ip-address&gt;:8080. If the configuration is correct, the Jenkins main page will be displayed.\nConfiguring slaves\nLogin as jenkins admin and run web browser with address http://&lt;server-ip-address&gt;:8080/configure\nFind API ip address option and change it to http://&lt;server-ip-address&gt;:4243\n50.10.17. Adding a new item\nThe easiest way to create a new job is to use the &quot;new item&quot; option. You can create a whole new item or copy the configuration from an already existing job with changing only the parameters we are interested in.\nClick &quot;new item&quot; in the menu and next and on the next page enter an item name and choose the type.\nIf you want, you can copy an existing configuration:\n50.10.18. Configuring job\nEach job can be configured according to your preferences. This is done on the page available in the menu of the job page. Just click on &quot;configure&quot;.\nLoad the main page again and next look at the middle of the page. Two job catalogs are displayed there.\nClick &quot;Training&quot; directory name. You will be taken to a page with a list of available jobs. In the default configuration there are:\nselenium_workshop\nselenium_workshop_cucumber\nselenium_workshop_cucumberParallel\nClick on the name - &quot;selenium_workshop&quot;. You will be taken to the main page of the selected Job\nFrom the menu on the left select &quot;configure&quot;.\nThe configuration categories are displayed there:\nGeneral\nSource Code Management\nBuild Triggers\nBuild Environment\nBuild\nPost-build Actions\n50.10.19. Running example build\nLet create the first build. Choose selenium workshop main page again.\nClick the link &quot;Build with parameters &quot; in the menu on the left. On this page you can choose the configuration of the branch name and the name of the test that will run.\nClick the button and see that your build has been targeted.\nAfter assigning to him the executor it will be executed.\nIt is also possible to preview the build page:\n50.10.20. Editing the list of plugins\nPlease, open /home/&lt;YourUserName&gt;/dev_ops_module/jenkinsdockercompose/jenkins_home_mng directory. You should see file plugins.txt on the list. Please edit it. It is possible to add or remove plugins that interest you.\n$ cd /home/&lt;YourUserName&gt;/dev_ops_module/jenkinsdockercompose/jenkins_home_mng\n$ ls\n$ nano plugins.txt\nFinish editing this file: Nano: Ctrl + X and Y Vi: Esc + wq. After this operation, the application must be restarted:\n* DevOps module installation\nThe second way is to edit the plugins option in the GUI.\n50.10.21. User management\nPlease, open /home/&lt;YourUserName&gt;/dev_ops_module/jenkinsdockercompose/jenkins_home_security/jenkins_home/users directory. Print content of it by ls command. You should see list of jenkins users.\n$ cd /home/&lt;YourUserName&gt;/dev_ops_module/jenkinsdockercompose/jenkins_home_security/jenkins_home/users\n$ ls\nAdding new user\nThe easiest way to create a new user is to copy his configuration file from one of the existing users and substitute the required options.\n# Please create directory for new user and copy files\n#\n$ mkdir newUserName\n$ cd newUserName\n$ cp ../user1/config.xml .\n$ ls\n# You should see Your config. Please edit chosen option if You want\n$ nano config.xml\nFinish editing this file: Nano: Ctrl + X and Y\nRemoving user\nOpen default directory of users again: /home/&lt;YourUserName&gt;/dev_ops_module/jenkinsdockercompose/jenkins_home_security/jenkins_home/users. Choose user name to remove and execute it by Unix rm command:\n$ cd /home/&lt;YourUserName&gt;/dev_ops_module/jenkinsdockercompose/jenkins_home_security/jenkins_home/users\n$ ls\n# execute commmand : rm -rf &lt;userName&gt;\n# ! Take special care, executing the following command in the wrong directory can cause undesirable effects for other\n# applications or the entire operating system\n$ rm -rf user1\n&#x2190;&#xA0;Previous:&#xA0;How to install&#xA0;| &#x2191;&#xA0;Up:&#xA0;devonfw testing&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;MyThaiStar&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-general-start.asciidoc.html","title":"I. Getting Started","body":"\nI. Getting Started\ndevonfw Introduction\nWhy should I use devonfw?\nDownload and Setup\nThe Devon IDE\n&#x2191;&#xA0;Up:&#xA0;devonfw guide&#xA0;| Next:&#xA0;devonfw Introduction&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-ide.asciidoc.html","title":"II. devonfw ide","body":"\nII. devonfw ide\nIntegrated Development Environment\n&#x2190;&#xA0;Previous:&#xA0;The Devon IDE&#xA0;| &#x2191;&#xA0;Up:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Integrated Development Environment&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-ide.asciidoc_integrated-development-environment.html","title":"5. Integrated Development Environment","body":"\n5. Integrated Development Environment\n5.1. devonfw-ide\nWelcome to the devonfw-ide!\nThe devonfw-ide is a fantastic tool to automatically download, install, setup and update the IDE (integrated development environment) of your software development projects.\nFor further details visit the following links:\nfeatures &amp; motivation\ndownload &amp; setup\nusage\n5.1.1. ATTENTION\nNote\nThe official public release of devonfw-ide is still pending due to some final clarification. The download links are not broken, but pointing to the place where the release will appear after the clarification is complete. If you want to use devonfw-ide today you can clone it from ide.git and run mvn install to build it, or contact us (check internal community annoucements).\nTL;DR?\ndownload\nextract to C:\\projects\\my-project-name or /projects/my-project-name\nrun setup script\nrun eclipse-main to launch Eclipse or vscode-main for Visual Studio code\nhappy hacking!\n5.1.2. License\nThe devonfw-ide is free and open-source. For details read our TERMS_OF_USE.\n5.2. Features\nEvery developer needs great tools to work efficient. Setting all this up manually can be tedious and error-prone. Further, different projects may require different versions and configurations of such tools. Especially configurations like code-formatters should be consistent within a project to avoid diff-wars.\nThe devonfw-ide will solve all these problems. Here are the features we can offer for you with devonfw-ide:\nEfficient\nSetup your IDE within minutes tailored for the requirements of your project.\nAutomated\nAutomate the setup and update, avoid manual steps and mistakes.\nSimple\nKISS (Keep It Small and Simple), no installers or tool-integrations that break with every release. Instead use archive-files (zip or tar.gz), templates and simple shell scripts.\nConfigurable\nYou can tweak the configuration to your needs. Further the settings contain configuration templates for the different tools (see configurator).\nMaintainable\nFor your project you should copy these settings to an own git repository that can be maintained and updated to manage the tool configurations during the project lifecycle. If you use github or gitlab every developer can easily suggest changes and improvements to these settings via pull/merge requests without ending in chaos with big teams.\nCustomizable\nYou need an additional tool that we never heard of? Put it in the software folder of the structure. The devon CLI will then automatically add it to your PATH variable.\nFurther you can create your own commandlet for your additional tool. For closed-source tools you can create your own archive and distribute to your team members as long as you care about the terms and licenses of these tools.\nMulti-platform\nWorks on all major platforms, which are Windows, Mac, and Linux.\nMulti-tenancy\nHave as many instances of the devonfw-ide &quot;installed&quot; on your machine for different projects with different tools, tool versions and configurations. No physical installation and no tweaking of your operating system. &quot;Installations&quot; of devonfw-ide do not interfere with each other nor with other installed software.\nMultiple Workspaces\nSupport working with different workspaces on different branches. Create and update new workspaces with few clicks. See the workspace name in the title-bar of your IDE so you do not get confused and work on the right branch.\nFree\nThe devonfw-ide is free just like everything from devonfw. See TERMS_OF_USE for details.\n5.2.1. IDEs\nWe support the following IDEs:\nEclipse\nVisual Studio Code\nIntelliJ\n5.2.2. Platforms\nWe support the following platforms:\njava (see also devon4j)\nC# (see devon4net)\nnode.js and angular (see devon4ng)\n5.2.3. Build-Systems\nWe support the following build-systems:\nmvn (maven)\nnpm\ngradle\nHowever, also other IDEs, platforms, or tools can be easily integrated as commandlet.\n5.2.4. Motivation\nTL;DR? Lets talk to developers in the proper language. Here are examples with devonfw-ide:\n[/]$ devon\nYou are not inside a devon IDE installation: /\n[/]$ cd /projects/devon\n[devon]$ mvn\nzsh: command not found: mvn\n[devon]$ devon\ndevonfw-ide has environment variables have been set for /projects/devon\n[devon]$ mvn -v\nApache Maven 3.6.0 (97c98ec64a1fdfee7767ce5ffb20918da4f719f3; 2018-10-24T20:41:47+02:00)\nMaven home: /projects/devon/software/maven\nJava version: 1.8.0_191, vendor: Oracle Corporation, runtime: /projects/devon/software/java\nDefault locale: en_DE, platform encoding: UTF-8\nOS name: &quot;mac os x&quot;, version: &quot;10.14.3&quot;, arch: &quot;x86_64&quot;, family: &quot;mac&quot;\n[devon]$ cd /projects/ide-test/workspaces/test/my-project\n[my-project]$ devon\ndevonfw-ide has environment variables have been set for /projects/ide-test\n[my-project]$ mvn -v\nApache Maven 3.6.0 (97c98ec64a1fdfee7767ce5ffb20918da4f719f3; 2018-10-24T20:41:47+02:00)\nMaven home: /projects/ide-test/software/maven\nJava version: 11.0.2, vendor: Oracle Corporation, runtime: /projects/ide-test/software/jdk/Contents/Home\nDefault locale: en_DE, platform encoding: UTF-8\nOS name: &quot;mac os x&quot;, version: &quot;10.14.3&quot;, arch: &quot;x86_64&quot;, family: &quot;mac&quot;\n[ide-test]$ devon eclipse\nlaunching Eclipse for workspace test...\n[my-project]$ devon build\n[INFO] Scanning for projects...\n...\n[INFO] BUILD SUCCESS\nThis was just a very simple demo of what devonfw-ide can do. For further details have a look at our CLI documentation.\nNow you might ask:\nBut I use Windows/Linux/MacOS/&#x2026;&#x200B;? - works on all platforms!\nBut how about Windows CMD or Power-Shell? - works!\nBut what if I use cygwin or git-bash on windows? - works!\nBut I love to use ConEmu or Commander? - works with full integration!\nHow about MacOS Terminal or iTerm2? - works with full integration!\nBut I use zsh? - works!\n&#x2026;&#x200B;? - works!\nWow! So lets get started with download &amp; setup.\n5.3. Setup\n5.3.1. Prerequisites\nWe try to make it as simple as possible for you. However, there are some minimal prerequisites:\nYou need to have a tool to extract *.tar.gz files (tar and gzip). On Windows use 7zip. On all other platforms this comes out of the box.\nYou need to have git and curl installed.\nOn Windows all you need is download and install git for windows. This also ships with bash and curl.\nOn Linux you might need to install the above tools in case they are not present (e.g. sudo apt-get install git curl or yum install git-core curl)\nOn MacOS all you need is download and install git for mac.\n5.3.2. ATTENTION\nNote\nThe official public release of devonfw-ide is still pending due to some final clarification. The download links are not broken, but pointing to the place where the release will appear after the clarification is complete. If you want to use devonfw-ide today you can clone it from ide.git and run mvn install to build it, or contact us (check internal community annoucements).\n5.3.3. Download\nReleases of devonfw-ide are published to maven central. You can download them from here.\n5.3.4. Install\nCreate a central folder like C:\\projects or /projects. Inside this folder create a sub-folder for your new project such as my-project and extract the contents of the downloaded archive (devonfw-ide-scripts-*.tar.gz) into this new folder. Run the command setup in this folder (on windows just double click setup.bat).\nThat&#x2019;s all. To get started read the usage.\n5.3.5. Uninstall\nTo &quot;uninstall&quot; your devonfw-ide you only need to call the following command:\ndevon ide uninstall\nThen you can delete the devonfw-ide top-level folder(s) (${DEVON_IDE_HOME}).\nThe devonfw-ide is designed to be not invasive to your operating system and computer. Therefore it is not &quot;installed&quot; into your system in a classical way. Instead you just create a folder and extract the downloaded archive to it. Only specific prerequisites like git have to be installed regularly by you in advance. All other software resists just locally inside the folder of your devonfw-ide. However, there are the following excuses (what is reverted by devon ide uninstall):\nThe devon command is copied to your home directory (~/.devon/devon)\nThe devon alias is added to your shell config (~/.bashrc and ~/.zshrc, search for alias devon=&quot;source ~/.devon/devon&quot;).\nOn Windows the devon.bat command is copied to your home directory (%USERPROFILE%\\scripts\\devon.bat)\nOn Windows this %USERPROFILE%\\scripts directory is added to the PATH of your user.\nThe devonfw-ide will download third party software to your ~/Downloads folder to reduce redundant storage.\n5.4. Usage\nThis section explains you how to use the devonfw-ide. We assume you have successfully installed it first.\n5.4.1. Developer\nAs a developer you are supported to setup your IDE within minutes. You only need the settings URL from your Architect.\nUpdate\nTo update your IDE (if instructed by your Architect), all you need to do is run the following command:\ndevon ide update\nWorking with multiple workspaces\nIf you are working on different branches in parallel you typically want to use multiple workspaces.\nGo to the workspaces folder in your ${DEVON_IDE_HOME} and create a new folder with the name of your choice (e.g. release2.1).\nCheck out (git clone &#x2026;&#x200B;) the according projects and branch into that workspace folder.\nOpen a shell in that new workspace folder (cd to it) and according to your IDE run e.g. eclipse, vscode, or intellij to create your workspace and launch the IDE. You can also add the parameter create-script to the IDE commandlet in order to create a launch-script for your IDE.\nYou can have multiple instances of eclipse running for each workspace in parallel. To distinguish these instances you will find the workspace name in the title of eclipse.\n5.4.2. Architect\nAs architect or technical lead of the project you can configure the devonfw-ide to your needs.\nProject specific settings\nFor your project you should create a git-repository for the settings. You can customize many aspects this way.\nDistribute\nTo redistribute the IDE you can decide to use the official vanilla releases of the devonfw-ide scripts.\nHowever, you may also add the cloned settings, a custom devon.properties file, or predefine software (be aware of multi-platform-support).\nUpdate\nWhen you have done changes in a larger project (big team), please first test the changes yourself, then pick a pilot user of the team, and only after that works well for a couple of days inform the entire team to update.\n5.5. License\nThis software is licensed as Open-Source product for free usage including commercial use.\nYou need to apply to our terms of use before using it.\n5.6. Structure\nThe directory layout of your devonfw-ide will look like this:\nListing 1. File structure of your devonfw-ide\n/ projects (or C:\\Projects, etc.)\n&#x2514;&#x2500;&#x2500;/ my-project ($DEVON_IDE_HOME)\n&#x251C;&#x2500;&#x2500;/ conf\n&#x251C;&#x2500;&#x2500;/ log\n&#x251C;&#x2500;&#x2500;/ scripts\n&#x251C;&#x2500;&#x2500;/ settings\n&#x251C;&#x2500;&#x2500;/ software\n&#x251C;&#x2500;&#x2500;/ system\n&#x251C;&#x2500;&#x2500;/ updates\n&#x251C;&#x2500;&#x2500;/ workspaces\n&#x251C;&#x2500;&#x2500; setup\n&#x251C;&#x2500;&#x2500; setup.bat\n&#x2514;&#x2500;&#x2500; TERMS_OF_USE.adoc\nThe elements of the above structure are described in the individual sections. As they are hyperlinks you can simply click them to get more details.\n5.6.1. TERMS_OF_USE\nYou need to agree with the TERMS_OF_USE in order to use devonfw-ide. Everything included out of the box applies to open-source licenses. However, due to the many third party components with their licenses and terms we want to make this clear to all our users and be compliant.\n5.7. Devon CLI\nThe devonfw-ide is shipped with a central command devon. The setup will automatically register this command so it is available in any shell on your system. This page describes the Command Line Interface (CLI) of this command.\n5.7.1. Devon\nWithout any arguments the devon command will determine your DEVON_IDE_HOME and setup your environment variables automatically. In case you are not inside of a devonfw-ide folder the command will echo a message and do nothing.\n[/]$ devon\nYou are not inside a devon IDE installation: /\n[/]$ cd /projects/my-project/workspaces/test/my-git-repo\n[my-git-repo]$ devon\ndevonfw-ide has environment variables have been set for /projects/my-project in workspace main\n[my-git-repo]$ echo $DEVON_IDE_HOME\n/projects/devon\n[my-git-repo]$ echo $JAVA_HOME\n/projects/my-project/software/java\n5.7.2. Commandlets\nThe devon command supports a pluggable set of commandlets. Such commandlet is provided as first argument to the devon command and my take additional arguments:\ndevon &#xAB;commandlet&#xBB; [&#xAB;arg&#xBB;]*\nTechnically a commandlet is a bash script located in $DEVON_IDE_HOME/scripts/command. So if you want to integrate another tool with devonfw-ide we are awaiting your pull-request.\nThe following commandlets are currently available:\nbuild\neclipse\ngradle\nhelp\nide\nintellij\njava\njenkins\nmvn\nng\nnode\nnpm\nrelease\nsonar\nvscode\nyarn\n5.8. Variables\nThe devonfw-ide defines a set of standard variables to your environment for configuration via variables[.bat] files.\nThese environment variables are described by the following table.\nThose variables printed bold are also exported in your shell (except for windows CMD that does not have such concept). Variables with the value - are not set by default but may be set via configuration to override defaults.\nPlease note that we are trying to minimize any potential side-effect from devonfw-ide to the outside world by reducing the number of variables and only exporting those that are required.\nTable 1. Variables of devonfw-ide\nVariable\nValue\nMeaning\nDEVON_IDE_HOME\ne.g. /projects/my-project\nThe top level directory of your devonfw-ide structure.\nPATH\n$PATH:$DEVON_IDE_HOME/software/java:&#x2026;&#x200B;\nYou system path is adjusted by devon command.\nDEVON_HOME_DIR\n~\nThe platform independent home directory of the current user. In some edge-cases (e.g. in cygwin) this differs from ~ to ensure a central home directory for the user on a single machine in any context or environment.\nDEVON_IDE_TOOLS\njava mvn eclipse vscode node npm ng\nList of tools that should be installed and upgraded by default for your current IDE.\nDEVON_OLD_PATH\n&#x2026;&#x200B;\nA &quot;backup&quot; of PATH before it was extended by devon to allow recovering it. Internal variable that should never be set or tweaked.\nWORKSPACE\nmain\nThe workspace you are currently in. Defaults to main if you are not inside a workspace. Never touch this variable in any varibales file.\nWORKSPACE_PATH\n$DEVON_IDE_HOME/workspaces/$WORKSPACE\nAbsolute path to current workspace. Never touch this variable in any varibales file.\nJAVA_HOME\n$DEVON_IDE_HOME/software/java\nPath to JDK\nSETTINGS_PATH\n$DEVON_IDE_HOME/settings\nPath to your settings. To keep oasp4j-ide legacy behaviour set this to $DEVON_IDE_HOME/workspaces/main/development/settings.\nM2_REPO\n$DEVON_IDE_HOME/conf/.m2/repository\nPath to your local maven repository. For projects without high security demands, you may change this to the maven default ~/.m2/repository and share your repository amongst multiple projects.\nMAVEN_HOME\n$DEVON_IDE_HOME/software/maven\nPath to Maven\nMAVEN_OPTS\n-Xmx512m -Duser.home=$DEVON_IDE_HOME/conf\nMaven options\nDEVON_SOFTWARE_REPOSITORY\n-\nProject specific or custom software-repository.\nDEVON_SOFTWARE_PATH\n-\nGlobally shared user-specific local software installation location.\nECLIPSE_VMARGS\n-Xms128M -Xmx768M -XX:MaxPermSize=256M\nJVM options for Eclipse\nECLIPSE_PLUGINS\n-\nArray with &quot;feature groups&quot; and &quot;update site URLs&quot; to customize required eclipse plugins.\n&#xAB;TOOL&#xBB;_VERSION\n-\nThe version of the tool &#xAB;TOOL&#xBB; to install and use (e.g. ECLIPSE_VERSION or MAVEN_VERSION).\n&#xAB;TOOL&#xBB;_BUILD_OPTS\ne.g.clean install\nThe arguments provided to the build-tool &#xAB;TOOL&#xBB; in order to run a build.\n&#xAB;TOOL&#xBB;_RELEASE_OPTS\ne.g.clean deploy -Dchangelist= -Pdeploy\nThe arguments provided to the build-tool &#xAB;TOOL&#xBB; in order to perform a release build.\n5.9. Configuration\nThe devonfw-ide aims to be highly configurable and flexible. The configuration of the devon command and environment variables takes place via devon.properties files. The following list shows these configuration files in the order they are loaded so files can override variables from files above in the list:\nbuild in defaults (for JAVA_VERSION, ECLIPSE_PLUGINS, etc.)\n~/devon.properties - user specific global defaults (on windows in %USERPROFILE%/devon.properties)\nscripts/devon.properties - defaults provided by devonfw-ide. Never directly modify this file!\ndevon.properties - vendor variables for custom distributions of devonfw-ide-scripts, may e.g. tweak SETTINGS_PATH or predefine SETTINGS_URL.\nsettings/devon.properties (${SETTINGS_PATH}/devon.properties) - project specific configurations from settings.\nworkspaces/${WORKSPACE}/devon.properties - optional workspace specific configurations (especially helpful in projects using docker).\nconf/devon.properties - user specific configurations (e.g. M2_REPO=~/.m2/repository). During setup this file is created by copying a template from ${SETTINGS_PATH}/devon/conf/devon.properties.\n5.9.1. devon.properties\nThe devon.properties files allow to define environment variables in a simple and OS independent way:\n# comments begin with a hash sign (#) and are ignored\nvariable_name=variable_value with space etc.\nvariable_name=${predefined_variable}/folder_name\nvariable values can refer to other variables that are already defined what will be resolved to their value. You have to used ${&#x2026;&#x200B;} syntax to make it work on all platforms (never use %&#x2026;&#x200B;%, $&#x2026;&#x200B;, or $(&#x2026;&#x200B;) syntax in devon.properties files).\nexport exported_variable=this value will be exported in bash, in windows CMD the export prefix is ignored\nvariable_name=\nthis will unset the specified variable\nvariable_name=~/some/path/and.file\ntilde is resolved to your personal home directory on any OS including windows.\narray_variable=(value1 value2 value3)\nThis will only work properly in bash worlds but as no arrays are used in CMD world of devonfw-ide it does not hurt on windows.\nPlease never surround values with quotes (var=&quot;value&quot;)\nThis format is similar to Java *.properties but does not support advanced features as unicode literals, multi-lined values, etc.\nIn order to know what to configure have a look at the available variables.\nPlease only tweak configurations that you need to change and take according responsibility. Flexibility comes with some price. Of course you can break everything if you do the wrong things.\nFurther, you can configure maven via conf/settings.xml. To configure your IDE such as eclipse or vscode you can tweak the settings.\n5.10. scripts\nThis directory is the heart of the devonfw-ide and contains the required scripts.\nListing 2. File structure of the conf folder\n/scripts\n&#x251C;&#x2500;&#x2500;/ command\n&#x2502; &#x251C;&#x2500;&#x2500; build\n&#x2502; &#x251C;&#x2500;&#x2500; eclipse\n&#x2502; &#x251C;&#x2500;&#x2500; gradle\n&#x2502; &#x251C;&#x2500;&#x2500; help\n&#x2502; &#x251C;&#x2500;&#x2500; ide\n&#x2502; &#x251C;&#x2500;&#x2500; intellij\n&#x2502; &#x251C;&#x2500;&#x2500; java\n&#x2502; &#x251C;&#x2500;&#x2500; jenkins\n&#x2502; &#x251C;&#x2500;&#x2500; mvn\n&#x2502; &#x251C;&#x2500;&#x2500; ng\n&#x2502; &#x251C;&#x2500;&#x2500; node\n&#x2502; &#x251C;&#x2500;&#x2500; npm\n&#x2502; &#x251C;&#x2500;&#x2500; release\n&#x2502; &#x251C;&#x2500;&#x2500; sonar\n&#x2502; &#x251C;&#x2500;&#x2500; vscode\n&#x2502; &#x2514;&#x2500;&#x2500; yarn\n&#x251C;&#x2500;&#x2500; devon\n&#x251C;&#x2500;&#x2500; devon.bat\n&#x251C;&#x2500;&#x2500; environment-project\n&#x251C;&#x2500;&#x2500; environment-project.bat\n&#x251C;&#x2500;&#x2500; functions\n&#x2514;&#x2500;&#x2500; devon.properties\nThe command folder contains the commandlets.\nThe devon script is the key command line interface for devonfw-ide.\nFor windows there is also devon.bat to be used in CMD or PowerShell.\nAs the devon CLI can be used as global command on your computer from any directory and gets installed centrally it aims to be stable, minimal, and lightweight.\nThe key logic to setup the environment variables is therefore in a separate script environment-project and its Windows variant environment-project.bat inside this scripts folder.\nThe file functions contains a collection of reusable bash functions.\nThese are sourced and used by the commandlets.\nFinally the devon.properties file contains defaults for the general configuration of devonfw-ide.\n5.11. settings\nThe devonfw-ide requires settings with configuration templates for the arbitrary tools.\nTo get an initial set of these settings we provide the devonfw-ide-settings as an initial package. These are also released so you can download the latest stable version at maven central.\nTo test devonfw-ide or for small projects you can also use these the latest default settings.\nHowever, for collaborative projects we strongly encourage you to distribute and maintain the settings via a dedicated and project specific git repository (or any other version-control-system). This gives you the freedom to control and manage the tools with their versions and configurations during the project lifecycle.\nTherefore a technical lead creates a settings git repository for the project. He creates a &quot;fork&quot; devonfw-ide-settings by adding it as &quot;upstream&quot; origin and pulls from there finally pushing it to the project settings git. This also allows to later merge changes from the official devonfw-ide-settings back into the project specific settings &quot;fork&quot;.\n5.11.1. Structure\nThe settings folder (see SETTINGS_PATH) has to following file structure:\nListing 3. File structure of settings\n/settings\n&#x251C;&#x2500;&#x2500;/ devon\n&#x2502; &#x251C;&#x2500;&#x2500;/ conf\n&#x2502; &#x2502; &#x251C;&#x2500;&#x2500;/ .m2\n&#x2502; &#x2502; &#x2502; &#x2514;&#x2500;&#x2500; settings.xml\n&#x2502; &#x2502; &#x251C;&#x2500;&#x2500;/ npm\n&#x2502; &#x2502; &#x2502; &#x2514;&#x2500;&#x2500; .npmrc\n&#x2502; &#x2502; &#x2514;&#x2500;&#x2500; devon.properties\n&#x251C;&#x2500;&#x2500;/ eclipse\n&#x2502; &#x251C;&#x2500;&#x2500;/ workspace\n&#x2502; &#x2502; &#x251C;&#x2500;&#x2500;/ setup\n&#x2502; &#x2502; &#x2514;&#x2500;&#x2500;/ update\n&#x2502; &#x251C;&#x2500;&#x2500; lifecycle-mapping-metadata.xml\n&#x2502; &#x2514;&#x2500;&#x2500; project.dictionary\n&#x251C;&#x2500;&#x2500;/ ...\n&#x251C;&#x2500;&#x2500;/ sonarqube\n&#x2502; &#x2514;&#x2500;&#x2500;/ profiles\n&#x2502; &#x251C;&#x2500;&#x2500; Devon-C#.xml\n&#x2502; &#x251C;&#x2500;&#x2500; ...\n&#x2502; &#x2514;&#x2500;&#x2500; Devon-XML.xml\n&#x251C;&#x2500;&#x2500;/ vscode\n&#x2502; &#x2514;&#x2500;&#x2500;/ workspace\n&#x2502; &#x251C;&#x2500;&#x2500;/ setup\n&#x2502; &#x2514;&#x2500;&#x2500;/ update\n&#x2514;&#x2500;&#x2500; devon.properties\nAs you can see the settings folder contains sub-folders for tools of the IDE.\nSo the devon folder contains devon.properties files for the configuration of your environment.\nFurther, for the IDEs such as eclipse or vscode the according folders contain the templates to manage the workspace via our configurator.\n5.11.2. Configuration Philosophy\nDifferent tools and configuration files require a different handling:\nWhere suitable we directly use these configurations from your settings (e.g. for eclipse/lifecycle-mapping-metadata.xml, or eclipse/project.dictionary).\nThe devon folder in settings contains templates for configuration files. There are copied to the devonfw-ide installation during setup (if no such file already exists). This way the settings repository can provide reasonable defaults but allows the user to take over control and customize to his personal needs (e.g. .m2/settings.xml).\nOther configurations need to be imported manually. To avoid manual steps and simplify usage we try to automate as much as possible. This currently applies to sonarqube profiles but will be automated with sonar-devon4j-plugin in the future.\nFor tools with complex configuration structures like eclipse, intellij, or vscode we provide a smart mechanism via our configurator.\n5.11.3. Customize Settings\nYou can easily customize these settings for the requirements of your project. We suggest that one team member is responsible to ensure that everything stays consistent and works.\nYou may also create new sub-folders in settings and put individual things according to your needs. E.g. you could add scripts for greasemonkey or tampermonkey, as well as scripts for your database or whatever may be useful and worth to share in your team. However, to share and maintain knowledge we recommend to use a wiki.\n5.12. software\nThe software folder contains the third party tools for your IDE such as maven, npm, java, etc.\nWith respect to the licensing terms you may create a custom archive containing a devonfw-ide together with the required software.\nHowever, to be platform independent and allow lightweight updates the devonfw-ide is capable to download and install the software automatically for you.\n5.12.1. Repository\nBy default software is downloaded via the internet from public download URLs of the according tools. However, some projects may need specific tools or tool versions that are not publicly available.\nIn such case, they can create their own software repository (e.g. in a VPN) and configure the base URL of it via DEVON_SOFTWARE_REPOSITORY variable.\nThen devonfw-ide will download all software from this repository only instead of the default public download URLs.\nThis repository (URL) should be accessible within your network via HTTPS (or HTTP) and without any authentication.\nThe repository needs to have the following structure:\n${DEVON_SOFTWARE_REPOSITORY}/&#xAB;tool&#xBB;/&#xAB;version&#xBB;/&#xAB;tool&#xBB;-&#xAB;version&#xBB;[-&#xAB;os&#xBB;].tgz\nSo for every tool &#xAB;tool&#xBB; (java, maven, vscode, eclipse, etc.) you need to provide a folder in your repository.\nWithin this folder for every supported version &#xAB;version&#xBB; you need a subfolder.\nThis subfolder needs to contain the tool in that version for every operating system &#xAB;os&#xBB; (windows, linux, or mac - omitted if platform independent, e.g. for maven).\n5.12.2. Shared\nBy default each installation of devonfw-ide has its own physical installations of the required tools in the desired versions stored in its local software folder.\nWhile this is great for isolation of devonfw-ide installations and to prevent side-effects, it can cause a huge waste of disc resources in case you are having many installations of devonfw-ide.\nIf you are a power-user of devonfw-ide with more then ten or even up to hundreds of installations on your machine, you might love to share installations of a software tool in a particular version between multiple devonfw-ide installations.\nCaution\nIf you use this power-feature you are taking responsibility for side-effects and should not expect support. Also if you are using Windows please read Symlinks in Windows and make your mind if you really want to do so. You might also use this hint and maintain it manually without enabling the following feature.\nIn order to do so, you only need to configure the variable DEVON_SOFTWARE_PATH in your ~/devon.properties pointing to an existing directory on your disc (e.g. /projects/software or C:\\projects\\software).\nThen devonfw-ide will install required software into ${DEVON_SOFTWARE_PATH}/${software_name}/${software_version} as needed and create a symbolic link to it in ${DEVON_IDE_HOME}/software/${software_name}.\nAs a benefit another devonfw-ide installation will using the same software with the same version can re-use the existing installation and only needs to create the symbolic link. No more waste of having many identical JDK installations on your disc.\nAs a drawback you need to be aware that specific tools may be &quot;manipulated&quot; after installation.\nThe most common case is that a tool allows to install plugins or extensions such as all IDEs do. Such &quot;manipulations&quot; will cause side-effects between the different devonfw-ide installations sharing the same version of that tool.\nWhile this can also be a benefit it may also cause trouble.\nIf you have a sensitive project that should not be affected by such side-effects, you may again override the DEVON_SOFTWARE_PATH variable to the empty value in your ${DEVON_IDE_HOME}/conf/devon.properties of that sensitive installation:\nDEVON_SOFTWARE_PATH=\nThis will disable this feature particularly for that specific sensitive devonfw-ide installation but let you use it for all other ones.\n5.13. Integration\nThe devonfw-ide already brings a lot of integration out of the box. This page is for users that want to get even more out of it. For instance this IDE ships with a console script to open a shell with the environment variables properly set for your devonfw-ide installation, so you get the correct version of your tools (Java, Maven, Yarn, etc.). However, you might want to open a shell from your IDE or your file manager. For some of these use-cases you need additional tweaks that are described on this page.\n5.13.1. Windows\ndevonfw-ide automatically integrates with Windows-Explorer during setup.\nCMD\nIf you want to open a CMD (MS Dos Shell) directly from Windows-Explorer simply right-click on the folder in your devonfw-ide you want to open. From the context menu click on Open Devon CMD shell here. This will open CMD and automatically initialize your environment according to the devonfw-ide project containing the folder (if any, see above).\nGit-Bash\nJust like for CMD you can also click Git Bash Here from Windows-Explorer context-menu to open a git bash. If you have selected a folder in your devonfw-ide installation, it will automatically initialize your environment.\nCygwin\nIn case you have cygwin installed on your machine, the devonfw-ide will autodetect this during setup and also install a Windows-Explorer integration. Just choose Open Devon Cygwin Bash Here to open cygwin bash and initialize your devonfw-ide environment.\nConEMU\nConEmu is a great extension that brings additional features such as tabs to your windows shells. If you like it, you will also want to have it integrated with devonfw-ide. All you need to do is follow these simple steps:\nCopy the file CmdInit.cmd from your ConEmu installation (C:\\Program Files\\ConEmu\\ConEmu\\CmdInit.cmd) to a personal folder (e.g. C:\\Users\\MYLOGIN\\scripts).\nModify this copy of CmdInit.cmd by adding the line IDEenv (e.g. at line 6) and saving.\nGo to ConEmu and open the settings (via context menu or [Windows][Alt][p]).\nSelect Startup &gt; Tasks from the left tree.\nSelect the first option form Predefined tasks (command groups) ({Shells::cmd})\nIn the text area at the right bottom modify the location of CmdInit.cmd to your customized copy (%HOME%\\scripts\\CmdInit.cmd).\nSelect Integration from the left tree.\nClick on the upper Register button (for ConEmu Here).\nClick on Save settings\nNow you have the option ConEmu here if you right click on a folder in Windows Explorer that will open a new tab in ConEmu and automatically setup your environment according to the devonfw-ide project containing the folder (if any, see above).\nConEMU and StartExplorer\nYou can even integrate the Eclipse StartExplorer plug-in and ConEMU to open up console right from the file tree of eclipse into ConEMU. You can do this by adding a custom command to StartExplorer:\nOpen up eclipse\nOpen Window &gt; Preferences\nSelect StartExplorer &gt; Custom Commands on the left\nAdd on the right and setup the following command: &quot;C:\\Program Files\\ConEmu\\ConEmu64.exe&quot; -Dir ${resource_path} -runlist cmd.exe /k &quot;&quot;%ConEmuBaseDir%\\CmdInit.cmd&quot; &amp; &quot;IDEenv&quot;&quot; -cur_console:n\nBe aware that you potentially have to adapt the ConEmu*.exe path to match your installation.\nYou can even add a shell login if you installed git bash on your machine. Please be aware to potentially adapt the sh.exe url to match your installation: &quot;C:\\Program Files\\ConEmu\\ConEmu64.exe&quot; -Dir ${resource_path} -runlist cmd.exe /k &quot;&quot;%ConEmuBaseDir%\\CmdInit.cmd&quot; &amp; &quot;IDEenv&quot; &amp; &quot;%SYSTEMDRIVE%\\Program Files\\Git\\bin\\sh.exe&quot; --login&quot; -cur_console:n\nState two times the &quot;Name for *&quot; to your choice like &quot;ConEMU&quot;\nOK &#x2192; OK\nRight click on any folder/file in your eclipse file explorer and select StartExplorer &gt; Custom Commands &gt; ConEMU.\nYou will get a initialized console at the file/folder location! Have fun!\n5.13.2. Eclipse\nYou might want to open a terminal directly as view inside your Eclipse IDE. Therefore we provide eclipse with the TM terminal plugin.\nFurther the settings already configure that plugin so it automatically sets the environment properties correctly. In other words the integration comes out of the box.\nTo use it all you need to do is to follow these steps:\nOpen the Terminal view (Window &gt; Show View &gt; Other &gt; Terminal &gt; Terminal &gt; OK).\nClick on the monitor icon from the left of the icon bar of the Terminal view.\nChoose terminal (e.g. Local Terminal) and confirm with OK\nExecute mvn -v to verify your environment.\n5.13.3. IntelliJ or WebStorm\nYou might want to open a terminal directly as view inside your IDEA IDE, that already ships with a feature for this out of the box.\nIf you start your IDE via the start-idea script provided by devonfw-ide then everything is configured and your environment is set automatically.\n5.14. Tools\nThis page is a general guide and collection to boost your productivity by using proper tools.\n5.14.1. Tabs everywhere\nMany people got used to tabs that have been introduced by all major browsers:\nFigure 1. Tabs in Firefox\nThis nice feature can be added to many other tools.\nTabs for Windows Explorer\nIf you want to have tabs for windows explorer simply install Clover\nFigure 2. Tabs in Windows Explorer\nTabs for SSH\nIf you want to have tabs for your SSH client Putty (or even better Kitty that comes with WinSCP integration) you simply install SuperPutty\nFigure 3. Tabs for SSH\nTabs for CMD\nIf you want to have tabs for your windows command-line you simply install ConEmu\nFigure 4. Tabs for CMD\nSee guide-integration for integration of ConEmu with devonfw-ide.\n5.14.2. Browser Plugins\nThere are tons of helpful browser plugins out there and it might be a matter of personal taste what you like to have installed. However, as we are heavily using github we want to promote octotree.\n5.14.3. Windows Helpers\nHandle passwords\nFor security you want complex passwords that differ for each account? For simplicity you only want to remember a single password? Want to have both? Then you need to install KeePass right now.\nReal text editor\nA real developer needs a real text editor and not windows build in notepad.\nThe most common choice is Notepad++.\nReal compression tool\nYou need to deal with ZIP files, TGZ, dpkg, etc.? Just install 7zip and forget about windows build-in ZIP support (that is buggy with long file paths, etc.).\nSmarter clipboard\nYou want to paste something from the clipboard but meanwhile you had to copy something else? Just one of many things you can easily do with ditto.\nSysinternals Tools\nA real developer will quickly notice that windows build in tools to analyze processes, network connections, autostarts, etc. are quite poor. So what you really want is the Sysinternals-Suite. Make process-explorer your default task manager asap. Use autoruns to prevent nasty background things to be started automatically. Use tcpview to figure out which process is blocking port 8080, etc.\nCope with file locks\nDid you ever fail to delete a file or directory that was locked by some process and you did not even know which one it was?\nThen you might love IoBit Unlocker.\nSee also this article.\nCreate symbolic links\nYou are used to symbolic and hard links in Linux? You have to work with Windows? You would also like to have such links in Windows? Why not? Windows supports real links (not these stupid shortcuts).\nIf you even want to have it integrated in windows explorer you might want to install linkshellextension. However, you might want to disable SmartMove in the configuration if you face strange performance issues when moving folders.\nLinux\nInstall Cygwin and get your bash in windows with ssh-agent, awk, sed, tar, and all the tools you love (or hate).\nX11\nWant to connect via SSH and need to open an X11 app from the server? Want to see the GUI on your windows desktop?\nNo problem: Install VcXsrv.\nKeyboard Freak\nAre you the keyboard shortcut guy? Want to have shortcuts for things like &#xAB; and &#xBB; ?\nThen you should try AutoHotKey.\nFor the example (&#xAB; and &#xBB;) you can simply use this script to get started:\n^&lt;::Send {U+00AB}\n^+&lt;::Send {U+00BB}\nNow just press [ctrl][&lt;] and [ctrl][&gt;] ([ctrl][shift][&lt;]). Next create shortcuts to launch your IDE, to open your favorite tool, etc.\nPaint anywhere on your desktop\nDo you collaborate sharing your screen, and want to mark a spot on top of what you see? Use Epic Pen to do just that.\nanalyze graphs\nNeed to visualise complex graph structures? Convert them to Trivial Graph Format (.tgf) an run yEd to get an interactive visualization of your graph.\nup your screen capture game\nCapture any part of your screen with a single click, directly upload to dropbox, or run an svn commit (oops sorry git ;-) ) all in one go with Greenshot.\nFast Search in Windows\nEverything is a desktop search utility for Windows that can rapidly find files and folders by name.\n5.15. OASP4J IDE Plugins:\nSince an application&#x2019;s code can greatly vary, and every program can be written in lots of ways without being semantically different, OASP4J IDE comes with pre-installed and pre-configured plugins that use some kind of a probabilistic approach, usually based on pattern matching, to determine which pieces of code should be reviewed. These hints are a real time-saver, helping you to review incoming changes and prevent bugs from propagating into the released artifacts. It provides, CheckStyle, SonarQube,FindBugs.\nDetails of each can be found in subsequent sections.\n5.15.1. CheckStyle\nWhat is CheckStyle?\nCheckStyle is a Open Source development tool to help you ensure that your Java code adheres to a set of coding standards. Checkstyle does this by inspecting your Java source code and pointing out items that deviate from a defined set of coding rules.\nWith the Checkstyle IDE Plugin your code is constantly inspected for coding standard deviations. Within the Eclipse workbench you are immediately notified of problems via the Eclipse Problems View and source code annotations similar to compiler errors or warnings.\nThis ensures an extremely short feedback loop right at the developers fingertips.\nWhy use it?\nIf your development team consists of more than one person, then obviously a common ground for coding standards (formatting rules, line lengths etc.) must be agreed upon - even if it is just for practical reasons to avoid superficial, format related merge conflicts.\nCheckstyle Plugin helps you define and easily apply those common rules.\nThe plugin uses a project builder to check your project files with Checkstyle. Assuming the IDE Auto-Build feature is enabled each modification of a project file will immediately get checked by Checkstyle on file save - giving you immediate feedback about the changes you made. To use a simple analogy, the Checkstyle Plug-in works very much like a compiler but instead of producing .class files it produces warnings where your code violates the Checkstyle rules. The discovered deviations are accessible in the Eclipse Problems View, as code editor annotations and via additional Checkstyle violations views.\nInstallation\nAfter IDE installation, IDE provides default checkstyle configuration file which has certain check rules specified .\nThe set of rules used to check your code is highly configurable. A Checkstyle configuration specifies which check rules are validated against your code and with which severity violations will be reported. Once defined a Checkstyle configuration can be used across multiple projects. The IDE comes with several pre-defined Checkstyle configurations.\nYou can create custom configurations using the plugin&#x2019;s Checkstyle configuration editor or even use an existing Checkstyle configuration file from an external location.\nYou can see violations in your workspace as shown in below figure.\nFigure 5. Depicts-Checkstyle-Violations\n&#xA0;\nUsage\nSo, once projects are created, follow steps mentioned below, to activate checkstyle:\nOpen the properties of the project you want to get checked.\nFigure 6. Click-on-properties\n&#xA0;\nSelect the Checkstyle section within the properties dialog .\nFigure 7. select-checkstyle\n&#xA0;\nActivate Checkstyle for your project by selecting the Checkstyle active for this project check box and press OK\nFigure 8. Activate-checkstyle\n&#xA0;\nNow Checkstyle should begin checking your code. This may take a while depending on how many source files your project contains.\nThe Checkstyle Plug-in uses background jobs to do its work - so while Checkstyle audits your source files you should be able to continue your work.\nAfter Checkstyle has finished checking your code please look into your Eclipse Problems View.\nThere should be some warnings from Checkstyle. This warnings point to the code locations where your code violates the preconfigured Checks configuration.\nFigure 9. view-checkstyle\n&#xA0;\nYou can navigate to the problems in your code by double-clicking the problem in you problems view.\nOn the left hand side of the editor an icon is shown for each line that contains a Checkstyle violation. Hovering with your mouse above this icon will show you the problem message.\nAlso note the editor annotations - they are there to make it even easier to see where the problems are.\n5.15.2. FindBugs:\nWhat is FindBugs?\nFindBugsis an open source project for a static analysis of the Java bytecode to identify potential software bugs. Findbugs provides early feedback about potential errors in the code.\nWhy use it?\nIt scans your code for bugs, breaking down the list of bugs in your code into a ranked list on a 20-point scale. The lower the number, the more hardcore the bug.This helps the developer to access these problems early in the development phase.\nInstallation and Usage.\nOASP4J IDE comes preinstalled with FindBugs plugin.\nYou can configure that FindBugs should run automatically for a selected project. For this right-click on a project and select Properties from the popup menu. via the project properties. Select FindBugs &#x2192; Run automatically as shown below.\nTo run the error analysis of FindBugs on a project, right-click on it and select the Find Bugs&#x2026;&#x200B; &#x2192; Find Bugs menu entry.\nPlugin provides specialized views to see the reported error messages. Select Window &#x2192; Show View &#x2192; Other&#x2026;&#x200B; to access the views.\nThe FindBugs error messages are also displayed in the Problems view or as decorators in the Package Explorer view.\n5.15.3. SonarQube\nwhat is SonarQube?\nSonarQube is an open platform to manage code quality.\nSonarQube is a web-based application. Rules, alerts, thresholds, exclusions, settings can be configured online. By leveraging its database, SonarQube not only allows to combine metrics altogether but also to mix them with historical measures.\nWhy use it?\nIt covers seven aspects of code quality like junits, coding rules,comments,complexity,duplications, architecture and design and potential bugs.\nSonarQube has got a very efficient way of navigating, a balance between high-level view, dashboard and defect hunting tools. This enables to quickly uncover projects and / or components that are in analysis to establish action plans.\nInstallation and usage:\nOASP4J IDE comes preinstalled with SonarQube.\nTo configure it , please follow below steps:\nFirst of all, you need to start sonar service.For that , from softwares folder in extracted from OASP4j IDE zip, choose sonarqube&#x2192;bin&#x2192;&lt;choose appropriate folder according to your OS&gt;-&#x2192;and execute startSonar bat file.\nIf your project is not already under analysis, you&#x2019;ll need to declare it through the SonarQube web interface as described here.\nOnce your project exists in SonarQube, you&#x2019;re ready to get started with SonarQube in Eclipse.\nGo to Window &gt; Preferences &gt; SonarQube &gt; Servers.\nFigure 10. Configure_in_IDE\n&#xA0;\nSonarQube in Eclipse is pre-configured to access a local SonarQube server listening on http://localhost:9000/.\nYou can edit this server, delete it or add new ones.By default, user and password is &quot;admin&quot;.If sonar service is started properly, test connection will give you successful result.\nLinking a project to one analysed on sonar server.\nFigure 11. associate-sonarqube\n&#xA0;\nIn the SonarQube project text field, start typing the name of the project and select it in the list box:\nFigure 12. link-with-project\n&#xA0;\nClick on Finish. Your project is now associated to one analyzed on your SonarQube server.\nChanging linkage\nAt any time, it is possible to change the project association.\nTo do so, right-click on the project in the Project Explorer, and then SonarQube &gt; Change Project Association&#x2026;&#x200B;:\nFigure 13. change-link-with-project\n&#xA0;\nUnlinking a Project\nTo do so, right-click on the project in the Project Explorer, and then SonarQube &gt; Remove SonarQube Nature.\nFigure 14. unlink-with-project\n&#xA0;\nAdvanced Configuration\nAdditional settings (such as markers for new issues) are available through Window &gt; Preferences &gt; SonarQube\nFigure 15. eclipse-settings\n&#xA0;\nTo analyse a project, right click on project , select SonarQube&#x2192;Analyse.\nFigure 16. Analyse-project\n&#xA0;\nTo look for sonarqube analysed issue, go to Window&#x2192;Show View&#x2192; Others&#x2192;SonarQube&#x2192;SonarQube Issues.\nNow you can see issues in sonarqube issues tab as shown\nFigure 17. SonarQube-issues-view\n&#xA0;\nOr you can go to link http://loclahost:9000 and login with admin as id and admin as password and goto Dashboard.you can see all the statistics of analysis of the configured projects on sonar server.\n5.15.4. Soap UI\nWhat is soap UI?\nSoapUI is an open-source web service testing application for service-oriented architectures (SOA) and representational state transfers (REST). Its functionality covers web service inspection, invoking, development, simulation and mocking, functional testing, load and compliance testing.\nOASP4J IDE comes preinstalled with this plugin.\nNote: There is no update site for this tool.\nWhy use it?\nSoapUI is a free and open source cross-platform Functional Testing solution. With an easy-to-use graphical interface, and enterprise-class features, SoapUI allows you to easily and rapidly create and execute automated functional, regression, compliance, and load tests. In a single test environment, SoapUI provides complete test coverage and supports all the standard protocols and technologies.For more details see here .\nInstallation and Usage:\nAs soon as , IDE is configured, soapUI can be seen in Windows&#x2192;Preferences.\nFigure 18. soap-preferences\n&#xA0;\nSoap ui Perspective can be opened as shown in below pictures\nFigure 19. soap-perspective\n&#xA0;\nCreation of new Project\nOnce Soap UI perspective is opened, right click on projects and &quot;select New Soap UI Project&quot;\nFigure 20. soap-new-project\n&#xA0;\nOnce above option is selected, a new dialog is opened as shown below:\nFigure 21. soap-create-new-project\n&#xA0;\nProvide initial wsdl and project name, and your soap project is created and ready for testing your webservice.\nFigure 22. soap-req-response\n&#xA0;\nSo, once project is created, and if &quot;create Requests&quot; option is selected , while creation of project, a new request with all the details mentioned in provided WSDL is created.\nAs , seen in above picture, when u click on &quot;Request1 &quot; node on project tree pane,on the left side request is generated automatically, and when u click on arrow button on tool bar,response is generated with the desired result.\nFor load testing, and functional testing, refer this link\n&#x2191;&#xA0;Up:&#xA0;devonfw ide&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;devon4j&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-my-thai-star.asciidoc.html","title":"XI. MyThaiStar","body":"\nXI. MyThaiStar\n1.\tMy Thai Star &#x2013; Agile Framework\n2.\tMy Thai Star &#x2013; Agile Diary\nUser Stories\nTechnical design\nSecurity\nTesting\nUI design\nCI/CD\n&#x2190;&#xA0;Previous:&#xA0;Mr Checker Test Framework modules&#xA0;| &#x2191;&#xA0;Up:&#xA0;devonfw guide&#xA0;| Next:&#xA0;1.\tMy Thai Star &#x2013; Agile Framework&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-my-thai-star.asciidoc_cicd.html","title":"58. CI/CD","body":"\n58. CI/CD\n58.1. My Thai Star in Production Line\nWhat is PL?\nThe Production Line Project is a set of server-side collaboration tools for Capgemini engagements. It has been developed for supporting project engagements with individual tools like issue tracking, continuous integration, continuous deployment, documentation, binary storage and much more!\nIntroduction\nAlthough the PL Project is a wide set of tools, only 3 are going to be mainly used for My Thai Star projects to build a Continuous Integration and Continuos Delivery environment. All three are available in the PL instance used for this project.\nJenkins\nThis is going to be the &quot;main tool&quot;. Jenkins helps to automate the non-human part of the development with Continuos Integration and is going to host all Pipelines (and, obviously, execute them).\nNexus\nNexus manages software &quot;artifacts&quot; required for development. It is possible to both download dependencies from Nexus and publish artifacts as well. It allows to share resources within an organization.\nSonarQube\nIt is a platform for continuous inspection of the code. It is going to be used for the Java back-end.\n58.1.3. Where can I find all My Thai Star Pipelines?\nThey are located under the MTS folder of the PL instance:\nThose Jenkins Pipelines will not have any code to execute. They&#x2019;re just pointing to all Jenkinsfiles under the /jenkins folder of the repository. They can be found here.\n58.1.4. CI in My Thai Star stack\nAngular CI\nJava CI\n58.1.5. How to configure everything out of the box\nProduction Line currently has a template to integrate My Thai Star. All information can be found at devon production line repository\n58.1.6. Angular CI\nThe Angular client-side of My Thai Star is going to have some specific needs for the CI-CD Pipeline to perform mandatory operations.\nPipeline\nThe Pipeline for the Angular client-side is going to be called MyThaiStar_FRONTEND_BUILD. It is located in the PL instance, under the MTS folder (as previously explained). It is going to follow a process flow like this one:\nEach of those steps are called stages in the Jenkins context.Let&#x2019;s see what those steps mean in the context of the Angular application:\nDeclarative: Checkout SCM\nRetrieves the project from the GitHub repository which it&#x2019;s located. This step is not defined directly in our pipeline, but as it is loaded from the repository this step should always be done at the beginning.\nDeclarative: Tool Install\nThe Pipeline needs some Tools to perform some operations with the Angular project. These tool is a correct version of NodeJS (10.14.0 LTS) with Yarn installed as global package.\ntools {\nnodejs &quot;NodeJS 10.14.0&quot;\n}\nLoading Custom Tools\nThe Pipeline also needs a browser in order to execute the tests, so in this step the chrome-stable will be loaded. We will use it in a headless mode.\ntool chrome\nFresh Dependency Installation\nThe script $ yarn does a package installation. As we always clean the workspace after the pipeline, all packages must be installed in every execution.\nCode Linting\nThis script executes a linting process of TypeScript. Rules can be defined in the tslint.json file of the project. It throws an exception whenever a file contains a non-compliant piece of code.\nExecute Angular tests\nThe CI testing of the Angular client is different than the standard local testing (adapted to CI environments, as specified in the Adaptation section of document). This script just executes the following commands:\nng test --browsers ChromeHeadless --watch=false\nSonarQube code analysis\nThe script load and execute the tool sonar-scanner. This tool is loaded here because it&#x2019;s not used in any other part of the pipeline. The sonar-scanner will take all code, upload it to sonarQube and wait until sonarQube send us a response with the quality of our code. If the code do not pass the quality gate, the pipeline will stop at this point.\nBuild Application\nThe building process of the Angular client would result in a folder called /dist in the main Angular&#x2019;s directory. That folder is the one that is going to be served afterwards as an artifact. This process has also been adapted to some Deployment needs. This building script executes the following:\nng build --configuration=docker\nDeliver application into Nexus\nOnce the scripts produce the Angular artifact (/dist folder), it&#x2019;s time to package it and store into nexus.\nDeclarative: Post Actions\nAt the end, this step is always executed, even if a previous stage fail. We use this step to clean up the workspace for future executions\npost {\nalways {\ncleanWs()\n}\n}\nAdjustments\nThe Angular project Pipeline needed some &quot;extra&quot; features to complete all planned processes. Those features resulted in some additions to the project.\nPipeline Environment\nIn order to easily reuse the pipeline in other angular projects, all variables have been defined in the block environment. All variables have the default values that Production Line uses, so if you&#x2019;re going to work in production line you won&#x2019;t have to change anything. Example:\nenvironment {\n// Script for build the application. Defined at package.json\nbuildScript = &apos;build --configuration=docker&apos;\n// Script for lint the application. Defined at package.json\nlintScript = &apos;lint&apos;\n// Script for test the application. Defined at package.json\ntestScript = &apos;test:ci&apos;\n// Angular directory\nangularDir = &apos;angular&apos;\n// SRC folder. It will be angularDir/srcDir\nsrcDir = &apos;src&apos;\n// Name of the custom tool for chrome stable\nchrome = &apos;Chrome-stable&apos;\n// sonarQube\n// Name of the sonarQube tool\nsonarTool = &apos;SonarQube&apos;\n// Name of the sonarQube environment\nsonarEnv = &quot;SonarQube&quot;\n// Nexus\n// Artifact groupId\ngroupId = &apos;com.devonfw.mythaistar&apos;\n// Nexus repository ID\nrepositoryId = &apos;pl-nexus&apos;\n// Nexus internal URL\nrepositoryUrl = &apos;http://nexus3-core:8081/nexus3/repository/maven-snapshots&apos;\n// Maven global settings configuration ID\nglobalSettingsId = &apos;MavenSettings&apos;\n// Maven tool id\nmavenInstallation = &apos;Maven3&apos;\n}\nDescription\nbuildScript: script for build the application. It must be defined at package.json.\nExample (package.json):\n{\n&quot;name&quot;: &quot;mythaistar-restaurant&quot;,\n...\n&quot;scripts&quot;: {\n...\n&quot;build&quot;: &quot;ng build&quot;,\n...\n}\n...\n}\nThis will be used as follows:\nsh &quot;&quot;&quot;yarn ${buildScript}&quot;&quot;&quot;\nlintScript: Script for lint the application. Defined at package.json\nExample (package.json):\n{\n&quot;name&quot;: &quot;mythaistar-restaurant&quot;,\n...\n&quot;scripts&quot;: {\n...\n&quot;lint&quot;: &quot;ng lint&quot;,\n...\n}\n...\n}\nThis will be used as follows:\nsh &quot;&quot;&quot;yarn ${lintScript}&quot;&quot;&quot;\ntestScript: Script for test the application. Defined at package.json\nExample (package.json):\n{\n&quot;name&quot;: &quot;mythaistar-restaurant&quot;,\n...\n&quot;scripts&quot;: {\n...\n&quot;test:ci&quot;: &quot;npm run postinstall:web &amp;&amp; ng test --browsers ChromeHeadless --watch=false&quot;,\n...\n}\n...\n}\nThis will be used as follows:\nsh &quot;&quot;&quot;yarn ${testScript}&quot;&quot;&quot;\nangularDir: Relative route to angular application. In My Thai Star this is the angular folder. The actual directory (.) is also allowed.\nsrcDir: Directory where you store the source code. For angular applications the default value is src\nchrome: Since you need a browser to run your tests, we must provide one. This variable contains the name of the custom tool for google chrome.\nsonarTool: Name of the sonarQube scanner installation.\nsonarEnv: Name of the sonarQube environment. SonarQube is the default value for PL.\ngroupId: Group id of the application. It will be used to storage the application in nexus3\nrepositoryId: Id of the nexus3 repository. It must be defined at maven global config file.\nrepositoryUrl: The url of the repository.\nglobalSettingsId: The id of the global settings file.\nmavenInstallation: The name of the maven tool.\n58.1.7. Java CI\nThe Java server-side of My Thai Star is an devon4j-based application. As long as Maven and a Java 8 are going to be needed, the Pipeline should have those tools available as well.\nPipeline\nThis Pipeline is called MyThaiStar_SERVER_BUILD, and it is located exactly in the same PL instance&#x2019;s folder than MyThaiStar_FRONTEND_BUILD. Let&#x2019;s see how the Pipeline&#x2019;s flow behaves.\nCheck those Pipeline stages with more detail:\nDeclarative: Checkout SCM\nGets the code from https://github.com/devonfw/my-thai-star . This step is not defined directly in our pipeline, but as it is loaded from the repository this step should always be done at the beginning.\nUnit Tests\nThis step will execute the project unit test with maven.\nmvn clean test\nSonarQube analysis\nThe code is evaluated using the integrated PL instance&#x2019;s SonarQube. Also, it will wait for the quality gate status. If the status is failing, the pipeline execution will be stopped.\nwithSonarQubeEnv(sonarEnv) {\nsh &quot;mvn sonar:sonar&quot;\n}\ndef qg = waitForQualityGate()\nif (qg.status != &apos;OK&apos;) {\nerror &quot;Pipeline aborted due to quality gate failure: ${qg.status}&quot;\n}\nDeliver application into Nexus\nStore all artifacts into nexus.\nmvn deploy -Dmaven.test.skip=true\nAdjustments\nPipeline Environment\nIn order to easily reuse the pipeline in other java projects, all variables have been defined in the block environment. All variables have the default values that Production Line uses, so if you&#x2019;re going to work in production line you won&#x2019;t have to change anything. Example:\nenvironment {\n// Directory with java project\njavaDir = &apos;java/mtsj&apos;\n// sonarQube\n// Name of the sonarQube environment\nsonarEnv = &quot;SonarQube&quot;\n// Nexus 3\n// Maven global settings configuration ID\nglobalSettingsId = &apos;MavenSettings&apos;\n// Maven tool id\nmavenInstallation = &apos;Maven3&apos;\n}\nDescription\njavaDir: Relative route to java application. In My Thai Star this is the java/mtsj folder. The actual directory (.) is also allowed.\nsonarEnv: Name of the sonarQube environment. SonarQube is the default value for PL.\nglobalSettingsId: The id of the global settings file. MavenSettings is the default value for PL.\nmavenInstallation: The name of the maven tool. Maven3 is the default value for PL.\nDistribution management\nThe only extra thing that needs to be added to the Java server-side is some information that determines where the artifact of the project is going to be stored in Nexus. This is going to be a section in the main pom.xml file called &lt;distributionManagement&gt;. This section will point to the PL instance&#x2019;s Nexus. Let&#x2019;s have a look at it. It&#x2019;s already configured with the PL default values.\n&lt;distributionManagement&gt;\n&lt;repository&gt;\n&lt;id&gt;pl-nexus&lt;/id&gt;\n&lt;name&gt;PL Releases&lt;/name&gt;\n&lt;url&gt;http://nexus3-core:8081/nexus/content/repositories/maven-releases/&lt;/url&gt;\n&lt;/repository&gt;\n&lt;snapshotRepository&gt;\n&lt;id&gt;pl-nexus&lt;/id&gt;\n&lt;name&gt;PL Snapshots&lt;/name&gt;\n&lt;url&gt;http://nexus3-core:8081/nexus3/repository/maven-snapshots&lt;/url&gt;\n&lt;/snapshotRepository&gt;\n&lt;/distributionManagement&gt;\n58.2. Deployment\nThe main deployment tool used for My Thai Star is be Docker.\nIt is a tool to run application in isolated environments. Those isolated environments will be what we call Docker containers. For instance, it won&#x2019;t be necessary any installation of nginx or Apache tomcat or anything necessary to deploy, because there will be some containers that actually have those technologies inside.\n58.2.1. Where Docker containers will be running?\nOf course, it is necessary to have an external Deployment Server. Every Docker process will run in it. It will be accessed from Production Line pipelines via SSH. Thus, the pipeline itself will manage the scenario of, if every previous process like testing passes as OK, stop actual containers and create new ones.\nThis external server will be located in http://de-mucdevondepl01 .\n58.2.2. Container Schema\n3 Docker containers are being used for the deployment of My Thai Star:\nnginx for the Reverse Proxy\ntomcat for the Java Server\nnginx for the Angular Client\nThe usage of the Reverse Proxy will allow the client to call via /api every single Java Server&#x2019;s REST operation. Moreover, there will only be 1 port in usage in the remote Docker host, the one mapped for the Reverse Proxy: 8080.\nBesides the deployment itself using nginx and tomcat, both client and server are previously built using nodejs and maven images. Artifacts produced by them will be pasted in servers&apos; containers using multi-stage docker builds. It will all follow this schema:\nThis orchestration of all 3 containers will be done by using a docker-compose.yml file. To redirect traffic from one container to another (i.e. reverse-proxy to angular client or angular client to java server) will be done by using, as host names, the service name docker-compose defines for each of them, followed by the internally exposed port:\nhttp://reverse-proxy:80\nhttp://angular:80\nhttp://java:8080\nNote\nA implementation using Traefik as reverse proxy instead of NGINX is also available.\n58.2.3. Run My Thai Star\nThe steps to run My Thai Star are:\nClone the repository $ git clone https://github.com/devonfw/my-thai-star.git\nRun the docker compose command: $ docker-compose up\n58.2.4. Deployment Pipelines\nAs PL does not support deployments, we have created separate pipelines for this purpose. Those pipelines are: MyThaiStar_DEPLOY-Together, MyThaiStar_FRONTEND_DEPLOY and MyThaiStar_SERVER_DEPLOY.\nThe application will be deployed using docker on a remote machine. The architecture is as follows:\nThe parts to be deployed are: an NGINX reverse proxy, the java application and the angular application.\nMyThaiStar_DEPLOY-Together Pipeline\nThe MyThaiStar_DEPLOY-Together pipeline will deploy all parts of My Thai Star application into a remote server via ssh.\nParameters\nsshAgentCredentials: The SSH private key for connection to remote server. The public key must be included in the remote server&#x2019;s authorized_keys.\nnexusApiUrl: The url to the nexus api. http://nexus3-core:8081/nexus3 is the default url for PL.\nnexusCredentialsId: The nexus credentials.\nrepository: Name of the repository where the artifacts are stored. maven-snapshots is the default value for PL.\nJAVA_VERSION: The version of the java project that you want to deploy.\nANGULAR_VERSION: The version of the java project that you want to deploy.\nEXTERNAL_SERVER_IP: The IP of the remote server where you will deploy My Thai Star.\nAPPLICATION_DIR: The folder of the application.\nPipeline steps\nCopy files to remote server: Copy all files required for the deployment to the remote server. Those files are all files inside the reverse_proxy folder in the My Thai Star repository.\nDeploy java application: Call to MyThaiStar_SERVER_DEPLOY pipeline.\nDeploy angular application: Call to MyThaiStar_FRONTEND_DEPLOY pipeline.\nMyThaiStar_SERVER_DEPLOY Pipeline\nDeploys on the server the Java part of My Thai Star.\nNote\nYou need to run the MyThaiStar_DEPLOY-Together pipeline at least once before you run this one.\nParameters\nsshAgentCredentials: The SSH private key for connection to remote server. The public key must be included in the remote server&#x2019;s authorized_keys.\nnexusApiUrl: The url to the nexus api. http://nexus3-core:8081/nexus3 is the default url for PL.\nnexusCredentialsId: The nexus credentials.\nrepository: Name of the repository where the artifacts are stored. maven-snapshots is the default value for PL.\nJAVA_VERSION: The version of the java project that you want to deploy.\nEXTERNAL_SERVER_IP: The IP of the remote server where you will deploy My Thai Star.\nAPPLICATION_DIR: The folder of the application.\nPipeline steps\nDownload artifact from Nexus: Download the artifact from nexus using the nexus3 API. The downloaded artifact will be the last snapshot of the version specified in the parameters.\nDeployment: Deploy the download artifact in the remote server. It will create a new docker image and redeploy the java docker container.\nMyThaiStar_FRONTEND_DEPLOY\nDeploys on the server the Angular part of My Thai Star\nNote\nYou need to run the MyThaiStar_DEPLOY-Together pipeline at least once before you run this one.\nParameters\nsshAgentCredentials: The SSH private key for connection to remote server. The public key must be included in the remote server&#x2019;s authorized_keys.\nnexusApiUrl: The url to the nexus api. http://nexus3-core:8081/nexus3 is the default url for PL.\nnexusCredentialsId: The nexus credentials.\nrepository: Name of the repository where the artifacts are stored. maven-snapshots is the default value for PL.\nANGULAR_VERSION: The version of the java project that you want to deploy.\nEXTERNAL_SERVER_IP: The IP of the remote server where you will deploy My Thai Star.\nAPPLICATION_DIR: The folder of the application.\nPipeline steps\nDownload artifact from Nexus: Download the artifact from nexus using the nexus3 API. The downloaded artifact will be the last snapshot of the version specified in the parameters.\nDeployment: Deploy the download artifact in the remote server. It will create a new docker image and redeploy the angular docker container.\n58.2.5. Deployment Strategies\nIn this chapter different way of deploying My Thai Star are explained. Everything will be based in Docker.\nIndependent Docker containers\nThe first way of deployment will use isolated Docker containers. That means that if the client-side container is deployed, it does not affect the server-side container&#x2019;s life cycle and vice versa.\nLet&#x2019;s show how the containers will behave during their life cycle.\n0) Copy everything you need into the Deployment Server directory\n1) Remove existing container (Nginx or Tomcat)\n2) Run new one from the Docker images collection of the external Deployment Server.\n3) Add the artifact /dist to the &quot;deployable&quot; folder of the Docker container (/usr/share/nginx/html/)\nNow, let&#x2019;s see how it&#x2019;s being executed in the command line (simplified due to documentation purposes). The next block of code represents what is inside of the last stage of the Pipeline.\nsshagent (credentials: [&apos;my_ssh_token&apos;]) {\nsh &quot;&quot;&quot;\n// Copy artifact from workspace to deployment server\n// Manage container:\ndocker rm -f [mts-container]\ndocker run -itd --name=[mts-container] [base_image]\ndocker exec [mts-container] bash -C \\\\&quot;rm [container_deployment_folder]/*\\\\&quot;\ndocker cp [artifact] [mts-container]:[container_deployment_folder]\n&quot;&quot;&quot;\n}\nFor every operation performed in the external Deployment Server, it is necessary to define where those commands are going to be executed. So, for each one of previous docker commands, this should appear before:\nssh -o StrictHostKeyChecking=no root@10.40.235.244\nDocker Compose\nThe second way of deployment will be by orchestrating both elements of the application: The Angular client-side and the Java server-side. Both elements will be running in Docker containers as well, but in this case they won&#x2019;t be independent anymore. Docker Compose will be in charge of keeping both containers up, or to put them down.\nProject adjustment\nIn order to perform this second way of deployment, some files will be created in the project. The first one is the Dockerfile for the Angular client-side. This file will pull (if necessary) an nginx Docker image and copy the Angular artifact (/dist folder) inside of the deployment folder of the image. It will be located in the main directory of the Angular client-side project.\nThe second file is the Dockerfile for the Java server-side. Its function will be quite similar to the Angular one. It will run a tomcat Docker image and copy the Java artifact (mythaistar.war file) in its deployment folder.\nFinally, as long as the docker-compose is being used, a file containing its configuration will be necessary as well. A new folder one the main My That Star&#x2019;s directory is created, and it&#x2019;s called /docker. Inside there is just a docker-compose.yml file. It contains all the information needed to orchestrate the deployment process. For example, which port both containers are going to be published on, and so on. This way of deployment will allow the application to be published or not just with one action.\ndocker-compose rm -f # down\ndocker-compose up --build -d # up fresh containers\nLet&#x2019;s have a look at the file itself:\nversion: &apos;3&apos;\nservices:\nclient_compose:\nbuild: &quot;angular&quot;\nports:\n- &quot;8091:80&quot;\ndepends_on:\n- server_compose\nserver_compose:\nbuild: &quot;java&quot;\nports:\n- &quot;9091:8080&quot;\nThis Orchestrated Deployment will offer some interesting possibilities for the future of the application.\n58.2.6. Future Deployment\nThe My Thai Star project is going to be built in many technologies. Thus, let&#x2019;s think about one deployment schema that allow the Angular client to communicate to all three back ends: Java, Node and .NET.\nAs long as Docker containers are being used, it shouldn&#x2019;t be that hard to deal with this &quot;distributed&quot; deployment. The schema represents 6 Docker containers that will have client-side(s) and server-side(s). Each of 3 Angular client containers (those in red) are going to communicate with different back-ends. So, when the deployment is finished, it would be possible to use all three server-sides just by changing the &quot;port&quot; in the URL.\nLet&#x2019;s see how it would look like:\nReverse proxy strategy using Traefik\nThis implementation is the same as described at My Thai Star deployment wiki page. The only thing that changes is that Traefik is used instead of NGINX.\nUsing Traefik as reverse proxy, we can define the routes using labels in the docker containers instead of using a nginx.conf file. With this, it is not necessary to modify the reverse proxy container for each application. In addition, as Traefik is listening to the docker daemon, it can detect new containers and create routes for them without rebooting.\nExample of labels:\nlabels:\n- &quot;traefik.http.routers.angular.rule=PathPrefix(`/`)&quot;\n- &quot;traefik.http.services.angular.loadBalancer.healthcheck.path=/health&quot;\n- &quot;traefik.http.services.angular.loadBalancer.healthcheck.interval=10s&quot;\n- &quot;traefik.http.services.angular.loadBalancer.healthcheck.scheme=http&quot;\nHow to use it\nIf you want to build the images from code, change to My Thai Star root folder and execute:\n$ docker-compose -f docker-compose.traefik.yml up -d --build\nIf you want to build the images from artifacts, change to traefik folder (reverse-proxy/traefik) and execute:\n$ docker-compose up -d --build\nAfter a seconds, when the healthcheck detects that containers are running, your application will be available at http://localhost:8090. Also, the Traefik dashboard is available at http://localhost:8080.\nIf you want to check the behaviour of the application when you scale up the backend, you can execute:\n$ docker-compose scale java=5\nWith this, the access to the java backend will be using the load balacing method: Weighted Round Robin.\n&#x2190;&#xA0;Previous:&#xA0;UI design&#xA0;| &#x2191;&#xA0;Up:&#xA0;MyThaiStar&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Contributing Guide&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-my-thai-star.asciidoc_security.html","title":"55. Security","body":"\n55. Security\n55.1. Two-Factor Authentication\nTwo-factor Authentication (2FA) provides an additional level of security to your account. Once enabled, in addition to supplying your username and password to login, you&#x2019;ll be prompted for a code generated by your Google authenticator. For example, a password manager on one of your devices.\nBy enabling 2FA, to log into your account an additional one-time password is required what requires access to your paired device. This massively increases the barrier for an attacker to break into your account.\nBackend mechanism\nIn the backend, we utilize Spring Security for any authentication.\nFollowing the arrows, one can see all processes regarding authentication. The main idea is to check all credentials depending on their 2FA status and then either grand access to the specific user or deny access. This picture illustrates a normal authentication with username and password.\nWhen dealing with 2FA, another provider and filter is handling the request from /verify\nHere you can observe which filter will be used.\nJWTAuthenticationFilter does intercept any request, which enforces being authenticated via JWT\nNote\nWhenever the secret or qr code gets transferred between two parties, one must enforce SSL/TLS or IPsec to be comply with RFC 6238.\nActivating Two-Factor Authentication\nIn the current state, TOTP\nwill be used for OTP generation. For this purpose we recommend the Google Authenticator or any TOTP generator out there.\nLogin with your account\nOpen the 2FA settings\nActivate the 2FA Status\nInitialize your device with either a QR-Code or a secret\nFrontend\nThese are the two main options, which you can obtain my toggling between QR-Code and secret.\nAfter an activation and logout. This prompt will ask you to enter the OTP given from your device.\n&#x2190;&#xA0;Previous:&#xA0;Technical design&#xA0;| &#x2191;&#xA0;Up:&#xA0;MyThaiStar&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Testing&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-my-thai-star.asciidoc_technical-design.html","title":"54. Technical design","body":"\n54. Technical design\n54.1. Data Model\n54.1.1. Data Model\n54.1.2. NoSQL Data Model\n54.2. Server Side\n54.2.1. Java design\nIntroduction\nThe Java backend for My Thai Star application is going to be based on:\nDEVON4J as the Java framework\nDevonfw as the Development environment\nCobigen as code generation tool\nTo know more details about the above technologies please visit the following documentation:\nDEVON4J\nDevonfw\nCobigen\nBasic architecture details\nFollowing the DEVON4J conventions the Java My Thai Star backend is going to be developed dividing the application in Components and using a three layers architecture.\nProject modules\nUsing the DEVON4J approach for the Java backend project we will have a structure of a Maven project formed by three projects\napi: Stores all the REST interfaces and corresponding Request/Response objects.\ncore: Stores all the logic and functionality of the application.\nserver: Configures the packaging of the application.\nWe can automatically generate this project structure using the DEVON4J Maven archetype\nComponents\nThe application is going to be divided in different components to encapsulate the different domains of the application functionalities.\nAs main components we will find:\nBookingmanagement: Manages the bookings part of the application. With this component the users (anonymous/logged in) can create new bookings or cancel an existing booking. The users with waiter role can see all scheduled bookings.\nOrdermanagement: This component handles the process to order dishes (related to bookings). A user (as a host or as a guest) can create orders (that contain dishes) or cancel an existing one. The users with waiter role can see all ordered orders.\nDishmanagement: This component groups the logic related to the menu (dishes) view. Its main feature is to provide the client with the data of the available dishes but also can be used by other components (Ordermanagement) as a data provider in some processes.\nUsermanagement: Takes care of the User Profile management, allowing to create and update the data profiles.\nAs common components (that don&#x2019;t exactly represent an application&#x2019;s area but provide functionalities that can be used by the main components):\nImagemanagement: Manages the images of the application. In a first approach the Dishmanagement component and the Usermanagement component will have an image as part of its data. The Imagemanagement component will expose the functionality to store and retrieve this kind of data.\nMailservice: with this service we will provide the functionality for sending email notifications. This is a shared service between different app components such as bookingmanagement or ordercomponent.\nOther components:\nSecurity (will manage the access to the private part of the application using a jwt implementation).\nTwitter integration: planned as a Microservice will provide the twitter integration needed for some specific functionalities of the application.\nLayers\nService Layer: this layer will expose the REST api to exchange information with the client applications.\nLogic Layer: the layer in charge of hosting the business logic of the application.\nData Access Layer: the layer to communicate with the data base.\nThis architecture is going to be reflected dividing each component of the application in different packages to match those three layers.\nComponent structure\nEach one of the components defined previously are going to be structured using the three-layers architecture. In each case we will have a service package, a logic package and a dataaccess package to fit the layers definition.\nDependency injection\nAs it is explained in the devonfw documentation we are going to implement the dependency injection pattern basing our solution on Spring and the Java standards: java.inject (JSR330) combined with JSR250.\nSeparation of API and implementation: Inside each layer we will separate the elements in different packages: api and impl. The api will store the interface with the methods definition and inside the impl we will store the class that implements the interface.\nUsage of JSR330: The Java standard set of annotations for dependency injection (@Named, @Inject, @PostConstruct, @PreDestroy, etc.) provides us with all the needed annotations to define our beans and inject them.\n@Named\npublic class MyBeanImpl implements MyBean {\n@Inject\nprivate MyOtherBean myOtherBean;\n@PostConstruct\npublic void init() {\n// initialization if required (otherwise omit this method)\n}\n@PreDestroy\npublic void dispose() {\n// shutdown bean, free resources if required (otherwise omit this method)\n}\n}\nLayers communication\nThe connection between layers, to access to the functionalities of each one, will be solved using the dependency injection and the JSR330 annotations.\nConnection Service - Logic\n@Named(&quot;DishmanagementRestService&quot;)\npublic class DishmanagementRestServiceImpl implements DishmanagementRestService {\n@Inject\nprivate Dishmanagement dishmanagement;\n// use the &apos;this.dishmanagement&apos; object to access to the functionalities of the logic layer of the component\n...\n}\nConnection Logic - Data Access\n@Named\npublic class DishmanagementImpl extends AbstractComponentFacade implements Dishmanagement {\n@Inject\nprivate DishDao dishDao;\n// use the &apos;this.dishDao&apos; to access to the functionalities of the data access layer of the component\n...\n}\nService layer\nThe services layer will be solved using REST services with the JAX-RS implementation.\nTo give service to the defined User Stories we will need to implement the following services:\nprovide all available dishes.\nsave a booking.\nsave an order.\nprovide a list of bookings (only for waiters) and allow filtering.\nprovide a list of orders (only for waiters) and allow filtering.\nlogin service (see the Security section).\nprovide the current user data (see the Security section)\nFollowing the naming conventions proposed for Devon4j applications we will define the following end points for the listed services.\n(POST) /mythaistar/services/rest/dishmanagement/v1/dish/search.\n(POST) /mythaistar/services/rest/bookingmanagement/v1/booking.\n(POST) /mythaistar/services/rest/ordermanagement/v1/order.\n(POST) /mythaistar/services/rest/bookingmanagement/v1/booking/search.\n(POST) /mythaistar/services/rest/ordermanagement/v1/order/search.\n(POST) /mythaistar/services/rest/ordermanagement/v1/order/filter (to filter with fields that does not belong to the Order entity).\n(POST) /mythaistar/login.\n(GET) /mythaistar/services/rest/security/v1/currentuser/.\nYou can find all the details for the services implementation in the Swagger definition included in the My Thai Star project on Github.\nService api\nThe api.rest package in the service layer of a component will store the definition of the service by a Java interface. In this definition of the service we will set-up the endpoints of the service, the type of data expected and returned, the HTTP method for each endpoint of the service and other configurations if needed.\n@Path(&quot;/dishmanagement/v1&quot;)\n@Consumes(MediaType.APPLICATION_JSON)\n@Produces(MediaType.APPLICATION_JSON)\npublic interface DishmanagementRestService {\n@GET\n@Path(&quot;/dish/{id}/&quot;)\npublic DishCto getDish(@PathParam(&quot;id&quot;) long id);\n...\n}\nService impl\nOnce the service api is defined we need to implement it using the Java interface as reference. We will add the service implementation class to the impl.rest package and implement the RestService interface.\n@Named(&quot;DishmanagementRestService&quot;)\npublic class DishmanagementRestServiceImpl implements DishmanagementRestService {\n@Inject\nprivate Dishmanagement dishmanagement;\n@Override\npublic DishCto getDish(long id) {\nreturn this.dishmanagement.findDish(id);\n}\n...\n}\nNote\nYou can see the Devon4j conventions for REST services here. And the My Thai Star services definition here as part of the My Thai Star project.\nLogic layer\nIn the logic layer we will locate all the business logic of the application. We will keep the same schema as we have done for the service layer, having an api package with the definition of the methods and a impl package for the implementation.\nAlso, inside the api package, a to package will be the place to store the transfer objects needed to pass data through the layers of the component.\nThe logic api definition:\npublic interface Dishmanagement {\nDishCto findDish(Long id);\n...\n}\nThe logic impl class:\n@Named\npublic class DishmanagementImpl extends AbstractComponentFacade implements Dishmanagement {\n@Inject\nprivate DishDao dishDao;\n@Override\npublic DishCto findDish(Long id) {\nreturn getBeanMapper().map(this.dishDao.findOne(id), DishCto.class);\n}\n...\n}\nThe BeanMapper will provide the needed transformations between entity and transfer objects.\nAlso, the logic layer is the place to add validation for Authorization based on roles as we will see later.\nData Access layer\nThe data-access layer is responsible for managing the connections to access and process data. The mapping between java objects to a relational database is done in Devon4j with the spring-data-jpa.\nAs in the previous layers, the data-access layer will have both api and impl packages. However, in this case, the implementation will be slightly different. The api package will store the component main entities and, inside the _api package, another api.repo package will store the Repositories. The repository interface will extend DefaultRepository interface (located in com.devonfw.module.jpa.dataaccess.api.data package of devon4j-starter-spring-data-jpa ).\nFor queries we will differentiate between static queries (that will be located in a mapped file) and dynamic queries (implemented with QueryDsl). You can find all the details about how to manage queries with Devon4j here.\nThe default data base included in the project will be the H2 instance included with the Devon4j projects.\nTo get more details about pagination, data base security, _concurrency control, inheritance or how to solve the different relationships between entities visit the official devon4j dataaccess documentation.\nSecurity with Json Web Token\nFor the Authentication and Authorization the app will implement the json web token protocol.\nJwt basics\nA user will provide a username / password combination to our auth server.\nThe auth server will try to identify the user and, if the credentials match, will issue a token.\nThe user will send the token as the Authorization header to access resources on server protected by JWT Authentication.\nJwt implementation details\nThe Json Web Token pattern will be implemented based on the Spring Security framework that is provided by default in the Devon4j projects.\nAuthentication\nBased on the Spring Security approach, we will implement a class extending WebSecurityConfigurerAdapter (Devon4j already provides the BaseWebSecurityConfig class) to define the security entry point and filters. Also, as My Thai Star is a mainly public application, we will define here the resources that won&#x2019;t be secured.\nList of unsecured resources:\n/services/rest/dishmanagement/**: to allow anonymous users to see the dishes info in the menu section.\n/services/rest/ordermanagement/v1/order: to allow anonymous users to save an order. They will need a booking token but they won&#x2019;t be authenticated to do this task.\n/services/rest/bookingmanagement/v1/booking: to allow anonymous users to create a booking. Only a booking token is necessary to accomplish this task.\n/services/rest/bookingmanagement/v1/booking/cancel/**: to allow cancelling a booking from an email. Only the booking token is needed.\n/services/rest/bookingmanagement/v1/invitedguest/accept/**: to allow guests to accept an invite. Only a guest token is needed.\n/services/rest/bookingmanagement/v1/invitedguest/decline/**: to allow guests to reject an invite. Only a guest token is needed.\nTo configure the login we will set up the HttpSecurity object in the configure method of the class. We will define a JWTLoginFilter class that will handle the requests to the /login endpoint.\nhttp.[...].antMatchers(HttpMethod.POST, &quot;/login&quot;).permitAll().[...].addFilterBefore(new JWTLoginFilter(&quot;/login&quot;, authenticationManager()), UsernamePasswordAuthenticationFilter.class);\nIn the same HttpSecurity object we will set up the filter for the rest of the requests, to check the presence of the JWT token in the header. First we will need to create a JWTAuthenticationFilter class extending the GenericFilterBean class. Then we can add the filter to the HttpSecurity object\nhttp.[...].addFilterBefore(new JWTAuthenticationFilter(), UsernamePasswordAuthenticationFilter.class);\nFinally, as default users to start using the My Thai Star app we are going to define two profiles using the inMemoryAuthentication of the Spring Security framework. In the configure(AuthenticationManagerBuilder auth) method we will create:\nuser: waiter\npassword: waiter\nrole: Waiter\nuser: user0\npassword: password\nrole: Customer\nauth.inMemoryAuthentication().withUser(&quot;waiter&quot;).password(&quot;waiter&quot;).roles(&quot;Waiter&quot;).and().withUser(&quot;user0&quot;).password(&quot;password&quot;).roles(&quot;Customer&quot;);\nToken set up\nFollowing the official documentation the implementation details for the MyThaiStar&#x2019;s jwt will be:\nSecret: Used as part of the signature of the token, acting as a private key. For the showcase purposes we will use simply &quot;ThisIsASecret&quot;.\nToken Prefix schema: Bearer. The token will look like Bearer &lt;token&gt;\nHeader: Authorization. The response header where the token will be included. Also, in the requests, when checking the token it will be expected to be in the same header.\nThe Authorization header should be part of the Access-Control-Expose-Headers header to allow clients access to the Authorization header content (the token);\nThe claims are the content of the payload of the token. The claims are statements about the user, so we will include the user info in this section.\nsubject: &quot;sub&quot;. The username.\nissuer: &quot;iss&quot;. Who creates the token. We could use the url of our service but, as this is a showcase app, we simply will use &quot;MyThaiStarApp&quot;\nexpiration date: &quot;exp&quot;. Defines when the token expires.\ncreation date: &quot;iat&quot;. Defines when the token has been created.\nscope: &quot;scope&quot;. Array of strings to store the user roles.\nSignature Algorithm: To encrypt the token we will use the default algorithm HS512.\nAn example of a token claims before encryption would be:\n{sub=waiter, scope=[ROLE_Waiter], iss=MyThaiStarApp, exp=1496920280, iat=1496916680}\nCurrent User request\nTo provide to the client with the current user data our application should expose a service to return the user details. In Devon4j applications the /general/service/impl/rest/SecurityRestServiceImpl.java class is ready to do that.\n@Path(&quot;/security/v1&quot;)\n@Named(&quot;SecurityRestService&quot;)\npublic class SecurityRestServiceImpl {\n@Produces(MediaType.APPLICATION_JSON)\n@GET\n@Path(&quot;/currentuser/&quot;)\npublic UserDetailsClientTo getCurrentUserDetails(@Context HttpServletRequest request) {\n}\n}\nwe only will need to implement the getCurrentUserDetails method.\nAuthorization\nWe need to secure three services, that only should be accessible for users with role Waiter:\n(POST) /mythaistar/services/rest/bookingmanagement/v1/booking/search.\n(POST) /mythaistar/services/rest/ordermanagement/v1/order/search.\n(POST) /mythaistar/services/rest/ordermanagement/v1/order/filter.\nAs part of the token we are providing the user Role. So, when validating the token, we can obtain that same information and build a UsernamePasswordAuthenticationToken with username and the roles as collection of Granted Authorities.\nDoing so, afterwards, in the implementation class of the logic layer we can set up the related methods with the java security &apos;@RolesAllowed&apos; annotation to block the access to the resource to users that does not match the expected roles.\n@RolesAllowed(Roles.WAITER)\npublic PaginatedListTo&lt;BookingEto&gt; findBookings(BookingSearchCriteriaTo criteria) {\nreturn findBookings(criteria);\n}\n54.2.2. .NET design\nTODO\n54.2.3. Node.js design (deprecated)\nIntroduction\nThe Node.js backend for My Thai Star application is going to be based on:\nExpress.js as the web application framework\nOASP4Fn as data access layer framework\nDynamoDB as NoSQL Database\nTo know more details about the above technologies please visit the following documentation:\nExpress.js\nOASP4Fn\nDynamoDB\nBasic architecture details\nThis structure can be shown in the following example image:\npublic - All files which be exposed on the server directly\nsrc\ndatabase folder - Folder with scripts to create/delete/seed the database\nmodel - Folder with all data model\nroutes - Folder with all Express.js routers\nutils - Folder with all utils like classes and functions\napp.ts - File with Express.js declaration\nconfig.ts - File with server configs\nlogic.ts - File with the business logic\ntest - Folder with all tests\nLayers\nService Layer: this layer will expose the REST api to exchange information with the client applications.\nLogic Layer: the layer in charge of hosting the business logic of the application.\nData Access Layer: the layer to communicate with the data base.\nService layer\nThe services layer will be solved using REST services with Express.js\nTo give service to the defined User Stories we will need to implement the following services:\nprovide all available dishes.\nsave a booking.\nsave an order.\nprovide a list of bookings (only for waiters) and allow filtering.\nprovide a list of orders (only for waiters) and allow filtering.\nlogin service (see the Security section).\nprovide the current user data (see the Security section)\nIn order to be compatible with the other backend implementations, we must follow the naming conventions proposed for Devon4j applications. We will define the following end points for the listed services.\n(POST) /mythaistar/services/rest/dishmanagement/v1/dish/search.\n(POST) /mythaistar/services/rest/bookingmanagement/v1/booking.\n(POST) /mythaistar/services/rest/ordermanagement/v1/order.\n(POST) /mythaistar/services/rest/bookingmanagement/v1/booking/search.\n(POST) /mythaistar/services/rest/ordermanagement/v1/order/search.\n(POST) /mythaistar/services/rest/ordermanagement/v1/order/filter (to filter with fields that does not belong to the Order entity).\n(POST) /mythaistar/login.\n(GET) /mythaistar/services/rest/security/v1/currentuser/.\nYou can find all the details for the services implementation in the Swagger definition included in the My Thai Star project on Github.\nTo treat these services separately, the following routers were created:\nbookingmanagement: will answer all requests with the prefix /mythaistar/services/rest/bookingmanagement/v1\ndishmanagement: will answer all requests with the prefix /mythaistar/services/rest/dishmanagement/v1\nordermanagement: will answer all requests with the prefix /mythaistar/services/rest/ordermanagement/v1\nThese routers will define the behavior for each service and use the logical layer.\nAn example of service definition:\nrouter.post(&apos;/booking/search&apos;, (req: types.CustomRequest, res: Response) =&gt; {\ntry {\n// body content must be SearchCriteria\nif (!types.isSearchCriteria(req.body)) {\nthrow {code: 400, message: &apos;No booking token given&apos; };\n}\n// use the searchBooking method defined at business logic\nbusiness.searchBooking(req.body, (err: types.Error | null, bookingEntity: types.PaginatedList) =&gt; {\nif (err) {\nres.status(err.code || 500).json(err.message);\n} else {\nres.json(bookingEntity);\n}\n});\n} catch (err) {\nres.status(err.code || 500).json({ message: err.message });\n}\n});\nLogic layer and Data access layer\nIn the logic layer we will locate all the business logic of the application. It will be located in the file logic.ts. If in this layer we need to get access to the data, we make use of data access layer directly, in this case using OASP4fn with the DynamoDB adapter.\nExample:\nexport async function cancelOrder(orderId: string, callback: (err: types.Error | null) =&gt; void) {\nlet order: dbtypes.Order;\ntry {\n// Data access\norder = await oasp4fn.table(&apos;Order&apos;, orderId).promise() as dbtypes.Order;\n[...]\n}\nWe could define the data access layer separately, but oasp4fn allows us to do this in a simple and clear way. So, we decided to not separate the access layer to the logic business.\nSecurity with Json Web Token\nFor the Authentication and Authorization the app will implement the json web token protocol.\nJwt basics\nRefer to Jwt basiscs for more information.\nJwt implementation details\nThe Json Web Token pattern will be implemented based on the JSON web token library available on npm.\nAuthentication\nBased on the JSON web token approach, we will implement a class Authentication to define the security entry point and filters. Also, as My Thai Star is a mainly public application, we will define here the resources that won&#x2019;t be secured.\nList of unsecured resources:\n/services/rest/dishmanagement/**: to allow anonymous users to see the dishes info in the menu section.\n/services/rest/ordermanagement/v1/order: to allow anonymous users to save an order. They will need a booking token but they won&#x2019;t be authenticated to do this task.\n/services/rest/bookingmanagement/v1/booking: to allow anonymous users to create a booking. Only a booking token is necessary to accomplish this task.\n/services/rest/bookingmanagement/v1/booking/cancel/**: to allow cancelling a booking from an email. Only the booking token is needed.\n/services/rest/bookingmanagement/v1/invitedguest/accept/**: to allow guests to accept an invite. Only a guest token is needed.\n/services/rest/bookingmanagement/v1/invitedguest/decline/**: to allow guests to reject an invite. Only a guest token is needed.\nTo configure the login we will create an instance of Authentication in the app file and then we will use the method auth for handle the requests to the /login endpoint.\napp.post(&apos;/mythaistar/login&apos;, auth.auth);\nTo verify the presence of the Authorization token in the headers, we will register in the express the Authentication.registerAuthentication middleware. This middleware will check if the token is correct, if so, it will place the user in the request and continue to process it. If the token is not correct it will continue processing the request normally.\napp.use(auth.registerAuthentication);\nFinally, we have two default users created in the database:\nuser: waiter\npassword: waiter\nrole: WAITER\nuser: user0\npassword: password\nrole: CUSTOMER\nToken set up\nFollowing the official documentation the implementation details for the MyThaiStar&#x2019;s jwt will be:\nSecret: Used as part of the signature of the token, acting as a private key. It can be modified at config.ts file.\nToken Prefix schema: Bearer. The token will look like Bearer &lt;token&gt;\nHeader: Authorization. The response header where the token will be included. Also, in the requests, when checking the token it will be expected to be in the same header.\nThe Authorization header should be part of the Access-Control-Expose-Headers header to allow clients access to the Authorization header content (the token);\nSignature Algorithm: To encrypt the token we will use the default algorithm HS512.\nCurrent User request\nTo provide to the client with the current user data our application should expose a service to return the user details. In this case the Authentication has a method called getCurrentUser which will return the user data. We only need register it at express.\napp.get(&apos;/mythaistar/services/rest/security/v1/currentuser&apos;, auth.getCurrentUser);\nAuthorization\nWe need to secure three services, that only should be accessible for users with role Waiter:\n(POST) /mythaistar/services/rest/bookingmanagement/v1/booking/search.\n(POST) /mythaistar/services/rest/ordermanagement/v1/order/search.\n(POST) /mythaistar/services/rest/ordermanagement/v1/order/filter.\nTo ensure this, the Authorization class has the securizedEndpoint method that guarantees access based on the role. This method can be used as middleware in secure services. As the role is included in the token, once validated we will have this information in the request and the middleware can guarantee access or return a 403 error.\napp.use(&apos;/mythaistar/services/rest/ordermanagement/v1/order/filter&apos;, auth.securizedEndpoint(&apos;WAITER&apos;));\napp.use(&apos;/mythaistar/services/rest/ordermanagement/v1/order/search&apos;, auth.securizedEndpoint(&apos;WAITER&apos;));\napp.use(&apos;/mythaistar/services/rest/bookingmanagement/v1/booking/search&apos;, auth.securizedEndpoint(&apos;WAITER&apos;));\n54.2.4. Serverless design (deprecated)\nIntroduction\nThe Node.js backend for My Thai Star application is going to be based on:\nServerless as serverless framework\nOASP4Fn as data access layer framework\nDynamoDB as NoSQL Database\nTo know more details about the above technologies please visit the following documentation:\nServerless\nOASP4Fn\nDynamoDB\nBasic architecture details\nThis structure can be shown in the following example image:\nhandlers - All function handlers following oasp4fn structure\nsrc\nmodel - Folder with all data model\nutils - Folder with all utils like classes and functions\nconfig.ts - File with server configs\nlogic.ts - File with the business logic\ntest - Folder with all tests\nLayers\nService Layer: this layer will expose the REST api to exchange information with the client applications.\nLogic Layer: the layer in charge of hosting the business logic of the application.\nData Access Layer: the layer to communicate with the data base.\nService layer\nThe services layer will be solved using REST services with Serverless\nTo give service to the defined User Stories we will need to implement the following services:\nprovide all available dishes.\nsave a booking.\nsave an order.\nprovide a list of bookings (only for waiters) and allow filtering.\nprovide a list of orders (only for waiters) and allow filtering.\nlogin service (see the Security section).\nprovide the current user data (see the Security section)\nIn order to be compatible with the other backend implementations, we must follow the naming conventions proposed for Devon4j applications. We will define the following end points for the listed services.\n(POST) /mythaistar/services/rest/dishmanagement/v1/dish/search.\n(POST) /mythaistar/services/rest/bookingmanagement/v1/booking.\n(POST) /mythaistar/services/rest/ordermanagement/v1/order.\n(POST) /mythaistar/services/rest/bookingmanagement/v1/booking/search.\n(POST) /mythaistar/services/rest/ordermanagement/v1/order/search.\n(POST) /mythaistar/services/rest/ordermanagement/v1/order/filter (to filter with fields that does not belong to the Order entity).\n(POST) /mythaistar/login.\n(GET) /mythaistar/services/rest/security/v1/currentuser/.\nYou can find all the details for the services implementation in the Swagger definition included in the My Thai Star project on Github.\nTo treat these http services, we must define the handlers following the oasp4fn convention:\n(handlers/Http/POST/dish-search-handler) /mythaistar/services/rest/dishmanagement/v1/dish/search.\n(handlers/Http/POST/booking-handler) /mythaistar/services/rest/bookingmanagement/v1/booking.\n(handlers/Http/POST/order-handler) /mythaistar/services/rest/ordermanagement/v1/order.\n(handlers/Http/POST/booking-search-handler) /mythaistar/services/rest/bookingmanagement/v1/booking/search.\n(handlers/Http/POST/order-search-handler) /mythaistar/services/rest/ordermanagement/v1/order/search.\n(handlers/Http/POST/order-filter-handler) /mythaistar/services/rest/ordermanagement/v1/order/filter (to filter with fields that does not belong to the Order entity).\n(handlers/Http/POST/login-handler) /mythaistar/login.\n(handlers/Http/GET/current-user-handler) /mythaistar/services/rest/security/v1/currentuser/.\nThese handlers will define the behavior for each service and use the logical layer.\nAn example of handler definition:\noasp4fn.config({ path: &apos;/mythaistar/services/rest/bookingmanagement/v1/booking/search&apos; });\nexport async function bookingSearch(event: HttpEvent, context: Context, callback: Function) {\ntry {\nconst search = &lt;types.SearchCriteria&gt;event.body;\nconst authToken = event.headers.Authorization;\n// falta lo que viene siendo comprobar el token y eso\nauth.decode(authToken, (err, decoded) =&gt; {\nif (err || decoded.role !== &apos;WAITER&apos;) {\nthrow { code: 403, message: &apos;Forbidden&apos;};\n}\n// body content must be SearchCriteria\nif (!types.isSearchCriteria(search)) {\nthrow { code: 400, message: &apos;No booking token given&apos; };\n}\nbusiness.searchBooking(search, (err: types.Error | null, bookingEntity: types.PaginatedList) =&gt; {\nif (err) {\ncallback(new Error(`[${err.code || 500}] ${err.message}`));\n} else {\ncallback(null, bookingEntity);\n}\n});\n});\n} catch (err) {\ncallback(new Error(`[${err.code || 500}] ${err.message}`));\n}\n}\nThe default integration for a handler is lambda. See oasp documentation for more information about default values and how to change it.\nNote\nIf you change the integration to lambda-proxy, you must take care that in this case the data will not be parsed. You must do JSON.parse explicitly\nAfter defining all the handlers, we must execute the fun command, which will generate the files serverless.yml and webpack.config.js.\nLogic layer and Data access layer\nSee in nodejs section\nSecurity with Json Web Token\nFor the Authentication and Authorization the app will implement the json web token protocol.\nJwt basics\nRefer to Jwt basiscs for more information.\nJwt implementation details\nThe Json Web Token pattern will be implemented based on the JSON web token library available on npm.\nAuthentication\nBased on the JSON web token approach, we will implement two methods in order to verify and user + generate the token and decode the token + return the user data. Also, as My Thai Star is a mainly public application, we will define here the resources that won&#x2019;t be secured.\nList of unsecured resources:\n/services/rest/dishmanagement/**: to allow anonymous users to see the dishes info in the menu section.\n/services/rest/ordermanagement/v1/order: to allow anonymous users to save an order. They will need a booking token but they won&#x2019;t be authenticated to do this task.\n/services/rest/bookingmanagement/v1/booking: to allow anonymous users to create a booking. Only a booking token is necessary to accomplish this task.\n/services/rest/bookingmanagement/v1/booking/cancel/**: to allow cancelling a booking from an email. Only the booking token is needed.\n/services/rest/bookingmanagement/v1/invitedguest/accept/**: to allow guests to accept an invite. Only a guest token is needed.\n/services/rest/bookingmanagement/v1/invitedguest/decline/**: to allow guests to reject an invite. Only a guest token is needed.\nTo configure the login we will create a handler called login and then we will use the method code to verify the user and generate the token.\napp.post(oasp4fn.config({ integration: &apos;lambda-proxy&apos;, path: &apos;/mythaistar/login&apos; });\nexport async function login(event: HttpEvent, context: Context, callback: Function) {\n.\n.\n.\n.\n}\nWe have two default users created in the database:\nuser: waiter\npassword: waiter\nrole: WAITER\nuser: user0\npassword: password\nrole: CUSTOMER\nToken set up\nSee in nodejs section\nCurrent User request\nTo provide the client with the current user data our application should expose a service to return the user details. In order to do this, we must define a handler called current-user-handler. This handler must decode the Authorization token and return the user data.\noasp4fn.config({\npath: &apos;/mythaistar/services/rest/security/v1/currentuser&apos;,\n});\nexport async function currentUser(event: HttpEvent, context: Context, callback: Function) {\nlet authToken = event.headers.Authorization;\ntry {\nauth.decode(authToken, (err: any, decoded?: any) =&gt; {\nif (err) {\ncallback(new Error(`[403] Forbidden`));\n} else {\ncallback(null, decoded);\n}\n});\n} catch (err) {\ncallback(new Error(`[${err.code || 500}] ${err.message}`));\n}\n}\nAuthorization\nWe need to secure three services, that only should be accessible for users with role Waiter:\n(POST) /mythaistar/services/rest/bookingmanagement/v1/booking/search.\n(POST) /mythaistar/services/rest/ordermanagement/v1/order/search.\n(POST) /mythaistar/services/rest/ordermanagement/v1/order/filter.\nTo ensure this, we must decode the Authorization token and check the result. As the role is included in the token, once validated we will have this information and can guarantee access or return a 403 error.\noasp4fn.config({ path: &apos;/mythaistar/services/rest/bookingmanagement/v1/booking/search&apos; });\nexport async function bookingSearch(event: HttpEvent, context: Context, callback: Function) {\nconst authToken = event.headers.Authorization;\nauth.decode(authToken, (err, decoded) =&gt; {\ntry {\nif (err || decoded.role !== &apos;WAITER&apos;) {\nthrow { code: 403, message: &apos;Forbidden&apos; };\n}\n[...]\n} catch (err) {\ncallback(new Error(`[${err.code || 500}] ${err.message}`));\n}\n});\n}\n54.2.5. GraphQL design\nTODO\n54.3. Client Side\n54.3.1. Angular design\nIntroduction\nMyThaiStar client side has been built using latest frameworks, component libraries and designs:\nAngular 4 as main front-end Framework. https://angular.io/\nAngular/CLI 1.0.5 as Angular tool helper. https://github.com/angular/angular-cli\nCovalent Teradata 1.0.0-beta4 as Angular native component library based on Material Design. https://teradata.github.io/covalent/#/\nAngular/Material2 1.0.0-beta5 used by Covalent Teradata. https://github.com/angular/material2\nNote: this dependencies are evolving at this moment and if it is possible, we are updating it on the project.\nBasic project structure\nThe project is using the basic project seed that Angular/CLI provides with &#x201C;ng new &lt;project name&gt;&#x201D;. Then the app folder has been organized as Angular recommends and goes as follows:\napp\ncomponents\nsub-components\nshared\ncomponent files\nmain app component\nassets folder\nenvironments folder\nrest of angular files\nThis structure can be shown in the following example image:\nMain Views and components\nList of components that serve as a main view to navigate or components developed to make atomically a group of functionalities which given their nature, can be highly reusable through the app.\nNote: no-name-route corresponds to whatever URL the user introduced and does not exist, it redirects to HomeComponent.\nPublic area\nAppComponent\nContains the components that are on top of all views, including:\nOrder sidenav\nSidenav where selected orders are displayed with their total price and some comments.\nNavigation sidenav (only for mobile)\nThis sidenav proposal is to let user navigate through the app when the screen is too small to show the navigation buttons on the header.\nHeader\nIt contains the title, and some other basic functions regarding open and close sidenavs.\nFooter (only for desktop)\nAt the end of the page that shows only when open on desktop.\nHomeComponent\nMain view that shows up when the app initializes.\nMenuComponent\nView where the users can view, filter and select the dishes (with their extras) they want to order it contains a component to each menu entry:\nMenu-card\nThis component composes all the data of a dish in a card. Component made to display indeterminate number of dishes easily.\nBookTableComponent\nView to make book a table in a given data with a given number of assistants or create a reservation with a number of invitations via email.\nBook-table-dialog\nDialog which opens as a result of fulfilling the booking form, it displays all the data of the booking attempt, if everything is correct, the user can send the information or cancel if something is wrong.\nInvitation-dialog\nDialog which opens as a result of fulfilling the invitation form, it displays all the data of the booking with friends attempt, if everything is correct, the user can send the information or cancel if something is wrong.\nUserArea\nGroup of dialogs with the proposal of giving some functionalities to the user, as login, register, change password or connect with Twitter.\nLogin-dialog\nDialog with a tab to navigate between login and register.\nPassword-dialog\nFunctionality reserved to already logged users, in this dialog the user can change freely their password.\nTwitter-dialog\nDialog designed specifically to connect your user account with Twitter.\nWaiter cockpit area\nRestricted area to workers of the restaurant, here we can see all information about booked tables with the selected orders and the reservations with all the guests and their acceptance or decline of the event.\nOrderCockpitComponent\nData table with all the booked tables and a filter to search them, to show more info about that table you can click on it and open a dialog.\nOrder-dialog\nComplete display of data regarding the selected table and its orders.\nReservationCockpitComponent\nData table with all the reservations and a filter to search them, to show more info about that table you can click on it and open a dialog.\nReservation-dialog\nComplete display of data regarding the selected table and its guests.\nEmail Management\nAs the application send emails to both guests and hosts, we choose an approach based on URL&#x2019;s where the email contain a button with an URL to a service in the app and a token, front-end read that token and depending on the URL, will redirect to one service or another. For example:\nhttp://localhost:4200/booking/cancel/CB_20170605_8fb5bc4c84a1c5049da1f6beb1968afc\nThis URL will tell the app that is a cancellation of a booking with the token CB_20170605_8fb5bc4c84a1c5049da1f6beb1968afc. The app will process this information, send it to back-end with the correct headers, show the confirmation of the event and redirect to home page.\nThe main cases at the moment are:\nAccept Invite\nA guest accept an invitation sent by a host. It will receive another email to decline if it change its mind later on.\nReject Invite\nA guest decline the invitation.\nCancel Reservation\nA host cancel the reservation, everybody that has accepted or not already answered will receive an email notifying this event is canceled. Also all the orders related to this reservations will be removed.\nCancel Orders\nWhen you have a reservation, you will be assigned to a token, with that token you can save your order in the restaurant. When sent, you will receive an email confirming the order and the possibility to remove it.\nServices and directives\nServices are where all the main logic between components of that view should be. This includes calling a remote server, composing objects, calculate prices, etc.\nDirectives are a single functionality that are related to a component.\nAs it can be seen in the basic structure, every view that has a minimum of logic or need to call a server has its own service located in the shared folder.\nAlso, services and directives can be created to compose a reusable piece of code that will be reused in some parts of the code:\nPrice-calculator-service\nThis service located in the shared folder of sidenav contains the basic logic to calculate the price of a single order (with all the possibilities) and to calculate the price of a full list of orders for a table. As this is used in the sidenav and in the waiter cockpit, it has been exported as a service to be imported where needed and easily testable.\nAuthentication\nAuthentication services serves as a validator of roles and login and, at the same time, stores the basic data regarding security and authentication.\nMain task of this services is to provide visibility at app level of the current user information:\nCheck if the user is logged or not.\nCheck the permissions of the current user.\nStore the username and the JWT token.\nSnackService\nService created to serve as a factory of Angular Material Snackbars, which are used commonly through the app. This service accepts some parameters to customize the snackBar and opens it with this parameters.\nWindowService\nFor responsiveness reasons, the dialogs have to accept a width parameter to adjust to screen width and this information is given by Window object, as it is a good practice to have it in an isolated service, which also calculates the width percentage to apply on the dialogs.\nEqual-validator-directive\nThis directive located in the shared folder of userArea is used in 2 fields to make sure they have the same value. This directive is used in confirm password fields in register and change password.\nMock Backend\nTo develop meanwhile a real back-end is being developed let us to make a more realistic application and to make easier the adaptation when the backend is able to be connected and called. Its structure is as following:\nContains the three main groups of functionalities in the application. Every group is composed by:\nAn interface with all the methods to implement.\nA service that implements that interface, the main task of this service is to choose between real backend and mock backend depending on an environment variable.\nMock backend service which implements all the methods declared in the interface using mock data stored in a local file and mainly uses Lodash to operate the arrays.\nReal backend service works as Mock backend but in this case the methods call for server rest services through http.\nBooking\nThe booking group of functionalities manages the calls to reserve a table with a given time and assistants or with guests, get reservations filtered, accept or decline invitations or cancel the reservation.\nOrders\nManagement of the orders, including saving, filtering and cancel an order.\nDishes\nThe dishes group of functionalities manages the calls to get and filter dishes.\nLogin\nLogin manages the userArea logic: login, register and change password.\nSecurity\nMy Thai Star security is composed by two main security services:\nAuth-guard\nFront-end security approach, this service implements an interface called CanActivate that comes from angular/router module. CanActivate interface forces you to implement a canActivate() function which returns a Boolean.\nThis service checks with the AuthService stored data if the user is logged and if he has enough permission to access the waiter cockpit. This prevents that a forbidden user could access to waiter cockpit just by editing the URL in the browser.\nJWT\nJSON Web Token consists of a token that is generated by the server when the user logs in. Once provided, the token has to be included in an Authentication header on every Http call to the rest service, otherwise the call will be forbidden.\nJWT also has an expiration date and a role checking, so if a user has not enough permissions or keeps logged for a long certain amount of time that exceeds this expiration date, the next time he calls for a service call, the server will return an error and forbid the call. You can log again to restore the token.\nHttpClient\nTo implement this Authorization header management, an HttpClient service has been implemented.\nThis services works as an envelope of Http, providing some more functionalities, likes a header management and an automatically management of a server token error in case the JWT has expired, corrupted or not permitted.\n54.3.2. Xamarin design\nTODO\n&#x2190;&#xA0;Previous:&#xA0;User Stories&#xA0;| &#x2191;&#xA0;Up:&#xA0;MyThaiStar&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Security&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-my-thai-star.asciidoc_testing.html","title":"56. Testing","body":"\n56. Testing\n56.1. Server Side\n56.1.1. Java testing\nComponent testing\nWe are going to test our components as a unit using Spring Test and Devon4j-test modules.\nIn order to test a basic component of the app first we will create a test class in the src/test/java folder and inside the main package of the test module. We will name the class following the convention.\n[Component]Test\nThen, in the declaration of the test class, we will use the @SpringBootTest annotation to run the application context. In addition, we will extend the ComponentTest from Devon4j-test module to have access to the main functionalities of the module, see more details here.\nSpring Test allows us to use Dependency Injection so we can inject our component directly using the @Inject annotation.\nEach test will be represented by a method annotated with @Test. Inside the method we will test one functionality, evaluating the result thanks to the asserts provided by the ComponentTest class that we are extending.\nA simple test example\n@SpringBootTest(classes = SpringBootApp.class)\npublic class DishmanagementTest extends ComponentTest {\n@Inject\nprivate Dishmanagement dishmanagement;\n@Test\npublic void findAllDishes() {\nPaginatedListTo&lt;DishCto&gt; result = this.dishmanagement.findDishes();\nassertThat(result).isNotNull();\n}\n...\n}\nRunning the tests\nFrom Eclipse\nWe can run the test from within Eclipse with the contextual menu Run As &gt; JUnit Test. This functionality can be launched from method level, class level or even package level. The results will be shown in the JUnit tab.\nFrom command line using Maven\nWe can also run tests using Maven and the command line, using the command mvn test (or mvn clean test).\nC:\\MyThaiStar&gt;mvn clean test\nDoing this we will run all the tests of the project (recognized by the Test word at the end of the classes) and the results will be shown by sub-project.\n...\n[D: 2017-07-17 09:30:08,457] [P: INFO ] [C: ] [T: Thread-5] [L: org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean] - [M: Closing JPA EntityManagerFactory for persistence unit &apos;default&apos;]\nResults :\nTests run: 11, Failures: 0, Errors: 0, Skipped: 1\n...\n[INFO]\n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ mtsj-server ---\n[INFO] No sources to compile\n[INFO]\n[INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ mtsj-server ---\n[INFO] No tests to run.\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO]\n[INFO] mtsj ............................................... SUCCESS [ 0.902 s]\n[INFO] mtsj-core .......................................... SUCCESS [02:30 min]\n[INFO] mtsj-server ........................................ SUCCESS [ 1.123 s]\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 02:35 min\n[INFO] Finished at: 20XX-07-17T09:30:13+02:00\n[INFO] Final Memory: 39M/193M\n[INFO] ------------------------------------------------------------------------\n56.1.2. .NET testing\nTODO\n56.1.3. Node.js testing\nTODO\n56.1.4. GraphQL testing\nTODO\n56.2. Client Side\n56.2.1. Angular testing\nMyThaiStar testing is made using Angular default testing environment and syntax language: Karma and Jasmine\nTo test an element of the application, you indicate that tests are a special type of files with the extension .spec.ts, then, in MyThaiStar angular/CLI config you can notice that there is an array with only one entry, Karma, with at the same time has one entry to Karma.config.js.\nIn the configuration of Karma we indicate which syntax language we are going to use (currently Jasmine as said before) between some other configurations, it is remarkable the last one: browsers. By default, the only available browser is chrome, that is because Karma works opening a chrome view to run all the tests, in that same window, Karma shows the result or errors of the test run. But we can add some other browser to adjust to our necessities, for example, in some automatic processes that run from console, it is not an option to open a chrome window, in that case, MyThaiStar used PhantomJS and ChromeHeadless.\nTaking all of this into account, to run the test in MyThaiStar we need to move to project root folder and run this command : ng test --browser &lt;browser&gt;\nNote\nIf you run just ng test it will run the three browser options simultaneously, giving as a result three test runs and outputs, it can cause timeouts and unwanted behaviors, if you want a shortcut to run the test with chrome window you can just run yarn test so we really encourage to not use just ng test.\nHere we are going to see how Client side testing of MyThaiStar has been done.\nTesting Components\nAngular components were created using angular/CLI ng create component so they already come with an spec file to test them. The only thing left to do is to add the providers and imports needed in the component to work as the component itself, once this is done, the most basic test is to be sure that all the dependencies and the component itself can be correctly created.\nAs an example, this is the spec.ts of the menu view component:\nall the imports...\ndescribe(&apos;MenuComponent&apos;, () =&gt; {\nlet component: MenuComponent;\nlet fixture: ComponentFixture&lt;MenuComponent&gt;;\nbeforeEach(async(() =&gt; {\nTestBed.configureTestingModule({\ndeclarations: [ MenuComponent, MenuCardComponent ],\nproviders: [SidenavService, MenuService, SnackBarService],\nimports: [\nBrowserAnimationsModule,\nBackendModule.forRoot({environmentType: 0, restServiceRoot: &apos;v1&apos;}),\nCovalentModule,\n],\n})\n.compileComponents();\n}));\nbeforeEach(() =&gt; {\nfixture = TestBed.createComponent(MenuComponent);\ncomponent = fixture.componentInstance;\nfixture.detectChanges();\n});\nit(&apos;should create&apos;, () =&gt; {\nexpect(component).toBeTruthy();\n});\n});\nFirst we declare the component to be tested and a Fixture object, then, we configure the testingModule right in the same way we could configure the MenuModule with the difference here that tests always have to use the mockBackend because we do not want to really depend on a server to test our components.\nOnce configured the test module, we have to prepare the context of the test, in this case we create the component, that is exactly what is going on in the beforeEach() function.\nFinally, we are ready to use the component and it&#x2019;s fixture to check if the component has bee correctly created.\nAt this moment this is the case for most of the components, in the future, some work would be applied on this matter to have a full testing experience in MyThaiStar components.\nDialog components\nDialog components are in a special category because they can not be tested normally. In the way Material implements the opening of dialogs, you have to create a component that will load into a dialog, to tell the module to load this components when needed, they have to be added into a special array category: EntryComponents. So, to test them, we need to import them in the test file as well.\nAlso, the testing code to open the component is a bit different too:\n...\nbeforeEach(() =&gt; {\ndialog = TestBed.get(MdDialog);\ncomponent = dialog.open(CommentDialogComponent).componentInstance;\n});\n...\nThat is right, the beforeEach() function is slightly different from the the example above, in this case we have to force to the test to know that the component is only displayed in a dialog, so we have to open a dialog with this component in order to access it.\nTesting Services\nAs well as components, services can be tested too, actually, they are even more necessary to be tested because they have inside more complex logic and data management.\nAs an example of testing services i am going to use a well done services, with a specific purpose and with its logic completely tested, the price-calculator service:\n...\ndescribe(&apos;PriceCalculatorService&apos;, () =&gt; {\nbeforeEach(() =&gt; {\nTestBed.configureTestingModule({\nproviders: [PriceCalculatorService],\n});\n});\nit(&apos;should be properly injected&apos;, inject([PriceCalculatorService], (service: PriceCalculatorService) =&gt; {\nexpect(service).toBeTruthy();\n}));\ndescribe(&apos;check getPrice method&apos;, () =&gt; {\nit(&apos;should calculate price for single order without extras&apos;, inject([PriceCalculatorService], (service: PriceCalculatorService) =&gt; {\nconst order: OrderView = {\ndish: {\nid: 0,\nprice: 12.50,\nname: &apos;Order without extras&apos;,\n},\norderLine: {\ncomment: &apos;&apos;,\namount: 1,\n},\nextras: [],\n};\nexpect(service.getPrice(order)).toEqual(order.dish.price);\n}));\n...\nIn services test, we have to inject the service in order to use it, then we can define some initializing contexts to test if the functions of the services returns the expected values, in the example we can see how an imaginary order is created and expected the function getPrice() to correctly calculate the price of that order.\nIn this same test file you can find some more test regarding all the possibilities of use in that services: orders with and without extras, single order, multiple orders and so on.\nSome services as well as the components have only tested that they are correctly created and they dependencies properly injected, in the future, will be full covering regarding this services test coverage.\nTesting in a CI environment\n56.2.2. Xamarin testing\nTODO\n56.3. End to end\n56.3.1. MrChecker E2E Testing\nIntroduction\nMrChecker is a testing framework included in devonfw with several useful modules, from which we will focus on the Selenium Module, a module designed to make end-to-end testing easier to implement.\nHow to use it\nFirst of all download the repository.\nYou must run My Thai Star frontend and backend application and modify your url to the front in mrchecker/endtoend-test/src/resources/settings.properties\nNow you can run end to end test to check if the application works properly.\nTo run the e2e test you have two options:\nThe first option is using the command line in devonfw distribution\ncd mrchecker/endtoend-test/\nmvn test -Dtest=MyThaiStarTest -Dbrowser=Chrome\noptionally you can use it with a headless version or using another navigator:\n// chrome headless (without visual component)\nmvn test -Dtest=MyThaiStarTest -Dbrowser=ChromeHeadless\n// use firefox navigator\nmvn test -Dtest=MyThaiStarTest -Dbrowser=FireFox\nThe second is importing the project in devonfw Eclipse and running MyThaiStarTest.java as JUnit (right click, run as JUnit)\nThey can be executed one by one or all in one go, comment or uncomment @Test before those tests to enable or disable them.\nFor more information about how to use MrChecker and build your own end to end test read:\n* MrChecker documentation\n* MrChecker tutorial for My Thai Star\nEnd to end tests in My Thai Star\nWe have included a test suite with four tests to run in My Thai Star to verify everything works properly.\nThe included tests do the following:\nTest_loginAndLogOut: Log in and log out.\nTest_loginFake: Attempt to log in with a fake user.\nTest_bookTable: Log in and book a table, then login with a waiter and check if the table was successfully booked.\nTest_orderMenu: Log in and order food for a certain booked table.\nThese four tests can be found inside MyThaiStarTest.java located here.\n&#x2190;&#xA0;Previous:&#xA0;Security&#xA0;| &#x2191;&#xA0;Up:&#xA0;MyThaiStar&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;UI design&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master-my-thai-star.asciidoc_ui-design.html","title":"57. UI design","body":"\n57. UI design\n57.1. Style guide\n57.2. Low and high fidelity wireframes\nHistory of mockup designs for My Thai Star.\nMTS Wireframes Low Fidelity\nMTS Wireframes High Fidelity (Sprint 1)\nMTS Wireframes High Fidelity (Sprint 1) - Copy\nMTS Wireframes High Fidelity (Sprint 1) - Mobile\nMTS Wireframes High Fidelity (Sprint 2)\nMTS Wireframes High Fidelity (Sprint 2) - Modifications\n&#x2190;&#xA0;Previous:&#xA0;Testing&#xA0;| &#x2191;&#xA0;Up:&#xA0;MyThaiStar&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;CI/CD&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/master.html","title":"not found","body":"\nGetting Started\ndevonfw ide\ndevon4j\ndevon4ng\ndevon4net\ndevon4node\ndevonfw shop floor\ncicdgen\nCobiGen&#x2009;&#x2014;&#x2009;Code-based incremental Generator\ndevonfw testing\nMyThaiStar\nContributing Guide\nRelease Notes\nNext:&#xA0;Getting Started&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/release-notes-version-2.1.asciidoc.html","title":"71. Release notes devonfw 2.1.1 &quot;Balu&quot;","body":"\n71. Release notes devonfw 2.1.1 &quot;Balu&quot;\n71.1. Version 2.1.2: OASP4J updates &amp; some new features\nWe&#x2019;ve released the latest update release of devonfw in the Balu series: version 2.1.2. The next major release, code named Courage, will be released approximately the end of June. This current release contains the following items:\n71.1.1. OASP4j 2.3.0 Release\nFriday the 12th of May 2017 OASP4J version 2.3.0 was released. Major features added are :\nDatabase Integration with PostGres, MSSQL Server, MariaDB\nAdded docs folder for gh pages and added oomph setups\nRefactored Code\nRefactored Test Infrastructure\nAdded Documentation on debugging tests\nAdded Two Batch Job tests in the restaurant sample\nBugfix: Fixed the error received when the Spring Boot Application from sample application that is created from maven archetype is launched\nBugfix: Fix for 404 error received when clicked on the link &apos;1. Table&apos; in index.html of the sample application created from maven archetype\nMore details on features added can be found at https://github.com/oasp/oasp4j/milestone/23?closed=1 .\nThe OASP4j wiki and other documents are updated for release 2.3.0.\n71.1.2. Cobigen Enhancements\nPrevious versions of CobiGen are able to generate code for REST services only. Now it is possible to generate the code for SOAP services as well. There are two use cases available in CobiGen:\nSOAP without nested data\nSOAP nested data\nThe &quot;nested data&quot; use case is when there are 3 or more entities which are interrelated with each other. Cobigen will generate code which will return the nested data. Currently Cobigen services return ETO classes, Cobigen has been enhanced as to return CTO classes (ETO + relationship).\nApart from the SOAP code generation, the capability to express nested relationships have been added to the existing ReST code generator as well.\nSee: https://github.com/devonfw/devon-guide/wiki/cookbook-cobigen-advanced-use-cases-soap-and-nested-data\n71.1.3. Micro services module (Spring Cloud/Netflix OSS)\nTo make it easier for devonfw users to design and develop applications based on microservices, this release provides a series of archetypes and resources based on Spring Cloud Netflix to automate the creation and configuration of microservices.\nNew documentation ind de devonfw Guide contais all the details to start creating microservices with devonfw\n71.1.4. Spring Integration Module\nBased on the Java Message Service (JMS) and Spring Integration, the devonfw Integration module provides a communication system (sender/subscriber) out-of-the-box with simple channels (only to send and read messages), request and reply channels (to send messages and responses) and request &amp; reply asynchronously channels. You can find more details about the implementation in the devonfw guide.\n71.1.5. WebSphere &amp; Wildfly deployment documentation\nThe new version of devonfw contains more elaborate and updated documentation about deployment on WebSpere and Wildfly.\n71.2. Version 2.1.1 Updates, fixes &amp; some new features\n71.2.1. Cobigen code-generator fixes\nThe Cobigen incremental code generator released in the previous version contained a regression which has now been fixed. Generating services in Batch mode whereby a package can be given as an input, using all Entities contained in that package, works again as expected.\nFor more information see: The Cobigen documentation and the corresponding change in the devonfw Guide\n71.2.2. Devcon enhancements\nIn this new release we have added devcon to the devonfw distribution itself so one can directly use devcon from the console.bat or ps-console.bat windows. It is therefore no longer necessary to independently install devcon. However, as devcon is useful outside of the devonfw distribution, this remains a viable option.\n71.2.3. Devon4Sencha\nin Devon4Sencha there are changes in the sample application. It now complies fully with the architecture which is known as &quot;universal app&quot;, so now it has screens custom tailored for desktop and mobile devices. All the basic logic remains the same for both versions. (The StarterTemplate is still only for creating a desktop app. This will be tackled in the next release.)\n71.2.4. New Winauth modules\nThe original winauth module that, in previous Devon versions, implemeted the Active Directory authentication and the Single Sign-on authentication now has been divided in two independent modules. The Active Directory authentication now is included in the new Winauth-ad module whereas the Single Sign-on implementation is included in a separate module called Winauth-sso.\nAlso some improvements have been added to Winauth-sso module to ease the way in which the module can be injected.\nFor more information about the update see: The Sencha docs within the devonfw Guide\n71.2.5. General updates\nThere are a series of updates to the devonfw documentation, principally the devonfw Guide. Further more, from this release on, you can find the devonfw guide in the doc folder of the distribution.\nFurthermore, the OASP4J and devonfw source-code in the &quot;examples&quot; workspace, have been updated to the latest version.\n71.3. Version 2.1 New features, improvements and updates\n71.3.1. Introduction\nWe are proud to present the new release of devonfw, version &quot;2.1&quot; which we&#x2019;ve baptized &quot;Balu&quot;. A major focus for this release is developer productivity. So that explains the name, as Balu is not just big, friendly and cuddly but also was very happy to let Mowgli do the work for him.\n71.3.2. Cobigen code-generator UI code generation and more\nThe Cobigen incremental code generator which is part of devonfw has been significantly improved. Based on a single data schema it can generate the JPA/Hibernate code for the whole service layer (from data-access code to web services) for all CRUD operations. When generating code, Cobigen is able to detect and leave untouched any code which developers have added manually.\nIn the new release it supports Spring Data for data access and it is now capable of generating the whole User Interface as well: data-grids and individual rows/records with support for filters, pagination etc. That is to say: Cobigen can now generate automatically all the code from the server-side database access layer all the way up to the UI &quot;screens&quot; in the web browser.\nCurrently we support Sencha Ext JS with support for Angular 2 coming soon. The code generated by Cobigen can be opened and used by Sencha Architect, the visual design tool, which enables the programmer to extend and enhance the generated UI non-programmatically. When Cobigen regenerates the code, even those additions are left intact. All these features combined allow for an iterative, incremental way of development which can be up to an order of an magnitude more productive than &quot;programming manual&quot;\nCobigen can now also be used for code-generation within the context of an engagement. It is easily extensible and the process of how to extend it for your own project is well documented. This becomes already worthwhile (&quot;delivers ROI&quot;) when having 5+ identical elements within the project.\nFor more information see: The Cobigen documentation and the corresponding changer in the devonfw Guide and\n71.3.3. Angular 2\nWith the official release of Angular 2 and TypeScript 2, we&#x2019;re slowly but steadily moving to embrace these important new players in the web development scene. We keep supporting the Angular 1 based OASP4js framework and are planning a migration of this framework to Angular 2 in the near future. For &quot;Balu&quot; we&#x2019;ve have decided to integrate &quot;vanilla&quot; Angular 2.\nWe have migrated the Restaurant Sample application to serve as a, documented and supported, blueprint for Angular 2 applications. Furthermore, we support three &quot;kickstarter&quot; projects which help engagement getting started with Angular2 - either using Bootstrap or Google&#xB4;s Material Design - or, alternatively, Ionic 2 (the mobile framework on top of Angular 2). For more information see: Angular 2 Kickstarter and Ionic 2 Kickstarter\n71.3.4. OASP4J 2.2.0 Release\nA new release of OASP4J, version 2.2.0, is included in this release of devonfw. This release mainly focuses on server side of oasp. i.e oasp4j.\nMajor features added are :\nUpgrade to Spring Boot 1.3.8.RELEASE\nUpgrade to Apache CXF 3.1.8\nDatabase Integration with Oracle 11g\nAdded Servlet for HTTP-Debugging\nRefactored code and improved JavaDoc\nBugfix: mvn spring-boot:run executes successfully for oasp4j application created using oasp4j template\nAdded subsystem tests of SalesmanagementRestService and several other tests\nAdded Tests to test java packages conformance to OASP conventions\nMore details on features added can be found at https://github.com/oasp/oasp4j/milestone/19?closed=1(here). The OASP4j wiki and other documents are updated for release 2.2.0.\n71.3.5. Devon4Sencha\nDevon4Sencha is an alternative view layer for web applications developed with devonfw. It is based on Sencha Ext JS. As it requires a license for commercial applications it is not provided as Open Source and is considered to be part of the IP of Capgemini.\nThese libraries provide support for creating SPA (Single Page Applications) with a very rich set of components for both desktop and mobile. In the new version we extend this functionality to support for &quot;Universal Apps&quot;, the Sencha specific term for true multi-device applications which make it possible to develop a single application for desktop, tablet as well as mobile devices. In the latest version Devon4Sencha has been upgraded to support Ext JS 6.2 and we now support the usage of Cobigen as well as Sencha Architect as extra option to improve developer productivity.\nFor more information about the update see: The Sencha docs within the devonfw Guide\n71.3.6. Devcon enhancements\nThe Devon Console, Devcon, is a cross-platform command line tool running on the JVM that provides many automated tasks around the full life-cycle of Devon applications, from installing the basic working environment and generating a new project, to running a test server and deploying an application to production. It can be used by the engagements to integrate with their proprietary tool chain.\nIn this new release we have added an optional graphical user interface (with integrated help) which makes using Devcon even easier to use. Another new feature is that it is now possible to easily extend it with commands just by adding your own or project specific Javascript files. This makes it an attractive option for project task automation. You can find more information in the Devcon Command Developers Guide\n71.3.7. Ready for the Cloud\ndevonfw is in active use in the Cloud, with projects running on IBM Bluemix and on Amazon AWS. The focus is very much to keep Cloud-specific functionality decoupled from the devonfw core. The engagement can choose between - and easily configure the use of - either CloudFoundry or Spring Cloud (alternatively, you can run devonfw in Docker containers in the Cloud as well. See elsewhere in the release notes). For more information\nabout how to configure devonfw for use in the cloud see: devonfw on Docker and devonfw in IBM Bluemix\n71.3.8. Spring Data\nThe java server stack within devonfw, OASP4J, is build on a very solid DDD architecture which uses JPA for its data access layer. We now offer integration of Spring Data as an alternative or to be used in conjunction with JPA. Spring Data offers significant advantages over JPA through its query mechanism which allows the developer to specify complex queries in an easy way. Overall working with Spring Data should be quite more productive compared with JPA for the average or junior developer. And extra advantage is that Spring Data also allows - and comes with support for - the usage of NoSQL databases like MongoDB, Cassandra, DynamoDB etc. THis becomes especially critical in the Cloud where NoSQL databases typically offer better scalability than relational databases.\nFor more information see: Integrating Spring Data in OASP4J\n71.3.9. Videos content in the devonfw Guide\nThe devonfw Guide is the single, authoritative tutorial and reference (&quot;cookbook&quot;) for all things devonfw, targeted at the general developer working with the platform (there is another document for Architects). It is clear and concise but because of the large scope and wide reach of devonfw, it comes with a hefty 370+ pages. For the impatient - and sometimes images do indeed say more than words - we&#x2019;ve added 17 videos to the Guide which significantly speed up getting started with the diverse aspects of devonfw.\nFor more information see: Video releases on TeamForge\n71.3.10. Containerisation with Docker and the Production Line\nDocker (see: https://www.docker.com/) containers wrap a piece of software in a complete filesystem that contains everything needed to run: code, runtime, system tools, system libraries &#x2013; anything that can be installed on a server. Docker containers resemble virtual machines but are far more resource efficient. Because of this, Docker and related technologies like Kubernetes are taking the Enterprise and Cloud by storm. We have certified and documented the usage of devonfw on Docker so we can now firmly state that &quot;devonfw is Docker&quot; ready. All the more so as the iCSD Production Line is now supporting devonfw as well. The Production Line is a Docker based set of methods and tools that make possible to develop custom software to our customers on time and with the expected quality. By having first-class support for devonfw on the Production Line, iCSD has got an unified, integral solution which covers all the phases involved on the application development cycle from requirements to testing and hand-off to the client.\nSee: devonfw on Docker and devonfw on the Production Line\n71.3.11. Eclipse Neon\ndevonfw comes with its own pre configured and enhanced Eclipse based IDE: the Open Source &quot;OASP IDE&quot; and &quot;devonfw Distr&quot; which falls under Capgemini IP. We&#x2019;ve updated both versions to the latest stable version of Eclipse, Neon. From Balu onwards we support the IDE on Linux as well and we offer downloadable versions for both Windows and Linux.\nSee: The Devon IDE\n71.3.12. Default Java 8 with Java 7 compatibility\nFrom version 2.1. &quot;Balu&quot; onwards, devonfw is using by default Java 8 for both the tool-chain as well as the integrated development environments. However, both the framework as well as the IDE and tool-set remain fully backward compatible with Java 7. We have added documentation to help configuring aspects of the framework to use Java 7 or to upgrade existing projects to Java 8. See: Compatibility guide for Java7, Java8 and Tomcat7, Tomcat8\n71.3.13. Full Linux support\nIn order to fully support the move towards the Cloud, from version 2.1. &quot;Balu&quot; onwards, devonfw is fully supported on Linux. Linux is the de-facto standard for most Cloud providers. We currently only offer first-class support for Ubuntu 16.04 LTS onward but most aspects of devonfw should run without problems on other and older distributions as well.\n71.3.14. Initial ATOM support\nAtom is a text editor that&#x2019;s modern, approachable, yet hackable to the core&#x2014;a tool you can customize to do anything but also use productively without ever touching a config file. It is turning into a standard for modern web development. In devonfw 2.1 &quot;Balu&quot; we provide a script which installs automatically the most recent version of Atom in the devonfw distribution with a preconfigured set of essential plugins. See: OASP/devonfw Atom editor (&quot;IDE&quot;) settings &amp; packages\n71.3.15. Database support\nThrough JPA (and now Spring Data as well) devonfw supports many databases. In Balu we&#x2019;ve extended this support to prepared configuration, extensive documentations and supporting examples for all major &quot;Enterprise&quot; DB servers. So it becomes even easier for engagements to start using these standard database options. Currently we provide this extended support for Oracle, Microsoft SQL Server, MySQL and PostgreSQL.\nFor more information see: OASP Database Migration Guide\n71.3.16. File upload and download\nFile up and download was supported in previous version of the framework, but as these operations are common but complex, we&#x2019;ve extended the base functionality and improved the available documentation so it becomes substantially easier to offer both File up- as well as download in devonfw based applications. See: devonfw Guide Cookbook: File Upload and Download\n71.3.17. Internationalisation (I18N) improvements\nLikewise, existing basic Internationalisation (I18N) support has been significantly enhanced through an new devonfw module and extended to support Ext JS and Angular 2 apps as well. This means that both server as well as client side applications can be made easily to support multiple languages (&quot;locales&quot;), using industry standard tools and without touching programming code (essential when working with teams of translators). For more information see: The I18N (Internationalization) module and Client GUI Sencha i18n\n71.3.18. Asynchronous HTTP support\nAsynchronous HTTP is an important feature allowing so-called &quot;long polling&quot; HTTP Requests (for streaming applications, for example) or with requests sending large amounts of data. By making HTTP Requests asynchronous, devonfw server instances can better support these types of use-cases while offering far better performance. Documentation about how to include the new devonfw module implementing this feature can be found at: The devonfw async module\n71.3.19. Security and License guarantees\nIn devonfw security comes first. The components of the framework are designed and implemented according to the recommendations and guidelines as specified by OWASP in order to confront the top 10 security vulnerabilities.\nFrom version 2.1 &quot;Balu&quot; onward we certify that devonfw has been scanned by software from &quot;Black Duck&quot;. This verifies that devonfw is based on 100% Open Source Software (non Copyleft) and demonstrates that at moment of release there are no known, critical security flaws. Less critical issues are clearly documented.\n71.3.20. Documentation improvements\nApart from the previously mentioned additions and improvements to diverse aspects of the devonfw documentation, principally the devonfw Guide, there are a number of other important changes. We&#x2019;ve incorporated the Devon Modules Developer&#xB4;s Guide which describes how to extend devonfw with its Spring-based module system. Furthermore we&#x2019;ve significantly improved the Guide to the usage of web services. We&#x2019;ve included a Compatibility Guide which details a series of considerations related with different version of the framework as well as Java 7 vs 8. And finally, we&#x2019;ve extended the F.A.Q. to provide the users with direct answers to common, Frequently Asked Questions.\n71.3.21. Contributors\nMany thanks to adrianbielewicz, aferre777, amarinso, arenstedt, azzigeorge, cbeldacap, cmammado, crisjdiaz, csiwiak, Dalgar, drhoet, Drophoff, dumbNickname, EastWindShak, fawinter, fbougeno, fkreis, GawandeKunal, henning-cg, hennk, hohwille, ivanderk, jarek-jpa, jart, jensbartelheimer, jhcore, jkokoszk, julianmetzler, kalmuczakm, kiran-vadla, kowalj, lgoerlach, ManjiriBirajdar, MarcoRose, maybeec, mmatczak, nelooo, oelsabba, pablo-parra, patrhel, pawelkorzeniowski, PriyankaBelorkar, RobertoGM, sekaiser, sesslinger, SimonHuber, sjimenez77, sobkowiak, sroeger, ssarmokadam, subashbasnet, szendo, tbialecki, thoptr, tsowada, znazir and anyone who we may have forgotten to add!\n&#x2190;&#xA0;Previous:&#xA0;devonfw Release notes 2.2 &quot;Courage&quot;&#xA0;| &#x2191;&#xA0;Up:&#xA0;Release Notes&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide\n"},{"id":"../website/pages/docs/release-notes-version-2.2.asciidoc.html","title":"70. devonfw Release notes 2.2 &quot;Courage&quot;","body":"\n70. devonfw Release notes 2.2 &quot;Courage&quot;\n70.1. Production Line Integration\ndevonfw is now fully supported on the Production Line v1.3 and the coming v2.0. Besides that, we now &quot;eat our own dogfood&quot; as the whole devonfw project, all &quot;buildable assets&quot;, now run on the Production Line.\n70.2. OASP4js 2.0\nThe main focus of the Courage release is the renewed introduction of &quot;OASP for JavaScript&quot;, or OASP4js. This new version is a completely new implementation based on Angular (version 4). This new &quot;stack&quot; comes with:\nNew application templates for Angular 4 application (as well as Ionic 3)\nA new reference application\nA new tutorial (and Architecture Guide following soon)\nComponent Gallery\nNew Cobigen templates for generation of both Angular 4 and Ionic 3 UI components (&quot;screens&quot;)\nIntegration of Covalent and Bootstrap offering a large number of components\nmy-thai-star, a showcase and reference implementation in Angular of a real, responsive usable app using recommended architecture and patterns\nA new Tutorial using my-thai-star as a starting point\nSee:\nhttps://github.com/oasp/oasp4js-application-template\nhttps://github.com/oasp/oasp4js-angular-catalog\nhttps://github.com/oasp/my-thai-star/tree/develop/angular\n70.3. A new OASP Portal\nAs part of the new framework(s) we have also done a complete redesign of the OASP Portal website at http://oasp.io/ which should make all things related with OASP more accessible and easier to find.\n70.4. New Cobigen\nMajor changes in this release:\nSupport for multi-module projects\nClient UI Generation:\nNew Angular 4 templates based on the latest - angular project seed\nBasic Typescript Merger\nBasic Angular Template Merger\nJSON Merger\nRefactored oasp4j templates to make use of Java template logic feature\nBugfixes:\nFixed merging of nested Java annotations including array values\nmore minor issues\nUnder the hood:\nLarge refactoring steps towards language agnostic templates formatting sensitive placeholder descriptions automatically formatting camelCase to TrainCase to snake-case, etc.\nEasy setup of CobiGen IDE to enable fluent contribution\nCI integration improved to integrate with GitHub for more valuable feedback\nSee: https://github.com/devonfw/tools-cobigen/releases\n70.5. MyThaiStar: New Restaurant Example, reference implementation &amp; Methodology showcase\nA major part of the new devonfw release is the incorporation of a new application, &quot;my-thai-star&quot; which among others:\nserve as an example of how to make a &quot;real&quot; devonfw application (i.e. the application could be used for real)\nServes as an attractive showcase\nServes as a reference application of devonfw patterns and practices as well as the standard example in the new devonfw tutorial\nhighlights modern security option like JWT Integration\nThe application is accompanied by a substantial new documentation asset, the devonfw methodology, which described in detail the whole lifecycle of the development of a devonfw application, from requirements gathering to technical design. Officially my-that-star is still considered to be an incubator as especially this last part is still not as mature as it could be. But the example application and tutorial are 100% complete and functional and form a marked improvement over the &quot;old&quot; restaurant example app. My-Thai-star will become the standard example app from devonfw 3.0 onwards.\nSee: https://github.com/oasp/my-thai-star\nhttps://github.com/oasp/my-thai-star/wiki\n70.6. The new OASP Tutorial\nThe OASP Tutorial is a new part of the combined OASP / devonfw documentation which changes the focus of how people can get started with the platform\nThere are tutorials for OASP4j, OASP4js (Angular), OASP4fn and more to come. My-Thai-Star is used throughout the tutorial series to demonstrate the basic principles, architecture, and good practices of the different OASP &quot;stacks&quot;. There is an elaborated exercise where the readers get to write their own application &quot;JumpTheQueue&quot;.\nWe hope that the new tutorial offers a better, more efficient way for people to get started with devonfw. Answering especially the question: how to make a devonfw application.\nOasp4j tutorial: https://github.com/oasp/oasp-tutorial-sources/wiki/OASP4jGettingStartedHome\nOasp4js tutorial: https://github.com/oasp/oasp-tutorial-sources/wiki/OASP4jsGettingStartedHome\nOasp4fn tutorial: https://github.com/oasp/oasp-tutorial-sources/wiki/OASP4FnGettingStartedHome\n70.7. OASP4j 2.4.0\n&quot;OASP for Java&quot; or OASP4j now includes updated versions of the latest stable versions of Spring Boot and the Spring Framework and all related dependencies. This allows guaranteed, stable, execution of any devonfw 2.X application on the latest versions of the Industry Standard Spring stack.\nAnother important new feature is a new testing architecture/infrastructure. All database options are updated to the latest versions as well as guaranteed to function on all Application Servers which should cause less friction and configuration time when starting a new OASP4j project.\nDetails:\nSpring Boot Upgrade to 1.5.3\nUpdated all underlying dependencies\nSpring version is 4.3.8\nExclude Third Party Libraries that are not needed from sample restaurant application\nBugfix:Fixed the &apos;WhiteLabel&apos; error received when tried to login to the sample restaurant application that is deployed onto external Tomcat\nBugfix:Removed the API api.org.apache.catalina.filters.SetCharacterEncodingFilter and used spring framework&#x2019;s API org.springframework.web.filter.CharacterEncodingFilter instead\nBugfix:Fixed the error &quot;class file for javax.interceptor.InterceptorBinding not found&quot; received when executing the command &apos;mvn site&apos; when trying to generate javadoc using Maven javadoc plugin\nRemoved the deprecated API io.oasp.module.web.common.base.PropertiesWebApplicationContextInitializer\nDocumentation of the usage of UserDetailsService of Spring Security\nSee: https://github.com/oasp/oasp4j\nWiki: https://github.com/oasp/oasp4j/wiki\n70.8. Microservices Netflix\ndevonfw now includes a microservices implementation based on Spring Cloud Netflix. It provides a Netflix OSS integrations for Spring Boot apps through autoconfiguration and binding to the Spring Environment. It offers microservices archetypes and a complete user guide with all the details to start creating microservices with devonfw.\nSee: https://github.com/devonfw-forge/devon-guide/wiki/devon-microservices\n70.9. devonfw distribution based on Eclipse OOMPH\nThe new Eclipse devonfw distribution is now based on Eclipse OOMPH, which allows us, an any engagement, to create and manage the distribution more effectively by formalizing the setup instructions so they can be performed automatically (due to a blocking issue postponed to devonfw 2.2.1 which will be released a few weeks after 2.2.0)\n70.10. Visual Studio Code / Atom\nThe devonfw distro now contains Visual Studio Code alongside Eclipse in order to provide a default, state of the art, environment for web based development.\nSee: https://github.com/oasp/oasp-vscode-ide\n70.11. More I18N options\nThe platform now contains more documentation and a conversion utility which makes it easier to share i18n resource files between the different frameworks.\nSee: https://github.com/devonfw/devon/wiki/cookbook-i18n-resource-converter\n70.12. Spring Integration as devonfw Module\nThis release includes a new module based on the Java Message Service (JMS) and Spring Integration which provides a communication system (sender/subscriber) out-of-the-box with simple channels (only to send and read messages), request and reply channels (to send messages and responses) and request &amp; reply asynchronously channels.\nSee: https://github.com/devonfw/devon/wiki/cookbook-integration-module\n70.13. devonfw Harvest contributions\ndevonfw contains a whole series of new components obtained through the Harvesting process. Examples are :\nNew backend IP module Compose for Redis: management component for cloud environments. Redis is an open-source, blazingly fast, key/value low maintenance store. Compose&#x2019;s platform gives you a configuration pre-tuned for high availability and locked down with additional security features. The component will manage the service connection and the main methods to manage the key/values on the storage. The library used is &quot;lettuce&quot;.\nSencha component for extending GMapPanel with the following functionality :\nMarkers management\nGoogle Maps options management\nGeoposition management\nSearch address and coordinates management\nMap events management\nMap life cycle and behavior management\nSencha responsive Footer that moves from horizontal to vertical layout depending on the screen resolution or the device type. It is a simple functionality but we consider it very useful and reusable.\nSee: https://github.com/devonfw/devon/wiki/cookbook-compose-for-redis-module\n70.14. More Deployment options to JEE Application Servers and Docker/CloudFoundry\nThe platform now fully supports deployment on the latest version of Weblogic, Websphere, Wildfly (JBoss) as well as Docker and Cloudfoundtry\nSee: https://github.com/devonfw/devon/wiki/Deployment-on-WebLogic\nhttps://github.com/devonfw/devon/wiki/cookbook-Deployment-on-WebSphere\nhttps://github.com/devonfw/devon/wiki/cookbook-Deployment-on-Wildfly\n70.15. Devcon on Linux\nDevcon is now fully supported on Linux which, together with the devonfw distro running on Linux, makes devonfw fully multi-platform and Cloud compatible (as Linux is the default OS in the Cloud!)\nSee: https://github.com/devonfw/devcon/releases\n70.16. New OASP Incubators\nFrom different Business Units (countries) have contributed &quot;incubator&quot; frameworks:\nOASP4NET (Stack based on .NET Core / .NET &quot;Classic&quot; (4.6))\nOASP4X (Stack based on Xamarin)\nOASP4Fn (Stack based on Node-js/Serverless): https://github.com/oasp/oasp4fn\nAn &quot;incubator&quot; status means that the frameworks are production ready, all are actually already used in production, but are still not fully compliant with the OASP definition of a &quot;Minimally Viable Product&quot;.\nDuring this summer the OASP4NET and OASP4X repos will be properly installed. In the mean time, if you want to have access to the source code, please contact the devonfw Core Team.\n&#x2190;&#xA0;Previous:&#xA0;devonfw Release notes 2.3 &quot;Dash&quot;&#xA0;| &#x2191;&#xA0;Up:&#xA0;Release Notes&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;Release notes devonfw 2.1.1 &quot;Balu&quot;&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/release-notes-version-2.3.asciidoc.html","title":"69. devonfw Release notes 2.3 &quot;Dash&quot;","body":"\n69. devonfw Release notes 2.3 &quot;Dash&quot;\n69.1. Release: improving &amp; strengthening the Platform\nWe are proud to announce the immediate release of devonfw version 2.3 (code named &#x201C;Dash&#x201D; during development). This release comes with a bit of a delay as we decided to wait for the publication of OASP4j 2.5. &#x201C;Dash&#x201D; contains a slew of new features but in essence it is already driven by what we expect to be the core focus of 2018: strengthening the platform and improving quality.\nAfter one year and a half of rapid expansion, we expect the next release(s) of the devonfw 2.x series to be fully focused on deepening the platform rather than expanding it. That is to say: we should work on improving existing features rather than adding new ones and strengthen the qualitative aspects of the software development life cycle, i.e. testing, infrastructure (CI, provisioning) etc.\n&#x201C;Dash&#x201D; already is very much an example of this. This release contains the Allure Test Framework as an incubator. This is an automated testing framework for functional testing of web applications. Another incubator is the devonfw Shop Floor which intended to be a compilation of DevOps experiences from the devonfw perspective. And based on this devonfw has been OpenShift Primed (&#x201C;certified&#x201D;) by Red Hat.\nThere is a whole range of new features and improvements which can be seen in that light. OASP4j 2.5 changes and improves the package structure of the core Java framework. The My Thai Star sample app has now been fully integrated in the different frameworks and the devonfw Guide has once again been significantly expanded and improved.\n69.2. An industrialized platform for the ADcenter\nAlthough less visible to the overall devonfw community, an important driving force was (meaning that lots of work has been done in the context of) the creation of the ADcenter concept towards the end of 2017. Based on a radical transformation of on/near/offshore software delivery, the focus of the ADcenters is to deliver agile &amp; accelerated &#x201C;Rightshore&#x201D; services with an emphasis on:\nDelivering Business Value and optimized User Experience\nInnovative software development with state of the art technology\nHighly automated devops; resulting in lower costs &amp; shorter time-to-market\nThe first two ADcenters, in Valencia (Spain) and Bangalore (India), are already servicing clients all over Europe - Germany, France, Switzerland and the Netherlands - while ADcenter aligned production teams are currently working for Capgemini UK as well (through Spain).Through the ADcenter, Capgemini establishes industrialized innovation; designed for &amp; with the user. The availability of platforms for industrialized software delivery like devonfw and the Production Line has allowed us to train and make available over a 150 people in very short time.\nThe creation of the ADcenter is such a short time is visible proof that we&#xB4;re getting closer to a situation where devonfw and Production Line are turning into the default development platform for APPS2, thereby standardizing all aspects of the software development life cycle: from training and design, architecture, devops and development, all the way up to QA and deployment.\n69.3. Changes and new features\n69.3.1. devonfw dist\nThe devonfw dist, or distribution, i.e. the central zip file which contains the main working environment for the devonfw developer, has been significantly enhanced. New features include:\nEclipse Oxygen integrated\nCheckStyle Plugin installed and configured\nSonarLint Plugin installed and configured\nGit Plugin installed\nFindBugs replaced by SpotBugs and configured\nTomcat8 specific Oxygen configuration\nCobiGen Plugin installed\nOther Software\nCmder integrated (when console.bat launched)\nVisual Studio Code latest version included and pre-configured with https://github.com/oasp/oasp-vscode-ide\nAnt updated to latest.\nMaven updated to latest.\nJava updated to latest.\nNodejs LTS updated to latest.\n@angular/cli included.\nYarn package manager included.\nPython3 integrated\nSpyder3 IDE integrated in python3 installation\nOASP4JS-application-template for Angular5 at workspaces/examples\nDevon4sencha starter templates updated\n69.3.2. OASP4j 2.5\nSupport for JAX-RS &amp; JAX-WS clients\nWith the aim to enhance the ease in consuming RESTful and SOAP web services, JAX-RS and JAX-WS clients have been introduced. They enable developers to concisely and efficiently implement portable client-side solutions that leverage existing and well-established client-side HTTP connector implementations. Furthermore, the getting started time for consuming web services has been considerably reduced with the default configuration out-of-the-box which can be tweaked as per individual project requirements.\nSee: https://github.com/oasp/oasp4j/issues/358\nSeparate security logs for OASP4J log component\nBased on OWASP(Open Web Application Security Project), OASP4J aims to give developers more control and flexibility with the logging of security events and tracking of forensic information. Furthermore, it helps classifying the information in log messages and applying masking when necessary. It provides powerful security features while based on set of logging APIs developers are already familiar with over a decade of their experience with Log4J and its successors.\nSee: https://github.com/oasp/oasp4j/issues/569\nSupport for Microservices\nIntegration of an OASP4J application to a Microservices environment can now be leveraged with this release of OASP4J. Introduction of service clients for RESTful and SOAP web services based on Java EE give developers agility and ease to access microservices in the Devon framework. It significantly cuts down the efforts on part of developers around boilerplate code and stresses more focus on the business code improving overall efficiency and quality of deliverables.\nSee: https://github.com/oasp/oasp4j/pull/589/commits\n69.3.3. Cobigen\nA new version of Cobigen has been included. New features include:\nSwagger/Yaml Plugin for CobiGen. Cobigen is able to read a swagger definition file that follows the OpenAPI 3.0 spec and generate code. A preliminary release was already included in 2.2.1 but the current version is much more mature and stable. See: https://github.com/devonfw/tools-cobigen/wiki/howto_openapi_generation\nIntegration of CobiGen into Maven build process. This already existed but has been improved. It consists mainly of documentation + better log output and bug fixes. See: https://github.com/devonfw/tools-cobigen/wiki/cobigen-maven_configuration\nCobiGen Ionic CRUD App generation based on https://github.com/oasp/oasp4js-ionic-application-template\nCobigen_Templates project and docs updated\nBugfixes and Hardening\n69.3.4. My Thai Star Sample Application\nFrom this release on the My Thai Star application has been fully integrated in the different frameworks in the platform. Further more, a more modularized approach has been followed in the current release of My Thai star application to decouple client from implementation details. Which provides better encapsulation of code and dependency management for API and implementation classes. This has been achieved with creation of a new &#x201C;API&#x201D; module that contain interfaces for REST services and corresponding Request/Response objects. With existing &#x201C;Core&#x201D; module being dependent on &#x201C;API&#x201D; module. To read further you can follow the link https://github.com/oasp/my-thai-star/wiki/java-design#basic-architecture-details\nFurthermore: an email and Twitter micro service were integrated in my-thai-star. This is just for demonstration purposes. A full micro service framework is already part of oasp4j 2.5.0\n69.3.5. Documentation refactoring\nThe complete devonfw guide is restructured and refactored. Getting started guides are added for easy start with devonfw.Integration of the new Tutorial with the existing devonfw Guide whereby existing chapters of the previous tutorial were converted to Cookbook chapters. Asciidoctor is used for devonfw guide PDF generation.\nSee: https://github.com/devonfw/devon-guide/wiki\n69.3.6. OASP4JS\nThe following changes have been incorporated in OASP4JS:\nAngular CLI 1.6.0,\nAngular 5.1,\nAngular Material 5 and Covalent 1.0.0 RC1,\nPWA enabled,\nCore and Shared Modules included to follow the recommended Angular projects structure,\nYarn and NPM compliant since both lock files are included in order to get a stable installation.\n69.3.7. Admin interface for oasp4j apps\nThe new version includes an Integration of an admin interface for oasp4j apps (Spring Boot). This module is based on CodeCentric&#xB4;s Spring Boot Admin (https://github.com/codecentric/spring-boot-admin). See: https://github.com/devonfw/devon-guide/wiki/Spring-boot-admin-Integration-with-devon4j\n69.3.8. Devcon\nA new version of Devcon has been released. Fixes and new features include:\nRenaming of system Commands.\nNew menu has been added - &#x201C;other modules&#x201D;, if menus are more than 10, other modules will display some menus.\nA progress bar has been added for installing the distribution\n69.3.9. devonfw Modules\nExisting devonfw modules can now be accessed with the help of starters following namespace devonfw-&lt;module_name&gt;-starter. Starters available for modules:\nReporting module\nWinAuth AD Module\nWinAuth SSO Module\nI18n Module\nAsync Module\nIntegration Module\nMicroservice Module\nCompose for Redis Module\nSee: https://github.com/devonfw/devon/wiki#ip-modules\n69.3.10. devonfw Shop Floor\nThis incubator is intended to be a compilation of DevOps experiences from the devonfw perspective. &#x201C;How we use our devonfw projects in DevOps environments&#x201D;. Integration with the Production Line, creation and service integration of a Docker-based CI environment and deploying devonfw applications in an OpenShift Origin cluster using devonfw templates.\nSee: https://github.com/devonfw/devonfw-shop-floor\n69.3.11. devonfw-testing\nThe Allure Test Framework is an automated testing framework for functional testing of web applications and in coming future native mobile apps, web services and databases. All modules have tangible examples of how to build resilient integration test cases based on delivered functions.\nExamples available under embedded project &#x201C;Allure-App-Under-Test&#x201D; and in project wiki: https://github.com/devonfw/devonfw-testing/wiki\nHow to install: https://github.com/devonfw/devonfw-testing/wiki/How-to-install\nRelease Notes:\nCore Module &#x2013; ver.4.12.0.3:\nTest report with logs and/or screenshots\nTest groups/tags\nData Driven (inside test case, external file)\nTest case parallel execution\nRun on independent Operating System (Java)\nExternalize test environment (DEV, QA, PROD)\nUI Selenium module &#x2013; ver. 3.4.0.3:\nMalleable resolution ( Remote Web Design, Mobile browsers)\nSupport for many browsers( Internet Explorer, Edge, Chrome, Firefox, Safari)\nUser friendly actions ( elementCheckBox, elementDropdown, etc. )\nUb&#xED;quese test execution (locally, against Selenium Grid through Jenkins)\nPage Object Model architecture\nSelenium WebDriver library ver. 3.4.0\nSee: https://github.com/devonfw/devonfw-testing/wiki\n69.3.12. DOT.NET Framework incubators\nThe .NET Core and Xamarin frameworks are still under development by a workgroup from The Netherlands, Spain, Poland, Italy, Norway and Germany. The 1.0 release is expected to be coming soon but the current incubator frameworks are already being used in several engagements. Some features to highlight are:\nFull .NET implementation with multi-platform support\nDetailed documentation for developers\nDocker ready\nWeb API server side template :\nSwagger auto-generation\nJWT security\nEntity Framework Support\nAdvanced log features\nXamarin Templates based on Excalibur framework\nMy Thai Star implementation:\nBackend (.NET Core)\nFrontEnd (Xamarin)\n69.3.13. devonfw has been Primed by Red Hat for OpenShift\nOpenShift is a supported distribution of Kubernetes from Red Hat for container-based software deployment and management. It is using Docker containers and DevOps tools for accelerated application development. Using Openshift allows Capgemini to avoid Cloud Vendor lock-in. Openshift provides devonfw with a state of the art CI/CD environment (devonfw Shop Floor), providing devonfw with a platform for the whole development life cycle: from development to staging / deploy.\nSee https://hub.openshift.com/primed/120-capgemini and https://github.com/oasp/s2i\n69.3.14. Harvested components and modules\nThe devonfw Harvesting process continues to add valuable components and modules to the devonfw platform. The last months the following elements were contributed:\nService Client support (for Micro service Projects).\nThis client is for consuming microservices from other application.This solution is already very flexible and customizable.As of now,this is suitable for small and simple project where two or three microservices are invoked. Donated by J&#xF6;rg Holwiller. See: https://github.com/devonfw/devon-microservices\nJHipster devonfw code generation\nThis compomnent was donated by the ADcenter in Valencia. It was made in order to comply with strong requirements (especially from the French BU) to use jHipster for code generation.\nJHipster is a code generator based on Yeoman generators. Its default generator generator-jhipster generates a specific JHipster structure. The purpose of generator-jhipster-DevonModule is to generate the structure and files of a typical OASP4j project. It is therefore equivalent to the standard OASP4j application template based Cobige code generation.\nSee: https://github.com/devonfw/devon-guide/wiki/cookbook-devon-jhipster-module\nSimple Jenkins task status dashboard\nThis component has been donated by, has been harvested from system in use by, Capgemini Valencia. This dashboard, apart from an optional gamification element, allows the display of multiple Jenkins instances. See: https://github.com/oasp/jenkins_view\n69.3.15. And lots more, among others:\nOASP4J/devonfw docker based build IN a docker process. See: https://github.com/devonfw/devon-guide/wiki/Dockerfile-for-the-maven-based-spring.io-projects\nCI test boot archetype. This is for unit testing.This will create a sample project and add sample web service to it. A Jenkins job will start oasp4j server and will call web service. See: https://github.com/devonfw/devonfw-shop-floor/tree/master/testing/Oasp4jTestingScripts\nCI test Angular starterTemplate. Testing automation for Angular applications (My Thai Star) in Continuous Integration environments by using Headless browsers and creating Node.js scripts. See: https://github.com/oasp/my-thai-star/blob/develop/angular/package.json#L8-L12 and https://github.com/oasp/my-thai-star/blob/develop/angular/karma.conf.js\n&#x2190;&#xA0;Previous:&#xA0;devonfw Release notes 2.4 &#x201C;EVE&#x201D;&#xA0;| &#x2191;&#xA0;Up:&#xA0;Release Notes&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;devonfw Release notes 2.2 &quot;Courage&quot;&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/release-notes-version-2.4.asciidoc.html","title":"68. devonfw Release notes 2.4 &#x201C;EVE&#x201D;","body":"\n68. devonfw Release notes 2.4 &#x201C;EVE&#x201D;\n68.1. Introduction\nWe are proud to announce the immediate release of devonfw version 2.4 (code named &#x201C;EVE&#x201D; during development). This version is the first one that fully embraces Open Source, including components like the documentation assets and Cobigen. Most of the IP (Intellectual Property or proprietary) part of devonfw are now published under the Apache License version 2.0 (with the documentation under the Creative Commons License (Attribution-NoDerivatives)). This includes the GitHub repositories where all the code and documentation is located. All of these repositories are now open for public viewing as well.\n&#x201C;EVE&#x201D; contains a slew of new features but in essence it is already driven by what we expect to be the core focus of 2018: strengthening the platform and improving quality.\nThis release is also fully focused on deepening the platform rather than expanding it. That is to say: we have worked on improving existing features rather than adding new ones and strengthen the qualitative aspects of the software development life cycle, i.e. security, testing, infrastructure (CI, provisioning) etc.\n&#x201C;EVE&#x201D; already is very much an example of this. This release contains the Allure Test Framework (included as an incubator in version 2.3) update called MrChecker Test Framework. MrChecker is an automated testing framework for functional testing of web applications, API web services, Service Virtualization, Security and in coming future native mobile apps, and databases. All modules have tangible examples of how to build resilient integration test cases based on delivered functions.\nAnother incubator being updated is the devonfw Shop Floor which intended to be a compilation of DevOps experiences from the devonfw perspective. A new part of the release is the new Solution Guide for Application Security based on the state of the art in OWASP based application security.\nThere is a whole range of new features and improvements which can be seen in that light. OASP4j 2.6 changes and improves the package structure of the core Java framework. The My Thai Star sample app has now been upgraded to Angular 6, lots of bugs have been fixed and the devonfw Guide has once again been improved.\nLast but not least, this release contains the formal publication of the devonfw Methodology or The Accelerated Solution Design - an Industry Standards based solution design and specification (documentation) methodology for Agile (and less-than-agile) projects.\n68.2. Changes and new features\n68.2.1. devonfw 2.4 is Open Source\nThis version is the first release of devonfw that fully embraces Open Source, including components like the documentation assets and Cobigen. This is done in response to intensive market pressure and demands from the MU&#xB4;s (Public Sector France, Netherlands)\nMost of the IP (Intellectual Property or proprietary) part of devonfw are now published under the Apache License version 2.0 (with the documentation under the Creative Commons License (Attribution-NoDerivatives)).\nSo you can now use the devonfw distribution (the &quot;zip&quot; file), Cobigen, the devonfw modules and all other components without any worry to expose the client unwittingly to Capgemini IP.\nNote: there are still some components which are IP and are not published under an OSS license. The class room trainings, the Sencha components and some Cobigen templates. But these are not includes in the distribution nor documentation and are now completely maintained separately.\n68.2.2. devonfw dist\nEclipse Oxygen integrated\nCheckStyle Plugin updated.\nSonarLint Plugin updated.\nGit Plugin updated.\nFindBugs Plugin updated.\nCobigen plugin updated.\nOther Software\nVisual Studio Code latest version included and preconfigured with https://github.com/oasp/oasp-vscode-ide\nAnt updated to latest.\nMaven updated to latest.\nJava updated to latest.\nNodejs LTS updated to latest.\n@angular/cli included.\nYarn package manager updated.\nPython3 updated.\nSpyder3 IDE integrated in python3 installation updated.\nOASP4JS-application-template for Angular 6 at workspaces/examples\n68.2.3. My Thay Star Sample Application\nThe new release of My Thai Star has focused on the following improvements:\nRelease 1.6.0.\nTravis CI integration with Docker. Now we get a valuable feedback of the current status and when collaborators make pull requests.\nDocker compose deployment.\nOASP4J:\nFlyway upgrade from 3.2.1 to 4.2.0\nBug fixes.\nOASP4JS:\nClient OASP4JS updated to Angular 6.\nFrontend translated into 9 languages.\nImproved mobile and tablet views.\nRouting fade animations.\nCompodoc included to generate dynamically frontend documentation.\n68.2.4. Documentation updates\nThe following contents in the devonfw guide have been updated:\ndevonfw OSS modules documentation.\nCreating a new OASP4J application.\nHow to update Angular CLI in devonfw.\nInclude Angular i18n.\nApart from this the documentation has been reviewed and some typos and errors have been fixed.\nThe current development of the guide has been moved to https://github.com/oasp-forge/devon-guide/wiki in order to be available as the rest of OSS assets.\n68.2.5. OASP4J\nThe following changes have been incorporated in OASP4J:\nIntegrate batch with archetype.\nApplication module structure and dependencies improved.\nIssues with Application Template fixed.\nSolved issue where Eclipse maven template oasp4j-template-server version 2.4.0 produced pom with missing dependency spring-boot-starter-jdbc.\nSolved datasource issue with project archetype 2.4.0.\nDecouple archetype from sample (restaurant).\nUpgrade to Flyway 4.\nFix for issue with Java 1.8 and QueryDSL #599.\n68.2.6. OASP4JS\nThe following changes have been incorporated in OASP4JS:\nFirst version of the new client application architecture guide https://github.com/oasp-forge/oasp4js-wiki/wiki\nAngular CLI 6,\nAngular 6,\nAngular Material 6 and Covalent 2.0.0-beta.1,\nIonic 3.20.0,\nCordova 8.0.0,\nOASP4JS Angular application template updated to Angular 6 with visual improvements and bugfixes https://github.com/oasp/oasp4js-application-template\nOASP4JS Ionic application template updated and improved https://github.com/oasp/oasp4js-ionic-application-template\nPWA enabled.\n68.2.7. AppSec Quick Solution Guide\nThis release incorporates a new Solution Guide for Application Security based on the state of the art in OWASP based application security. The purpose of this guide is to offer quick solutions for common application security issues for all applications based on devonfw. It&#x2019;s often the case that we need our systems to comply to certain sets of security requirements and standards. Each of these requirements needs to be understood, addressed and converted to code or project activity. We want this guide to prevent the wheel from being reinvented over and over again and to give clear hints and solutions to common security problems.\nThe wiki can be accessed here: https://github.com/devonfw/devonfw-security/wiki\nThe PDF can be accessed here: https://github.com/devonfw/devonfw-security\n68.2.8. CobiGen\nCobiGen_Templates project and docs updated.\nCobiGen Angular 6 generation improved based on the updated application template\nCobiGen Ionic CRUD App generation based on Ionic application template. Although a first version was already implemented, it has been deeply improved:\nChanged the code structure to comply with Ionic standards.\nAdded pagination.\nPull-to-refresh, swipe and attributes header implemented.\nCode documented and JSDoc enabled (similar to Javadoc)\nCobiGen TSPlugin Interface Merge support.\nCobiGen XML plugin comes out with new cool features:\nEnabled the use of XPath within variable assignment. You can now retrieve almost any data from an XML file and store it on a variable for further processing on the templates. Documented here.\nAble to generate multiple output files per XML input file.\nGenerating code from UML diagrams. XMI files (standard XML for UML) can be now read and processed. This means that you can develop templates and generate code from an XMI like class diagrams.\nCobiGen OpenAPI plugin released with multiple bug-fixes and other functionalities like:\nAssigning global and local variables is now possible. Therefore you can set any string for further processing on the templates. For instance, changing the root package name of the generated files. Documented here.\nEnabled having a class with more than one relationship to another class (more than one property of the same type).\nCobiGen Text merger plugin has been extended and now it is able to merge text blocks. This means, for example, that the generation and merging of AsciiDoc documentation is possible. Documented here.\n68.2.9. Devcon\nA new version of Devcon has been released. Fixes and new features include:\nNow Devcon is OSS, with public repository at https://github.com/devonfw/devcon\nUpdated to match current OASP4J\nUpdate to download Linux distribution.\nCustom modules creation improvements.\nBugfixes.\n68.2.10. devonfw OSS Modules\nExisting devonfw IP modules have been moved to OSS.\nThey can now be accessed in any OASP4J project as optional dependencies from Maven Central.\nThe repository now has public access https://github.com/devonfw/devon\nStarters available for modules:\nReporting module\nWinAuth AD Module\nWinAuth SSO Module\nI18n Module\nAsync Module\nIntegration Module\nMicroservice Module\nCompose for Redis Module\nSee: https://github.com/devonfw/devon/wiki#devonfw-modules\n68.2.11. devonfw Shop Floor\ndevonfw Shop Floor 4 Docker\nDocker-based CICD environment\ndocker-compose.yml (installation file)\ndsf4docker.sh (installation script)\nService Integration (documentation in Wiki)\ndevonfw projects build and deployment with Docker\nDockerfiles (multi-stage building)\nBuild artifact (NodeJS for Angular and Maven for Java)\nDeploy built artifact (NGINX for Angular and Tomcat for Java)\nNGINX Reverse-Proxy to redirect traffic between both Angular client and Java server containers.\ndevonfw Shop Floor 4 OpenShift\ndevonfw projects deployment in OpenShift cluster\ns2i images\nOpenShift templates\nVideo showcase (OpenShift Origin 3.6)\nThis incubator is intended to be a compilation of DevOps experiences from the devonfw perspective. &#x201C;How we use our devonfw projects in DevOps environments&#x201D;. Integration with the Production Line, creation and service integration of a Docker-based CI environment and deploying devonfw applications in an OpenShift Origin cluster using devonfw templates.\nSee: https://github.com/devonfw/devonfw-shop-floor\n68.2.12. devonfw Testing\nThe MrChecker Test Framework is an automated testing framework for functional testing of web applications, API web services, Service Virtualization, Security and in coming future native mobile apps, and databases. All modules have tangible examples of how to build resilient integration test cases based on delivered functions.\nExamples available under embedded project &#x201C;MrChecker-App-Under-Test&#x201D; and in project wiki: https://github.com/devonfw/devonfw-testing/wiki\nHow to install:\nWiki : https://github.com/devonfw/devonfw-testing/wiki/How-to-install\nRelease Note:\nmodule core - 4.12.0.8:\nfixes on getting Environment values\ntop notch example how to keep vulnerable data in repo , like passwords\nmodule selenium - 3.8.1.8:\nbrowser driver auto downloader\nlist of out off the box examples to use in any web page\nmodule webAPI - ver. 1.0.2 :\napi service virtualization with REST and SOAP examples\napi service virtualization with dynamic arguments\nREST working test examples with page object model\nmodule security - 1.0.1 (security tests against My Thai Start)\nmodule DevOps :\ndockerfile for Test environment execution\nCI + CD as jenkinsfile code\n68.2.13. devonfw methodology: Accelerated Solution Design\nOne of the prime challenges in Distributed Agile Delivery is the maintenance of a common understanding and unity of intent among all participants in the process of creating a product. That is: how can you guarantee that different parties in the client, different providers, all in different locations and time zones during a particular period of time actually understand the requirements of the client, the proposed solution space and the state of implementation.\nWe offer the Accelerated Solution Design as a possible answer to these challenges. The ASD is carefully designed to be a practical guideline that fosters and ensures the collaboration and communication among all team members.\nThe Accelerated Solution Design is:\nA practical guideline rather than a &#x201C;methodology&#x201D;\nBased on industry standards rather than proprietary methods\nConsisting of an evolving, &#x201C;living&#x201D;, document set rather than a static, fixed document\nEncapsulating the business requirements, functional definitions as well as Architecture design\nBased on the intersection of Lean, Agile, DDD and User Story Mapping\nAnd further it is based on the essential belief or paradigm that ASD should be:\nFocused on the design (definition) of the &#x201C;externally observable behavior of a system&#x201D;\nPromoting communication and collaboration between team members\nGuided by prototypes\nFor more on the devonfw Methodology / ASD, see:\nhttps://github.com/devonfw/devon-methodology/blob/master/design-guidelines/Accelerated_Solution_Design.adoc\n&#x2190;&#xA0;Previous:&#xA0;devonfw Release notes 3.0 &#x201C;Fry&#x201D;&#xA0;| &#x2191;&#xA0;Up:&#xA0;Release Notes&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;devonfw Release notes 2.3 &quot;Dash&quot;&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/release-notes-version-3.0.asciidoc.html","title":"67. devonfw Release notes 3.0 &#x201C;Fry&#x201D;","body":"\n67. devonfw Release notes 3.0 &#x201C;Fry&#x201D;\n67.1. Introduction\nWe are proud to announce the immediate release of devonfw version 3.0 (code named &#x201C;Fry&#x201D; during development). This version is the consolidation of Open Source, focused on the major namespace change ever in the platform, removing the OASP references and adopting the new devonfw names for each technical stack or framework.\nThe new stack names are the following:\ndevon4j, former OASP4J, is the new name for Java.\ndevon4ng, former OASP4JS, is the new one for Angular.\ndevon4net, is the new .NET stack.\ndevon4X, is the new stack for Xamarin development.\ndevon4node, is the new devonfw incubator for node.js.\nThe new devon4j version was created directly from the latest oasp4j version (3.0.0). Hence it brings all the features and values that oasp4j offered. However, the namespace migration was used to do some housekeeping and remove deprecated code as well as reduce dependencies. Therefore your data-access layer will no longer have to depend on any third party except for devon4j as well as of course the JPA. We also have improved the application template that now comes with a modern JSON logging ready for docker and logstash based environments.\nTo help you upgrading we introduced a migration feature in devcon. This can automatically migrate your code from oasp4j (even older versions starting from 2.4.0) to the latest version of devon4j. There might be some small manual changes left to do but 90% of the migration will be done automatically for you.\nBesides, the first version of the devonfw plugin for SonarQube has been released. It extends SonarQube with the ability to validate your code according to the devon4j architecture. More details at https://github.com/devonfw/sonar-devon-plugin.\nThis is the first release that integrates the new devonfw .NET framework, called devon4net, and Xamarin for mobile native development, devon4X. devon4NET and devon4X are the Capgemini standard frameworks for .NET and Xamarin software development. With the two new family members devonfw provides guidance and acceleration for the major software development platforms in our industry. Their interoperability provides you the assurance your multichannel solution will be consistent across web and mobile channels.\n&#x201C;Fry&#x201D; release contains lots of improvements in our Mr.Checker E2E Testing Framework, including a complete E2E sample inside our reference application My Thai Star. Besides Mr.Checker, we include as an incubator Testar, a test tool (and framework) to test applications at the GUI level whose objective is to solve part of the maintenance problem affecting tests by automatically generating test cases based on a structure that is automatically derived from the GUI. Testar is not included to replace Mr.Checker but rather to provide development teams with a series of interesting options which go beyond what Mr.Checker already provides.\nApart from Mr.Checker, engagements can now use Testar as an extra option for testing. This is a tool that enables the automated system testing of desktop, web and mobile applications at the GUI level. Testar has been added as an incubator to the platform awaiting further development during 2019.\nThe new incubator for node.js, called devon4node, has been included and implemented in several internal projects. This incubator is based on the Nest framework https://www.nestjs.com/. Nest is a framework for building efficient, scalable Node.js server-side applications. It uses progressive JavaScript, is built with TypeScript (preserves compatibility with pure JavaScript) and combines elements of OOP (Object Oriented Programming), FP (Functional Programming), and FRP (Functional Reactive Programming). Under the hood, Nest makes use of Express, but also provides compatibility with a wide range of other libraries (e.g. Fastify). This allows for easy use of the myriad third-party plugins which are available.\nIn order to facilitate the utilization of Microsoft Visual Studio Code in devonfw, we have developed and included the new devonfw Platform Extension Pack with lots of features to develop and test applications with this IDE in languages and frameworks such as TypeScript, JavaScript, .NET, Java, Rust, C++ and many more. More information at https://marketplace.visualstudio.com/items?itemName=devonfw.devonfw-extension-pack. Also, you can contribute to this extension in this GitHub repository https://github.com/devonfw/devonfw-extension-pack-vscode.\nThere is a whole range of new features and improvements which can be seen in that light. The My Thai Star sample app has now been upgraded to devon4j and devon4ng, a new devon4node backend implementation has been included that is seamless interchangeable, an E2E MrChecker sample project, CICD and deployment scripts and lots of bugs have been fixed.\nLast but not least, the projects wikis and the devonfw Guide has once again been updated accordingly before the big refactor that will be addressed in the following release in 2019.\n67.2. Changes and new features\n67.2.1. Devonfw dist\nEclipse 2018.9 integrated\nCheckStyle Plugin updated.\nSonarLint Plugin updated.\nGit Plugin updated.\nFindBugs Plugin updated.\nCobigen plugin updated.\nOther Software\nVisual Studio Code latest version included and preconfigured with the devonfw Platform Extension Pack.\nAnt updated to latest.\nMaven updated to latest.\nJava updated to latest.\nNodejs LTS updated to latest.\n@angular/cli included.\nYarn package manager updated.\nPython3 updated.\nSpyder3 IDE integrated in python3 installation updated.\ndevon4ng-application-template for Angular 7 at workspaces/examples\ndevon4ng-ionic-application-template for Ionic 3.20 at workspace/samples\n67.2.2. My Thay Star Sample Application\nThe new release of My Thai Star has focused on the following improvements:\nRelease 1.12.2.\ndevon4j:\ndevon4j 3.0.0 integrated.\nSpring Boot 2.0.4 integrated.\nSpring Data integration.\nNew pagination and search system.\nBug fixes.\ndevon4ng:\nClient devon4ng updated to Angular 7.\nAngular Material and Covalent UI frameworks updated.\nElectron framework integrated.\ndevon4node\nTypeScript 3.1.3.\nBased on Nest framework.\nAligned with devon4j.\nComplete backend implementation.\nTypeORM integrated with SQLite database configuration.\nWebpack bundler.\nNodemon runner.\nJest unit tests.\nMr.Checker\nExample cases for end-to-end test.\nProduction line configuration.\nCICD\nImproved integration with Production Line\nNew deployment from artifact\nNew CICD pipelines\nNew deployment pipelines\nAutomated creation of pipelines in Jenkins\n67.2.3. Documentation updates\nThe following contents in the devonfw guide have been updated:\nUpgrade of all the new devonfw named assets.\ndevon4j\ndevon4ng\nMr.Checker\nElectron integration cookbook.\nUpdated cookbook about Swagger.\nRemoved deprecated entries.\nApart from this the documentation has been reviewed and some typos and errors have been fixed.\nThe current development of the guide has been moved to https://github.com/devonfw-forge/devon-guide/wiki in order to be available as the rest of OSS assets.\n67.2.4. devon4j\nThe following changes have been incorporated in devon4j:\nSpring Boot 2.0.4 Integrated.\nSpring Data layer Integrated.\nDecouple mmm.util.*\nRemoved depreciated restaurant sample.\nUpdated Pagination support for Spring Data\nAdd support for hana as dbType.\nBugfixes.\n67.2.5. devon4ng\nThe following changes have been incorporated in devon4ng:\nNew client application architecture guide https://github.com/devonfw/devon4ng/wiki\nAngular CLI 7,\nAngular 7,\nAngular Material 7 and Covalent 2.0.0-beta.7,\nIonic 3.20.0,\nCordova 8.0.0,\ndevon4ng Angular application template updated to Angular 7 with visual improvements and bugfixes https://github.com/devonfw/devon4ng-application-template\ndevon4ng Ionic application template updated and improved https://github.com/devonfw/devon4ng-ionic-application-template\nPWA enabled.\nElectron integrated to run My Thai Star as a desktop application in Windows, Linux or macOS.\n67.2.6. devon4net\nSome of the highlights of devon4net 1.0 are:\nExternal configuration file for each environment.\n.NET Core 2.1.X working solution (Latest 2.1.402).\nPackages and solution templates published on nuget.org.\nFull components customization by config file.\nDocker ready (My Thai Star sample fully working on docker).\nPort specification by configuration.\nDependency injection by Microsoft .NET Core.\nAutomapper support.\nEntity framework ORM (Unit of work, async methods).\n.NET Standard library 2.0 ready.\nMulti-platform support: Windows, Linux, Mac.\nSamples: My Thai Star back-end, Google API integration, Azure login, AOP with Castle.\nDocumentation site.\nSPA page support.\nAnd included the following features:\nLogging:\nText File.\nSqlite database support.\nSerilog Seq Server support.\nGraylog integration ready through TCP/UDP/HTTP protocols.\nAPI Call params interception (simple and compose objects).\nAPI error exception management.\nSwagger:\nSwagger auto generating client from comments and annotations on controller classes.\nFull swagger client customization (Version, Title, Description, Terms, License, Json endpoint definition).\nJWT:\nIssuer, audience, token expiration customization by external file configuration.\nToken generation via certificate.\nMVC inherited classes to access JWT user properties.\nAPI method security access based on JWT Claims.\nCORS:\nSimple CORS definition ready.\nMultiple CORS domain origin definition with specific headers and verbs.\nHeaders:\nAutomatic header injection with middleware.\nSupported header definitions: AccessControlExposeHeader, StrictTransportSecurityHeader, XFrameOptionsHeader, XssProtectionHeader, XContentTypeOptionsHeader, ContentSecurityPolicyHeader, PermittedCrossDomainPoliciesHeader, ReferrerPolicyHeader.\nReporting server:\nPartial implementation of reporting server based on My-FyiReporting (now runs on linux container).\nTesting:\nIntegration test template with sqlite support.\nUnit test template.\nMoq, xunit frameworks integrated.\n67.2.7. devon4X\nSome of the highlights of the new devonfw Xamarin framework are:\nBased on Excalibur framework by Hans Harts (https://github.com/Xciles/Excalibur).\nUpdated to latest MVVMCross 6 version.\nMy Thai Star Excalibur forms sample.\nXamarin Forms template available on nuget.org.\n67.2.8. AppSec Quick Solution Guide\nThis release incorporates a new Solution Guide for Application Security based on the state of the art in OWASP based application security. The purpose of this guide is to offer quick solutions for common application security issues for all applications based on devonfw. It&#x2019;s often the case that we need our systems to comply to certain sets of security requirements and standards. Each of these requirements needs to be understood, addressed and converted to code or project activity. We want this guide to prevent the wheel from being reinvented over and over again and to give clear hints and solutions to common security problems.\nThe wiki can be accessed here: https://github.com/devonfw/devonfw-security/wiki\nThe PDF can be accessed here: https://github.com/devonfw/devonfw-security\n67.2.9. CobiGen\nCobiGen core new features:\nCobiGen_Templates will not need to be imported into the workspace anymore. However, If you want to adapt them, you can still click on a button that automatically imports them for you.\nCobiGen_Templates can be updated by one-click whenever the user wants to have the latest version.\nAdded the possibility to reference external increments on configuration level. This is used for reducing the number of duplicated templates.\nCobiGen_Templates project and docs updated:\nSpring standard has been followed better than ever.\nInterface templates get automatically relocated to the api project. Needed for following the new devon4j standard.\nCobiGen Angular:\nAngular 7 generation improved based on the updated application template.\nPagination changed to fit Spring standard.\nCobiGen Ionic: Pagination changed to fit Spring standard.\nCobiGen OpenAPI plugin released with multiple bug-fixes and other functionalities like:\nResponse and parameter types are parsed properly when they are a reference to an entity.\nParameters defined on the body of a request are being read correctly.\n67.2.10. Devcon\nA new version of Devcon has been released. Fixes and new features include:\nUpdated to match current devon4j\nUpdate to download Linux distribution.\nCustom modules creation improvements.\nCode Migration feature added\nBugfixes.\n67.2.11. Devonfw OSS Modules\nModules upgraded to be used in new devon4j projects:\nReporting module\nWinAuth AD Module\nWinAuth SSO Module\nI18n Module\nAsync Module\nIntegration Module\nMicroservice Module\nCompose for Redis Module\nSee: https://github.com/devonfw/devon/wiki#devonfw-modules\n67.2.12. Devonfw Testing\nMr.Checker\nThe Mr.Checker Test Framework is an automated testing framework for functional testing of web applications, API web services, Service Virtualization, Security and in coming future native mobile apps, and databases. All modules have tangible examples of how to build resilient integration test cases based on delivered functions. Mr.Checker updates and improvements:\nExamples available under embedded project &#x201C;MrChecker-App-Under-Test&#x201D; and in project wiki: https://github.com/devonfw/devonfw-testing/wiki\nHow to install:\nWiki : https://github.com/devonfw/devonfw-testing/wiki/How-to-install\nRelease Note:\nmodule selenium - 3.8.1.13:\nheadless browser\nenable browser options\nmodule DevOps :\nJenkinsfile align with ProductionLine\nTestar\nWe have added Test*, Testar, as an incubator to the available test tools within devonfw. This ground-breaking tool is being developed by the Technical University of Valencia (UPV). In 2019 Capgemini will co-develop Testar with the UPV.\nTestar is a tool that enables the automated system testing of desktop, web and mobile applications at the GUI level.\nWith Testar, you can start testing immediately. It automatically generates and executes test sequences based on a structure that is automatically derived from the UI through the accessibility API. Testar can detect the violation of general-purpose system requirements and you can use plugins to customize your tests.\nYou do not need test scripts and maintenance of it. The tests are random and are generated and executed automatically.\nIf you need to do directed tests you can create scripts to test specific requirements of your application.\nTestar is included in the devonfw distro or can be downloaded from https://testar.org/download/.\nThe Github repository can be found at o: https://github.com/TESTARtool/TESTAR.\n&#x2190;&#xA0;Previous:&#xA0;devonfw Release notes 3.1 &#x201C;Goku&#x201D;&#xA0;| &#x2191;&#xA0;Up:&#xA0;Release Notes&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;devonfw Release notes 2.4 &#x201C;EVE&#x201D;&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/release-notes-version-3.1.asciidoc.html","title":"66. devonfw Release notes 3.1 &#x201C;Goku&#x201D;","body":"\n66. devonfw Release notes 3.1 &#x201C;Goku&#x201D;\n66.1. Introduction\nWe are proud to announce the immediate release of devonfw version 3.1 (code named &#x201C;Goku&#x201D; during development). This version is the first one that implements our new documentation workflow, that will allow users to get the updated documentation at any moment and not to wait for the next devonfw release.\nThis is now possible as we have established a new workflow and rules during development of our assets. The idea behind this is that all the repositories contain a documentation folder and, in any pull request, the developer must include the related documentation change. A new Travis CI configuration added to all these repositories will automatically take the changes and publish them in the wiki section of every repository and in the new devonfw-guide repository that consolidates all the changes from all the repositories. Another pipeline will take changes from this consolidated repository and generate dynamically the devonfw guide in PDF and in the next weeks in HTML for the new planned devonfw website. The following schema explains this process:\nThis release includes the very first version of the new CobiGen CLI. Now using commands, you will be able to generate code the same way as you do with Eclipse. This means that you can use CobiGen on other IDEs like Visual Studio Code or IntelliJ. Please take a look at https://github.com/devonfw/tools-cobigen/wiki/howto_Cobigen-CLI-generation for more info.\nThe devonfw-shop-floor project has got a lot of updates in order to make even easier the creation of devonfw projects with CICD pipelines that run on the Production Line, deploy on Red Hat OpenShift Clusters and in general Docker environments. See the details below.\nThis release includes the very first version of our devonfw-ide tool that will allow users to automate devonfw setup and update the development environment. This tool will become the default devonfw setup tool in future releases. For more information please visit the repository https://github.com/devonfw/devon-ide.\nFollowing the same collaboration model we used in order to improve the integration of devonfw with Red Hat OpenShift and which allowed us to get the Red Hat Open Shift Primed certification, we have been working alongside with SAP HANA developers in order to support this database in the devon4j. This model was based on the contribution and review of pull requests in our reference application My Thai Star. In this case, SAP developers collaborated with us in the following two new use cases:\nPrediction of future demand\nGeospatial analysis and clustering of customers\nMore info at https://blogs.sap.com/2019/06/17/introducing-devonfw-support-for-sap-hana/.\nLast but not least the devonfw extension pack for VS Code has been improved with the latest extensions and helpers for this IDE. Among many others you can now use:\nRemote development on Docker containers and VMs https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.vscode-remote-extensionpack\nDependency Analysis for maven and npm https://marketplace.visualstudio.com/items?itemName=redhat.fabric8-analytics\nReact Native Tools https://marketplace.visualstudio.com/items?itemName=msjsdiag.vscode-react-native\nNgRx Snippets https://marketplace.visualstudio.com/itemdetails?itemName=hardikpthv.NgRxSnippets\nAlso it is worth the try of the updated support for Java and Spring Boot development in VS Code. Check it out for yourself!\nMore information at https://marketplace.visualstudio.com/items?itemName=devonfw.devonfw-extension-pack. Also, you can contribute to this extension in this GitHub repository https://github.com/devonfw/devonfw-extension-pack-vscode.\n66.2. Changes and new features\n66.2.1. Devonfw dist\nEclipse 2018.12 integrated\nCheckStyle Plugin updated.\nSonarLint Plugin updated.\nGit Plugin updated.\nFindBugs Plugin updated.\nCobigen plugin updated.\nOther Software\nVisual Studio Code latest version included and preconfigured with the devonfw Platform Extension Pack.\nAnt updated to latest.\nMaven updated to latest.\nJava updated to latest.\nNodejs LTS updated to latest.\n@angular/cli included.\n@devonfw/cicdgen included.\nYarn package manager updated.\nPython3 updated.\nSpyder3 IDE integrated in python3 installation updated.\ndevon4ng-application-template for Angular 8 at workspaces/examples\ndevon4ng-ionic-application-template for Ionic 4 at workspace/samples\n66.2.2. My Thay Star Sample Application\nThe new release of My Thai Star has focused on the following improvements:\nRelease 3.1.0.\ndevon4j:\ndevon4j 3.1.0 integrated.\nSpring Boot 2.1.6 integrated.\nSAP 4/HANA prediction use case.\nBug fixes.\ndevon4ng:\nSAP 4/HANA prediction use case.\n2FA togglable (two factor authentication).\nNgRx integration in process (PR #234).\ndevon4node\nTypeScript 3.1.3.\nBased on Nest framework.\nAligned with devon4j.\nComplete backend implementation.\nTypeORM integrated with SQLite database configuration.\nWebpack bundler.\nNodemon runner.\nJest unit tests.\nMr.Checker\nExample cases for end-to-end test.\nProduction line configuration.\nCICD\nImproved integration with Production Line\nNew Traefik load balancer and reverse proxy\nNew deployment from artifact\nNew CICD pipelines\nNew deployment pipelines\nAutomated creation of pipelines in Jenkins\n66.2.3. Documentation updates\nThis release addresses the new documentation workflow, being now possible to keep the documentation synced with any change. The new documentation includes the following contents:\nGetting started\nContribution guide\nDevcon\nRelease notes\ndevon4j documentation\ndevon4ng documentation\ndevon4net documentation\ndevonfw-shop-floor documentation\ncicdgen documentation\ndevonfw testing with MrChecker\nMy Thai Star documentation\n66.2.4. devon4j\nThe following changes have been incorporated in devon4j:\nAdded Support for Java8 up to Java11\nUpgrade to Spring Boot 2.1.6.\nUpgrade to Spring 5.1.8\nUpgrade to JPA 2.2\nUpgrade to Hibernate 5.3\nUpgrade to Dozer 6.4.1 (ATTENTION: Requires Migration, use devon-ide for automatic upgrade)\nMany improvements to documentation (added JDK guide, architecture-mapping, JMS, etc.)\nCompleted support (JSON, Beanmapping) for pagination, IdRef, and java.time\nAdded MasterCto\nFor all details see milestone.\n66.2.5. devon4ng\nThe following changes have been incorporated in devon4ng:\nAngular CLI 8,\nAngular 8,\nAngular Material 8,\nIonic 4,\nCapacitor 1.0 as Cordova replacement,\nNgRx 8 support for State Management,\ndevon4ng Angular application template updated to Angular 8 with visual improvements and bugfixes https://github.com/devonfw/devon4ng-application-template\ndevon4ng Ionic application template updated and improved https://github.com/devonfw/devon4ng-ionic-application-template\nNew devon4ng Angular application template with state management using Angular 8 and NgRx 8 https://github.com/devonfw/devon4ng-ngrx-template\nNew devon4ng library https://github.com/devonfw/devon4ng-library that includes the following libraries:\nCache Module for Angular 7+ projects.\nAuthorization Module for Angular 7+ projects.\nNew use cases with documentation and samples:\nWeb Components with Angular Elements\nInitial configuration with App Initializer pattern\nError Handling\nPWA with Angular and Ionic\nLazy Loading\nLibrary construction\nLayout with Angular Material\nTheming with Angular Material\n66.2.6. devon4net\nThe following changes have been incorporated in devon4net:\nNew circuit breaker component to communicate microservices via HTTP\nResolved the update packages issue\n66.2.7. AppSec Quick Solution Guide\nThis release incorporates a new Solution Guide for Application Security based on the state of the art in OWASP based application security. The purpose of this guide is to offer quick solutions for common application security issues for all applications based on devonfw. It&#x2019;s often the case that we need our systems to comply to certain sets of security requirements and standards. Each of these requirements needs to be understood, addressed and converted to code or project activity. We want this guide to prevent the wheel from being reinvented over and over again and to give clear hints and solutions to common security problems.\nThe wiki can be accessed here: https://github.com/devonfw/devonfw-security/wiki\nThe PDF can be accessed here: https://github.com/devonfw/devonfw-security\n66.2.8. CobiGen\nCobiGen core new features:\nCobiGen CLI: New command line interface for CobiGen. Using commands, you will be able to generate code the same way as you do with Eclipse. This means that you can use CobiGen on other IDEs like Visual Studio Code or IntelliJ. Please take a look into the documentation for more info.\nPerformance improves greatly in the CLI thanks to the lack of GUI.\nYou will be able to use path globs for selecting multiple input files.\nWe have implemented a search functionality so that you can easily search for increments or templates.\nFirst steps taken on CobiGen refactoring: With the new refactoring we will be able to decouple Cobigen completely from the target and input language. This will facilitate the creation of parsers and mergers for any language.\nNashornJS has been deprecated: It was used for executing JavaScript code inside JVM. With the refactoring, performance has improved on the TypeScript merger.\nImproving CobiGen templates:\nRemoved Covalent from Angular templates as it is not compatible with Angular 8.\nAdded devon4ng-NgRx templates that implement reactive state management. Note: The TypeScript merger is currently being improved in order to accept NgRx. The current templates are set as overridable by default.\nTest data builder templates now make use of Lambdas and Consumers.\nCTOs and ETOs increments have been correctly separated.\nTypeScript merger has been improved: Now it is possible to merge comments (like tsdoc) and enums.\nOpenAPI parsing extended to read enums. Also fixed some bugs when no properties were set or when URLs were too short.\nJava static and object initializers now get merged.\nFixed bugs when downloading and adapting templates.\n66.2.9. Devcon\nA new version of Devcon has been released. Fixes and new features include:\nUpdated to match current devon4j\nUpdate to download Linux distribution.\nCustom modules creation improvements.\nCode Migration feature added.\nBugfixes.\n66.2.10. Devonfw OSS Modules\nModules upgraded to be used in new devon4j projects:\nReporting module\nWinAuth AD Module\nWinAuth SSO Module\nI18n Module\nAsync Module\nIntegration Module\nMicroservice Module\nCompose for Redis Module\nSee: https://github.com/devonfw/devon/wiki#devonfw-modules\n66.2.11. devonfw shop floor\nIndustrialization oriented to configure the provisioning environment provided by Production Line and deploy applications on an OpenShift cluster.\nAdded Jenkinsfiles to configure automatically OpenShift environments to deploy devonfw applications.\nIndustrialization to start new projects and configure them with CICD.\nUpgrade the documentation with getting started guide to configure CICD in any devonfw project and deploy it.\nAdded new tool cicdgen to generate CICD code/files.\ncicdgen\ncicdgen is a devonfw tool to generate all code/files related to CICD in your project. It&#x2019;s based on angular schematics and it has its own CLI.\nMore information here.\nCICD configuration for devon4j, devon4ng and devon4node projects\nOption to deploy devonfw projects with Docker\nOption to deploy devonfw projects with OpenShift\n66.2.12. Devonfw Testing\nMr.Checker\nThe Mr.Checker Test Framework is an automated testing framework for functional testing of web applications, API web services, Service Virtualization, Security and in coming future native mobile apps, and databases. All modules have tangible examples of how to build resilient integration test cases based on delivered functions. Mr.Checker updates and improvements:\nExamples available under embedded project &#x201C;MrChecker-App-Under-Test&#x201D; and in project wiki: https://github.com/devonfw/devonfw-testing/wiki\nHow to install:\nWiki : https://github.com/devonfw/devonfw-testing/wiki/How-to-install\nRelease Note:\nmodule selenium - 3.8.2.1:\npossibility to define version of driver in properties.file\nautomatic driver download if the version is not specified\npossibility to run with different browser options\nmodule webAPI &#x2013; 1.2.1:\npossibility to connect to the remote WireMock server\n&#x2190;&#xA0;Previous:&#xA0;devonfw Release notes 3.2 &#x201C;Homer&#x201D;&#xA0;| &#x2191;&#xA0;Up:&#xA0;Release Notes&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;devonfw Release notes 3.0 &#x201C;Fry&#x201D;&#xA0;&#x2192;\n"},{"id":"../website/pages/docs/release-notes-version-3.2.asciidoc.html","title":"65. devonfw Release notes 3.2 &#x201C;Homer&#x201D;","body":"\n65. devonfw Release notes 3.2 &#x201C;Homer&#x201D;\n65.1. Introduction\nWe are proud to announce the immediate release of devonfw version 3.2 (code named &#x201C;Homer&#x201D; during development). This version is the first one that contains the new devonfw IDE by default, so there is no need to download a huge ZIP with the whole distribution regardless of the use to which it will be put. The new devonfw IDE CLI will allow any user to setup a customized development environment completely configured with access to all the devonfw features, frameworks and tools. As we access to the official IDEs this is also the first version macOS compatible.\nThis release consolidates the documentation workflow adding the contents dynamically to the new devonfw website at the same time the PDF is generated. This have been achieved using a new GitHub action that takes the contents and builds the HTMLs for the documentation section of the website. The documentation workflow proposed in the following picture is now complete:\nThis release also includes the first version of devon4node. We consider that node.js should be a first-class citizen inside the devonfw platform and for that reason we have included the latest development technologies for this ecosystem. The devon4node CLI, schematics and other tools will allow our users to create powerful node.js applications with the same philosophy you may find in the other languages and frameworks included. More information at its section below.\nThe new devon4net 3.2.0 version is also included in this release. Based on the .NET Core 3.0 and containing lots of new features gathered from important and recent projects, it represents a great improvement and an intermediate step to provide support for the incoming .NET Core 3.1 LTS. More information at its section below.\nThis release includes the final version of the new CobiGen CLI and completely integrated with the new devonfw IDE. Now using commands, you will be able to generate code the same way as you do with Eclipse. This means that you can use CobiGen on other IDEs like Visual Studio Code or IntelliJ. Besides the Update command has been implemented. Now you will be able to update easily all your CobiGen plug-ins and templates inside the CLI.\nOn the other hand, the refactoring process has been completely developed, improving the mergers and including input readers for any other languages and frameworks, allowing the creation of models to generate code from them. Last, but not least, this new version includes the new templates for devon4net, devon4ng and devon4j generation.\nAnd as always, My Thai Star has been updated to the latest versions of devon4j, devon4node and devon4net including completely State Management with NgRx in its devon4ng implementation upgrade.\n65.2. Changes and new features\n65.2.1. devonfw-ide\nWe have entirely rewritten our automated solution for your local IDE (integrated desktop environment). The former oasp4j-ide and devonfw distributions with their extra-large gigabite zip files are not entirely replaced with devonfw-ide. This new solution is provided as a small *.tar.gz file that is publicly available. It works on all platforms and has been tested on Windows, MacOS, and Linux. After extraction you only need to run a setup script. Here you provide a settings git URL for your customer project or simply hit return for testing or small projects. After reading and confirming the terms of use it will download all required tools in the proper versions for your operating system and configure them. Instead of various confusing scripts there is now only one CLI command devon for all use-cases what gives a much better user experience.\nTo get started go to the home page. There is even a migration-guide if you are currently used to the old approach and want to quickly jump into the new solution.\n65.2.2. My Thay Star Sample Application\nThe new release of My Thai Star has focused on the following improvements:\nRelease 3.2.0.\ndevon4j:\ndevon4j 3.2.0 integrated.\nSpring Boot 2.1.9 integrated.\nSAP 4/HANA prediction use case.\nBug fixes.\ndevon4ng:\nSAP 4/HANA prediction use case.\n2FA togglable (two factor authentication).\nNgRx full integrated (PR #285).\ndevon4net\ndevon4net for dotnet core 3.0 updated\nUpdated the API contract compatible with the other stacks\nJWT implementation reviewed to increase security\nASP.NET users database dependencies removed\nHTTP2 support\nMore clear CRUD pattern implementation\ndevon4node\nTypeScript 3.6.3.\nBased on Nest framework.\nConfiguration Module\nAdded cors and security headers\nAdded mailer module and email templates.\nBuilt in winston logger\nCustom ClassSerializerInterceptor\nMr.Checker\nExample cases for end-to-end test.\nProduction line configuration.\nCICD\nImproved integration with Production Line\nNew Traefik load balancer and reverse proxy\nNew deployment from artifact\nNew CICD pipelines\nNew deployment pipelines\nAutomated creation of pipelines in Jenkins\n65.2.3. Documentation updates\nThis release addresses the new documentation workflow, being now possible to keep the documentation synced with any change. The new documentation includes the following contents:\nGetting started\ndevonfw ide\ndevon4j documentation\ndevon4ng documentation\ndevon4net documentation\ndevon4node documentation\nCobiGen documentation\ndevonfw-shop-floor documentation\ncicdgen documentation\ndevonfw testing with MrChecker\nMy Thai Star documentation\nContribution guide\nRelease notes\n65.2.4. devon4j\nThe following changes have been incorporated in devon4j:\nCompleted full support from Java8 to Java11\nSeveral security fixes\nUpgrade to Spring Boot 2.1.9\nUpgrade to Spring 5.1.8\nUpgrade to JUnit 5 (requires migration via devonfw-ide)\nImproved JPA support for IdRef\nImproved auditing metadata support\nMany improvements to documentation (added JDK guide, architecture-mapping, JMS, etc.)\nFor all details see milestone.\n65.2.5. devon4ng\nThe following changes have been incorporated in devon4ng:\nAngular CLI 8.3.1,\nAngular 8.2.11,\nAngular Material 8.2.3,\nIonic 4.11.1,\nCapacitor 1.2.1 as Cordova replacement,\nNgRx 8.3 support for State Management,\ndevon4ng Angular application template updated to Angular 8.2.11 with visual improvements and bugfixes https://github.com/devonfw/devon4ng-application-template\ndevon4ng Ionic application template updated to 4.11.1 and improved https://github.com/devonfw/devon4ng-ionic-application-template\nImproved devon4ng Angular application template with state management using Angular 8 and NgRx 8 https://github.com/devonfw/devon4ng-ngrx-template\nDocumentation and samples updated to latest versions:\nWeb Components with Angular Elements\nInitial configuration with App Initializer pattern\nError Handling\nPWA with Angular and Ionic\nLazy Loading\nLibrary construction\nLayout with Angular Material\nTheming with Angular Material\n65.2.6. devon4net\nThe following changes have been incorporated in devon4net:\nUpdated to latest .net core 3.0 version\nTemplate\nGlobal configuration automated. devon4net can be instantiated on any .net core application template with no effort\nAdded support for HTTP2\nNumber of libraries minimized\nArchitecture layer review. More clear and scalable\nAdded red button functionality (aka killswitch) to stop attending API request with custom error\nImproved API error management\nAdded support to only accept request from clients with a specific client certificate on Kestrel server. Special thanks to Bart Roozendaal (Capgemini NL)\nAll components use IOptions pattern to be set up properly\nSwagger generation compatible con open api v3\nModules\nThe devon4net netstandard libraries have been updated to netstandard 2.1\nJWT:\nAdded token encryption (token cannot be decrypted anymore by external parties). Now You can choose the encryption algorithm depending on your needs\nAdded support for secret key or certificate encryption\nAdded authorization for swagger portal\nCircuit breaker\nAdded support to bypass certificate validation\nAdded support to use a certificate for https communications using Microsoft&#x2019;s httpclient factory\nUnit of Work\nRepository classes unified and reviewed for increasing performance and reduce the consumed memory\nAdded support for different database servers: In memory, Cosmos, MySQL + MariaDB, Firebird, PostgreSQL, Oracle, SQLite, Access, MS Local.\n65.2.7. devon4node\nThe following changes have been incorporated in devon4node:\nTypeScript 3.6.3.\nBased on Nest framework.\nComplete backend implementation.\nNew devon4node CLI. It will provide you some commands\nnew: create a new devon4node interactively\ngenerate: generate code based on schematics\ndb: manage the database\nNew devon4node schematics\napplication: create a new devon4node application\nconfig-module: add a configuration module to the project\nmailer: install and configure the devon4node mailer module\ntypeorm: install TypeORM in the project\nauth-jwt: add users and auth-jwt modules to the project\nswagger: expose an enpoint with the autogenerated swagger\nsecurity: add cors and other sercurity headers to the project.\ncrud: create all CRUD for an entity\nentity: create an entity\nNew mailer module\nNew common library\nBuild in winston logger\nCustom ClassSerializerInterceptor\nExtendable base entity\nNew application samples\n65.2.8. CobiGen\nCobiGen core new features:\nCobiGen CLI: Update command implemented. Now you will be able to update easily all your CobiGen plug-ins and templates inside the CLI. Please take a look into the documentation for more info.\nCobiGen CLI is now JDK11 compatible.\nCobiGen CLI commandlet for devonfw-ide has been added. You can use it to setup easily your CLI and to run CobiGen related commands.\nAdded a version provider so that you will be able to know all the CobiGen plug-ins versions.\nAdded a process bar when the CLI is downloading the CobiGen plug-ins.\nCobiGen refactoring finished: With this refactoring we have been able to decouple Cobigen completely from the target and input language. This facilitates the creation of parsers and mergers for any language. For more information please take a look here.\nNew TypeScript input reader: We are now able to parse any TypeScript class and generate code using the parsed information. We currently use TypeORM entities as a base for generation.\nImproving CobiGen templates:\nUpdated devon4ng-NgRx templates to NgRx 8.\nGeneration of an Angular client using as input a TypeORM entity. This is possible thanks to the new TypeScript input reader.\n.Net templates have been upgraded to .Net Core 3.0\nCobiGen for Eclipse is now JDK11 compatible.\nFixed bugs when adapting templates and other bugs on the CobiGen core.\n65.2.9. devonfw shop floor\nAdded devon4ng OpenShift templates\nAdded devon4j OpenShift templates\nAdded devon4node OpenShift templates\nAdded more methods to link https://github.com/devonfw-forge/devon-production-line-shared-lib [devonfw Production Line shared library]\nUpdated link: devonfw Production Line templates\ncicdgen\nPatched minor bugs\n65.2.10. sonar-devon4j-plugin\nsonar-devon4j-plugin is a SonarQube plugin for architecture governance of devon4j applications. It verifies the architecture and conventions of devon4j, the Java stack of devonfw. The following changes have been incorporated:\n* Plugin was renamed from sonar-devon-plugin to sonar-devon4j-plugin\n* Rules/checks have been added to verify naming conventions\n* New rule for proper JPA datatype mapping\n* Proper tagging of rules as architecture-violation and not as bug, etc.\n* Several improvements have been made to prepare the plugin to enter the SonarQube marketplace, what will happen with the very next release.\n* Details can be found here: https://github.com/devonfw/sonar-devon4j-plugin/milestone/2?closed=1\n&#x2191;&#xA0;Up:&#xA0;Release Notes&#xA0;| &#x2302;&#xA0;Home:&#xA0;devonfw guide&#xA0;| Next:&#xA0;devonfw Release notes 3.1 &#x201C;Goku&#x201D;&#xA0;&#x2192;\n"}]